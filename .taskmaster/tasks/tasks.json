{
  "master": {
    "tasks": [
      {
        "id": "1",
        "title": "Implement Transcript File Upload Backend",
        "description": "Create backend endpoints for transcript file upload and storage in Azure Blob Storage",
        "details": "Create POST /api/transcripts/upload endpoint that accepts .txt, .md, .docx files up to 10MB. Validate file types, store in Azure Blob Storage 'transcripts' container, and return blob URL with metadata. Use Azure Storage SDK for Python: from azure.storage.blob import BlobServiceClient. Implement file validation using python-magic library for MIME type checking. Return JSON with blob_url, file_name, file_size, upload_timestamp.",
        "testStrategy": "Unit tests for file validation, integration tests for blob storage upload, test file size limits, test invalid file types rejection, verify blob URL accessibility",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up Azure Blob Storage client and environment",
            "description": "Configure the Azure Blob Storage SDK for Python and set up environment variables for authentication and storage account access.",
            "dependencies": [],
            "details": "Install azure-storage-blob and azure-identity packages. Set environment variables for AZURE_STORAGE_BLOB_URL and authentication (e.g., DefaultAzureCredential). Initialize BlobServiceClient using the storage URL and credential.\n<info added on 2025-11-24T21:13:29.673Z>\nI'll analyze the codebase to provide specific implementation details for the subtask update.Based on my analysis of the codebase, here's the implementation update text:\n\nImplementation completed in backend/src/routers/transcripts.py. Created APIRouter with prefix '/api/transcripts' at line 12. Implemented POST /upload endpoint (lines 115-163) with FastAPI File upload handling. File validation includes extension check against ALLOWED_EXTENSIONS (.txt, .md, .docx) and MIME type validation (text/plain, text/markdown, application/vnd.openxmlformats-officedocument.wordprocessingml.document). Size limit enforced at 10MB (MAX_FILE_SIZE_BYTES = 10485760 bytes). Created TranscriptBlobService class (lines 24-92) using BlobServiceClient.from_connection_string with dedicated 'transcripts' container (separate from default 'resources' container). Endpoint returns JSON with blob_url, file_name, file_size, and upload_timestamp. Router registered in main.py line 62 via app.include_router(transcripts.router). Dependencies satisfied: azure-storage-blob==12.23.1 already in requirements.txt.\n</info added on 2025-11-24T21:13:29.673Z>",
            "status": "done",
            "testStrategy": "Verify environment variables are loaded and BlobServiceClient initializes without errors.",
            "parentId": "undefined",
            "updatedAt": "2025-11-24T21:09:41.539Z"
          },
          {
            "id": 2,
            "title": "Create file upload endpoint with validation",
            "description": "Implement a POST /api/transcripts/upload endpoint that accepts file uploads and validates file type and size.",
            "dependencies": [
              1
            ],
            "details": "Use FastAPI or Flask to create the endpoint. Accept only .txt, .md, .docx files up to 10MB. Use python-magic to check MIME types. Reject invalid files with appropriate error responses.",
            "status": "done",
            "testStrategy": "Unit tests for file validation, test file size limits, test invalid file types rejection.",
            "parentId": "undefined",
            "updatedAt": "2025-11-24T21:50:53.937Z"
          },
          {
            "id": 3,
            "title": "Store validated files in Azure Blob Storage",
            "description": "Upload validated transcript files to the 'transcripts' container in Azure Blob Storage.",
            "dependencies": [
              2
            ],
            "details": "Generate a unique blob name for each file. Use BlobServiceClient to upload the file to the 'transcripts' container. Handle upload errors and ensure files are stored securely.",
            "status": "pending",
            "testStrategy": "Integration tests for blob storage upload, verify blob URL accessibility.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Return upload metadata as JSON response",
            "description": "Generate and return a JSON response with blob URL, file name, file size, and upload timestamp.",
            "dependencies": [
              3
            ],
            "details": "After successful upload, construct a JSON response containing the blob URL, original file name, file size in bytes, and upload timestamp. Ensure all metadata is accurate and complete.",
            "status": "pending",
            "testStrategy": "Verify JSON response structure and content, check metadata accuracy.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement comprehensive error handling and logging",
            "description": "Add error handling for all steps and implement logging for debugging and monitoring.",
            "dependencies": [
              4
            ],
            "details": "Catch and handle exceptions during file validation, upload, and metadata generation. Log errors and important events for debugging and monitoring purposes. Ensure user-friendly error messages are returned.",
            "status": "pending",
            "testStrategy": "Test error scenarios, verify logging output, check user-friendly error messages.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-11-24T21:52:25.019Z"
      },
      {
        "id": "2",
        "title": "Build AI Transcript Processing Pipeline",
        "description": "Implement AI processing pipeline to extract meeting insights from transcripts using Azure AI Foundry Model Router",
        "details": "Create POST /api/transcripts/process endpoint. Read transcript from Blob Storage, implement chunking for >15k tokens (split into 10k chunks with 500 token overlap). Use Azure AI Foundry Model Router with system prompt to extract: meeting summary (2-3 sentences), action items list, key decisions, topics/keywords. For chunking: use tiktoken library to count tokens. Merge and deduplicate results from multiple chunks. Update Cosmos DB meeting record with extracted data in fields: summary, action_items[], decisions[], topics[].",
        "testStrategy": "Test with various transcript sizes, verify chunking logic, test AI extraction accuracy, validate deduplication, test Cosmos DB updates, mock AI responses for unit tests",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-11-24T21:59:35.385Z"
      },
      {
        "id": "3",
        "title": "Implement Auto-Task Creation from Action Items",
        "description": "Automatically create tasks in Cosmos DB from extracted action items with proper linking and assignment",
        "details": "For each extracted action item, create task document in Cosmos DB 'tasks' collection. Parse action item text to detect assignee names using regex/NLP. Set task fields: title (action item text), status='Pending', created_from_meeting=meeting_id, assigned_to (if detected), priority based on keywords (urgent/critical/ASAP = High, otherwise Medium). Use spaCy or similar for name entity recognition. Update meeting status from 'Scheduled' to 'Completed' and set updated_at timestamp.",
        "testStrategy": "Test action item parsing, verify task creation with correct fields, test assignee detection accuracy, validate meeting status updates, test priority assignment logic",
        "priority": "high",
        "dependencies": [
          "2"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-11-24T22:01:18.942Z"
      },
      {
        "id": "4",
        "title": "Connect Frontend Upload Dialog to Backend",
        "description": "Integrate existing upload dialog in meetings page with backend transcript processing",
        "details": "Update meetings/page.tsx upload dialog to call /api/transcripts/upload then /api/transcripts/process. Implement file upload with progress indicator using fetch API with FormData. Show processing states: 'Uploading' → 'Processing' → 'Complete'. Display success message with count of extracted items (X action items, Y decisions created). Add error handling with user-friendly messages. Refresh meeting list after successful processing using SWR revalidation or state update.",
        "testStrategy": "Test file upload flow, verify progress indicators, test error handling, validate success messages, test meeting list refresh, test with various file sizes",
        "priority": "high",
        "dependencies": [
          "3"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-11-24T22:10:45.118Z"
      },
      {
        "id": "5",
        "title": "Setup Azure AI Search Index for RAG",
        "description": "Create and configure Azure AI Search index for meeting transcripts, tasks, agents, and governance data",
        "details": "Create Azure AI Search service and index with fields: id, content, type (meeting|task|agent|governance), metadata (title, date, status). Implement vector embeddings using Azure OpenAI text-embedding-ada-002 model. Create search client using azure-search-documents SDK. Index existing data on application startup: chunk meeting transcripts into 1000-character segments, index task descriptions, agent information, static governance policies. Implement incremental indexing function for new data.",
        "testStrategy": "Verify index creation, test embedding generation, validate data indexing, test search functionality, verify incremental updates work correctly",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Azure AI Search service client and index schema definition",
            "description": "Create search_service.py module with SearchIndexClient initialization and define index schema with fields for RAG content",
            "dependencies": [],
            "details": "Create backend/src/search_service.py file. Initialize SearchIndexClient using azure_search_endpoint and azure_search_api_key from settings. Define index schema using SearchIndex from azure.search.documents.indexes.models with fields: id (string, key), content (string, searchable), type (string, filterable - values: meeting|task|agent|governance), metadata (complex type with title, date, status sub-fields). Configure vector search using VectorSearch with HnswAlgorithmConfiguration for embeddings field (Collection(Edm.Single), dimension 1536 for text-embedding-ada-002). Set up SearchField configurations with appropriate attributes (searchable, filterable, retrievable).\n<info added on 2025-11-24T21:47:36.888Z>\nImplementation completed with comprehensive Azure AI Search functionality:\n\n**Implementation Completed:**\n- Created backend/src/search_service.py with Azure AI Search integration\n- Implemented SearchIndexClient initialization with endpoint and API key from settings\n- Defined comprehensive index schema with fields: id (key), content (searchable), content_vector (Collection(Edm.Single), 1536 dimensions), type (filterable), title (searchable), metadata (string), status, priority, category\n- Configured vector search using VectorSearch with HnswAlgorithmConfiguration (dimension 1536 for text-embedding-ada-002 model)\n- Added helper functions: create_or_update_index(), delete_index(), index_exists()\n- SearchField configurations set with appropriate attributes (searchable, filterable, retrievable, key)\n\n**File Location:** backend/src/search_service.py\n\n**Next Step:** Implement Azure OpenAI embedding generation function to transform text content into 1536-dimensional vectors for semantic search capability.\n</info added on 2025-11-24T21:47:36.888Z>",
            "status": "done",
            "testStrategy": "Unit test index schema validation, verify SearchIndexClient initialization with valid/invalid credentials, test index field definitions match requirements",
            "updatedAt": "2025-11-24T21:46:35.759Z",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Azure OpenAI embedding generation function",
            "description": "Create embedding generation function using Azure OpenAI text-embedding-ada-002 model for vectorizing content",
            "dependencies": [
              1
            ],
            "details": "In search_service.py, create async function generate_embeddings(texts: List[str]) -> List[List[float]]. Initialize AzureOpenAI client using azure_openai_endpoint, azure_openai_api_key, and azure_openai_embeddings_deployment from settings. Implement batch embedding generation (max 16 texts per batch per API limits). Handle rate limiting with exponential backoff retry logic. Return list of 1536-dimension embedding vectors. Add error handling for API failures and timeout scenarios. Cache OpenAI client instance as class attribute to avoid re-initialization.\n<info added on 2025-11-24T21:52:09.110Z>\nI'll analyze the codebase to understand the current implementation and provide an accurate update.Based on my analysis of the actual implementation in `backend/src/search_service.py`, I can see the embedding functions have been implemented with the following specifics:\n\nImplemented generate_embedding() for single text (lines 224-259) and generate_embeddings_batch() for multiple texts (lines 261-299). Both functions use text-embedding-ada-002 model via Azure OpenAI client initialized in __init__() (lines 48-54). Text truncation implemented with 32,000 character limit for both functions to stay within 8191 token limit. Error handling includes try-except blocks with logging. Azure OpenAI client cached as class instance attribute (self.openai_client). Returns 1536-dimensional vectors extracted from response.data[].embedding. Batch function processes all texts in single API call using list input to embeddings.create().\n</info added on 2025-11-24T21:52:09.110Z>",
            "status": "done",
            "testStrategy": "Test embedding generation with single text, test batch processing with 20+ texts, verify vector dimensions (1536), test error handling with invalid API credentials, mock OpenAI API responses for unit tests",
            "parentId": "undefined",
            "updatedAt": "2025-11-24T21:50:43.044Z"
          },
          {
            "id": 3,
            "title": "Implement index creation and document indexing functions",
            "description": "Create functions to create/update Azure AI Search index and index documents with embeddings",
            "dependencies": [
              1,
              2
            ],
            "details": "In search_service.py, create SearchService class with methods: create_or_update_index() to create index if not exists using SearchIndexClient.create_or_update_index(). Implement index_documents(documents: List[dict]) method that accepts documents with id, content, type, metadata fields, generates embeddings for content using generate_embeddings(), constructs search documents with embeddings vector, and uploads using SearchClient.upload_documents() with batch size of 100. Add delete_documents(document_ids: List[str]) for removing documents. Implement get_search_client() to return SearchClient instance for querying. Add logging for all operations.\n<info added on 2025-11-24T21:57:25.403Z>\nI need to analyze the codebase to understand the current implementation and provide accurate update information. Let me search for the relevant files and code.Based on my analysis of the search_service.py file, I can see that the implementation includes all five functions mentioned in the user request. Let me provide the appropriate update text:\n\nCompleted implementation in search_service.py lines 301-589: upload_document() method (lines 301-361) handles single document uploads with automatic embedding generation via generate_embedding(), accepts all required fields (doc_id, content, doc_type, title, metadata) plus optional fields (status, priority, category). upload_documents_batch() method (lines 363-428) performs efficient batch uploads by extracting all content first, calling generate_embeddings_batch() for parallel embedding generation, preparing documents with embeddings, uploading via search_client.upload_documents(), and returning success/failure counts. update_document() method (lines 430-483) supports selective field updates with parameters for content, title, metadata, status, priority, category, conditionally regenerates embeddings only when content changes, uses search_client.merge_documents() for partial updates. delete_document() method (lines 485-502) removes documents by ID using search_client.delete_documents(). search() method (lines 504-589) implements semantic vector search by generating query embeddings, creating VectorizedQuery with k_nearest_neighbors, supports optional doc_type filtering, returns formatted results with scores and parsed metadata JSON. All functions include comprehensive error handling with try-except blocks, detailed logging via logger.info() and logger.error(), and raise exceptions on failures. Implementation uses Azure Search SDK SearchClient for document operations and integrates with Azure OpenAI embeddings (text-embedding-ada-002, 1536 dimensions).\n</info added on 2025-11-24T21:57:25.403Z>",
            "status": "done",
            "testStrategy": "Test index creation with mock SearchIndexClient, verify document structure before upload, test batch uploading with 150+ documents, test delete_documents functionality, verify embedding vectors are correctly added to documents",
            "parentId": "undefined",
            "updatedAt": "2025-11-24T21:55:46.511Z"
          },
          {
            "id": 4,
            "title": "Implement data chunking and indexing for existing data",
            "description": "Create functions to chunk meeting transcripts and index existing meetings, tasks, agents, and governance data",
            "dependencies": [
              3
            ],
            "details": "In search_service.py, add async function chunk_text(text: str, chunk_size: int = 1000, overlap: int = 100) -> List[dict] that splits text into overlapping chunks with metadata preserved. Create async index_existing_data() function that: (1) queries Cosmos DB meetings container, chunks transcript_text into 1000-char segments, creates documents with type='meeting' and metadata={title, date, status}, (2) queries tasks container, creates documents with type='task' from title+description, metadata={title, status, priority}, (3) queries agents container, creates documents with type='agent' from name+description+use_case, (4) creates static governance documents with type='governance' from hardcoded policies (approval thresholds, agent tiers, meeting types). Batch all documents and call index_documents(). Add progress logging.",
            "status": "done",
            "testStrategy": "Test text chunking with 5000-char text, verify overlap logic, test indexing pipeline with mock Cosmos DB data, verify document types and metadata structure, test with empty containers",
            "parentId": "undefined",
            "updatedAt": "2025-11-24T22:21:23.537Z"
          },
          {
            "id": 5,
            "title": "Implement incremental indexing and integrate with application startup",
            "description": "Create incremental indexing function for new data and integrate search service initialization with FastAPI lifespan",
            "dependencies": [
              4
            ],
            "details": "In search_service.py, create async index_meeting(meeting: Meeting), async index_task(task: Task), async index_agent(agent: Agent) functions for incremental updates that generate embeddings and call index_documents() with single document. Add to main.py lifespan function: initialize SearchService instance, call create_or_update_index(), call index_existing_data() on first startup (add flag in Cosmos DB to track if initial indexing complete). Update POST/PUT endpoints in main.py for meetings (/api/meetings), tasks (/api/tasks), agents (/api/agents) to call respective incremental indexing functions after successful Cosmos DB operations. Add search_service global instance similar to db and ai_client patterns. Handle exceptions gracefully without blocking main operations.",
            "status": "done",
            "testStrategy": "Test incremental indexing functions with new meeting/task/agent objects, verify startup initialization in lifespan context, test endpoint integration with indexing calls, verify error handling doesn't break API operations, test with search service unavailable scenario",
            "parentId": "undefined",
            "updatedAt": "2025-11-24T22:37:02.255Z"
          }
        ],
        "updatedAt": "2025-11-24T22:37:02.255Z"
      },
      {
        "id": "6",
        "title": "Implement RAG Query Processing Backend",
        "description": "Build RAG implementation with Azure AI Search and Model Router for intelligent query answering",
        "details": "Create POST /api/agent/query endpoint accepting query string and optional conversation history. Search Azure AI Search for top 5 relevant results using vector similarity. Construct prompt with user query, retrieved context, conversation history (last 5 messages), and system instructions. Call Azure AI Foundry Model Router with constructed prompt. Return response with source citations including document IDs and relevance scores. Handle query types: meeting discussions, task assignments, agent status, budget information, governance policies.\n\n**[2025-11-25 Enhancement - Live CosmosDB Integration]**\nEnhanced endpoint with live CosmosDB data querying to fix stale data issue:\n1. Added `_detect_query_entities()` function - parses user queries using keyword matching to detect entity types (tasks, meetings, agents, proposals, decisions)\n2. Added `_get_live_context()` function - fetches live data directly from CosmosDB containers with formatted context strings showing status, priority, dates, assignments\n3. Updated endpoint flow: (a) Detect entity types from query, (b) Fetch live data from CosmosDB as PRIMARY context, (c) Fetch from semantic search for supplementary context, (d) Pass enriched context to AI\n4. Fixed 401 Azure AI Foundry authentication error by correcting endpoint format in root .env (changed from services.ai.azure.com to openai.azure.com)\n\n**Verified Results:**\n- Tasks query: Returns live 15 tasks from database (not stale search index)\n- Meetings query: Returns live 2 meetings from database\n- Agents query: Returns live 1 agent from database",
        "testStrategy": "Test various query types, verify search result relevance, validate prompt construction, test conversation history handling, verify source citations accuracy, verify live CosmosDB data is returned for tasks/meetings/agents queries",
        "priority": "high",
        "dependencies": [
          "5"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2025-11-25T19:55:00.000Z"
      },
      {
        "id": "7",
        "title": "Build AI Chat Interface Frontend with Floating Widget",
        "description": "Create comprehensive dual-interface AI assistant with full /guide page AND floating widget available across all pages. Features page-aware context, proactive suggestions, session-only memory, and read-only actions.",
        "details": "Implement complete front-end AI agent system with two interfaces:\n\n1. FLOATING WIDGET (across all pages):\n- GuideProvider context at RootLayout level for global state\n- GuideWidget component with FAB trigger (Sparkles icon, bottom-right)\n- GuideWidgetPanel with chat UI when expanded\n- Page-aware context detection via usePageContext hook\n- Proactive suggestions based on current page\n- Hidden on /guide page (full page takes over)\n\n2. FULL /guide PAGE (rewrite existing):\n- Refactor to use shared components\n- GuideChatMessage, GuideChatInput, GuideTypingIndicator extracted\n- GuideSuggestionBar with contextual suggestions\n- GuideSourceBadge with type-based colors\n- Full conversation interface with clear button\n\n3. SHARED INFRASTRUCTURE:\n- GuideProvider: Global state (messages, loading, widget open/closed)\n- useGuideChat: Conversation logic with context enrichment\n- usePageContext: Route detection and context mapping\n- useGuideSuggestions: Page-specific suggestion generator\n- guide-context.ts: Page-to-context mappings\n\nFiles to create:\n- frontend/components/guide/*.tsx (9 components)\n- frontend/components/providers/GuideProvider.tsx\n- frontend/hooks/useGuideChat.ts, usePageContext.ts, useGuideSuggestions.ts\n- frontend/lib/guide-context.ts\n\nFiles to modify:\n- frontend/app/layout.tsx (add GuideProvider + GuideWidget)\n- frontend/app/guide/page.tsx (rewrite to use shared components)\n\nAPI types already exist in api.ts (ChatMessage, ChatSource, GuideQueryResponse).",
        "testStrategy": "Test widget appears on all pages except /guide, verify suggestions update when navigating pages, test chat persists across page navigation (session), verify chat clears on page refresh, test sources display correctly with color coding, verify widget doesn't overlap dialogs (z-index), test responsive behavior on mobile, verify keyboard shortcut works, confirm /guide page uses same components as widget",
        "priority": "high",
        "dependencies": [
          "6"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create guide-context.ts with page-to-context mappings",
            "description": "Define PAGE_CONTEXT_MAP with route-to-context configurations including pageType, displayName, contextPrompt, and page-specific suggestions for all platform pages",
            "dependencies": [],
            "details": "Create frontend/lib/guide-context.ts with mappings for: /, /meetings, /tasks, /agents, /decisions, /governance, /budget, /resources, /tech-radar, /architecture. Each mapping includes pageType, displayName, contextPrompt (for AI context), and suggestions array.",
            "status": "done",
            "testStrategy": "Verify all routes have mappings, test dynamic route handling",
            "parentId": "undefined",
            "updatedAt": "2025-11-25T23:41:20.604Z"
          },
          {
            "id": 2,
            "title": "Create usePageContext and useGuideSuggestions hooks",
            "description": "Implement hooks for detecting current page context and generating contextual suggestions",
            "dependencies": [
              1
            ],
            "details": "Create frontend/hooks/usePageContext.ts using usePathname() to detect route and return PageContext. Create frontend/hooks/useGuideSuggestions.ts that uses page context to return relevant Suggestion[] array.",
            "status": "done",
            "testStrategy": "Test route detection, verify suggestions change per page",
            "parentId": "undefined",
            "updatedAt": "2025-11-25T23:41:26.539Z"
          },
          {
            "id": 3,
            "title": "Create GuideProvider context",
            "description": "Implement global context provider for guide state management",
            "dependencies": [
              2
            ],
            "details": "Create frontend/components/providers/GuideProvider.tsx with state for messages, isLoading, error, isWidgetOpen, isMinimized, currentPage, suggestions. Provide actions: sendMessage, clearConversation, toggleWidget.",
            "status": "done",
            "testStrategy": "Test state updates, verify context provides all required values",
            "parentId": "undefined",
            "updatedAt": "2025-11-25T23:41:32.741Z"
          },
          {
            "id": 4,
            "title": "Create shared chat components",
            "description": "Extract reusable components: GuideChatMessage, GuideChatInput, GuideTypingIndicator, GuideSourceBadge, GuideSuggestionChip, GuideSuggestionBar",
            "dependencies": [],
            "details": "Create frontend/components/guide/ directory with shared components. Reference existing patterns from guide/page.tsx for styling. Include barrel export index.ts.",
            "status": "done",
            "testStrategy": "Test component rendering, verify styling matches design",
            "parentId": "undefined",
            "updatedAt": "2025-11-25T23:41:39.041Z"
          },
          {
            "id": 5,
            "title": "Create GuideWidget components",
            "description": "Build floating widget with GuideWidgetTrigger (FAB), GuideWidgetPanel (chat), and GuideWidget (orchestrator)",
            "dependencies": [
              3,
              4
            ],
            "details": "Create GuideWidgetTrigger.tsx (Sparkles FAB, bottom-6 right-6, z-40). Create GuideWidgetPanel.tsx (w-96, h-[500px] chat panel). Create GuideWidget.tsx orchestrator that manages widget states and conditionally hides on /guide page.",
            "status": "done",
            "testStrategy": "Test widget toggle, verify positioning, test conditional rendering",
            "parentId": "undefined",
            "updatedAt": "2025-11-25T23:41:46.708Z"
          },
          {
            "id": 6,
            "title": "Create useGuideChat hook with context enrichment",
            "description": "Implement chat logic hook that enriches queries with page context before sending to API",
            "dependencies": [
              2,
              3
            ],
            "details": "Create frontend/hooks/useGuideChat.ts that builds contextString from pageContext (displayName, pageType, contextPrompt), calls api.guide.query with enriched context, and manages message state.",
            "status": "done",
            "testStrategy": "Test context enrichment, verify API calls include context",
            "parentId": "undefined",
            "updatedAt": "2025-11-25T23:41:57.155Z"
          },
          {
            "id": 7,
            "title": "Integrate GuideProvider and GuideWidget into layout.tsx",
            "description": "Add global provider and widget to application layout",
            "dependencies": [
              3,
              5
            ],
            "details": "Modify frontend/app/layout.tsx to wrap app in GuideProvider and add GuideWidget as sibling to main content area.",
            "status": "done",
            "testStrategy": "Verify widget appears on all pages, test provider context available",
            "parentId": "undefined",
            "updatedAt": "2025-11-25T23:42:02.733Z"
          },
          {
            "id": 8,
            "title": "Rewrite guide/page.tsx to use shared components",
            "description": "Refactor existing guide page to use new shared components and GuideProvider",
            "dependencies": [
              4,
              6
            ],
            "details": "Delete existing guide/page.tsx and rewrite using GuideChatMessage, GuideChatInput, GuideSuggestionBar, etc. Consume state from GuideProvider context. Keep full-page layout design.",
            "status": "done",
            "testStrategy": "Test full page functionality, verify component integration",
            "parentId": "undefined",
            "updatedAt": "2025-11-25T23:42:07.915Z"
          },
          {
            "id": 9,
            "title": "Add keyboard shortcut (Cmd/Ctrl+K)",
            "description": "Implement keyboard shortcut to focus/open the guide widget",
            "dependencies": [
              7
            ],
            "details": "Add event listener in GuideWidget for Cmd+K (Mac) / Ctrl+K (Windows) to toggle widget open and focus input.",
            "status": "done",
            "testStrategy": "Test keyboard shortcut on Mac and Windows",
            "parentId": "undefined",
            "updatedAt": "2025-11-25T23:42:13.390Z"
          },
          {
            "id": 10,
            "title": "Testing and polish",
            "description": "Add animations, responsive adjustments, and comprehensive testing",
            "dependencies": [
              8,
              9
            ],
            "details": "Add expand/collapse animations. Test responsive behavior (mobile full-screen). Cross-page navigation testing. Verify z-index doesn't conflict with dialogs.",
            "status": "done",
            "testStrategy": "Full testing checklist from plan",
            "parentId": "undefined",
            "updatedAt": "2025-11-25T23:42:20.204Z"
          },
          {
            "id": 11,
            "title": "Phase 1.1: Remove hardcoded limit=15 for AGGREGATION queries",
            "description": "Modify query_agent() to use intent-aware limits: AGGREGATION queries get no limit (need all items for accurate counts), LIST queries get increased limit of 50",
            "details": "Implemented intent-aware limits in _get_live_context_with_intent(). AGGREGATION queries now fetch all items for accurate counts.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7,
            "updatedAt": "2025-11-26T02:00:00.000Z",
            "parentId": "undefined"
          },
          {
            "id": 12,
            "title": "Phase 1.2: Add showing X of Y total context",
            "description": "Add total count to entity results context so AI knows partial vs full data. Format: TASKS (showing 15 of 47 total)",
            "details": "Added DataBasis tracking with items_shown, total_items, entity_types. Context now shows 'showing X of Y total' format.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7,
            "updatedAt": "2025-11-26T02:00:00.000Z",
            "parentId": "undefined"
          },
          {
            "id": 13,
            "title": "Phase 2.1: Update frontend api.guide.query to accept page_context",
            "description": "Modify api.ts guide.query() to accept optional page_context parameter with current_page, selected_ids, active_filters, visible_entity_type",
            "details": "Added DataBasis interface and updated GuideQueryResponse and ChatMessage interfaces in api.ts.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7,
            "updatedAt": "2025-11-26T02:00:00.000Z",
            "parentId": "undefined"
          },
          {
            "id": 14,
            "title": "Phase 2.2: Update GuideProvider to pass page_context to backend",
            "description": "Modify GuideProvider.tsx sendMessage() to construct and pass page_context object including pathname, filters from URL params or component state",
            "details": "Updated GuideProvider.tsx to import DataBasis and pass data_basis from response to assistant messages.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7,
            "updatedAt": "2025-11-26T02:00:00.000Z",
            "parentId": "undefined"
          },
          {
            "id": 15,
            "title": "Phase 3.1: Add case-insensitive status normalization",
            "description": "Normalize status values with .strip() and case handling to prevent mismatched aggregation counts",
            "details": "Implemented status normalization in backend with .strip().lower() and title case output for consistent aggregation counts.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7,
            "updatedAt": "2025-11-26T02:00:00.000Z",
            "parentId": "undefined"
          },
          {
            "id": 16,
            "title": "Phase 3.2: Add AI data scope instructions to context",
            "description": "Add explicit DATA SCOPE NOTICE to AI context explaining it has partial data and for aggregation queries all matching items were retrieved for accurate counts",
            "details": "Added DATA SCOPE NOTICE to context explaining data basis and accurate counts for aggregation queries.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7,
            "updatedAt": "2025-11-26T02:00:00.000Z",
            "parentId": "undefined"
          },
          {
            "id": 17,
            "title": "Phase 4.1: Real-time count queries",
            "description": "Run separate COUNT(*) CosmosDB queries to get accurate totals even when limiting result items",
            "details": "Implemented real-time COUNT(*) queries in CosmosDB. Backend returns data_basis with items_shown and total_items.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7,
            "updatedAt": "2025-11-26T02:00:00.000Z",
            "parentId": "undefined"
          },
          {
            "id": 18,
            "title": "Phase 4.2: Confidence indicators in UI",
            "description": "Show Based on X items indicator in AI Guide responses for transparency",
            "details": "Added confidence indicator in GuideChatMessage.tsx with Database icon showing 'Based on X items' or 'Based on X of Y items'.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7,
            "updatedAt": "2025-11-26T02:00:00.000Z",
            "parentId": "undefined"
          },
          {
            "id": 19,
            "title": "Phase 4.3: View sync button",
            "description": "Add Sync with current page view button to align AI Guide context with user's current filtered view",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 7,
            "parentId": "undefined"
          },
          {
            "id": 20,
            "title": "Phase 4.4: Query validation logging",
            "description": "Add debug logging to compare AI reported counts vs actual CosmosDB counts for diagnostics",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 7,
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-03T03:52:31.378Z"
      },
      {
        "id": "8",
        "title": "Create Resources Library Backend",
        "description": "Build Azure Cloud Asset Inventory backend for real-time Azure resource tracking using Azure Resource Graph API, Cost Management API, and Resource Manager API",
        "status": "done",
        "dependencies": [],
        "priority": "medium",
        "details": "Create endpoints: GET /api/resources/azure-assets for live infrastructure tracking. Display AI services (Foundry, OpenAI, AI Search), data storage (Cosmos DB, Blob), compute (App Services, Container Apps), and security resources (Key Vault, Log Analytics). Show resource cards with name, type, status, location, cost, and tags. Implement Azure Resource Graph API integration for resource discovery, Cost Management API for pricing data, and Resource Manager API for detailed resource information. Use Managed Identity for authentication. Implement caching: 5 minutes for resource data, 1 hour for cost data. Store cached data in memory or Redis. See detailed specifications in Resource-Library-Enhancement-Proposal.md Section 1.",
        "testStrategy": "Test Azure API integrations (Resource Graph, Cost Management, Resource Manager), verify Managed Identity authentication, test caching mechanisms and expiration, validate resource data accuracy, test cost data retrieval, verify real-time updates",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up Azure API clients and authentication",
            "description": "Configure Azure Resource Graph, Cost Management, and Resource Manager API clients with Managed Identity authentication",
            "dependencies": [],
            "details": "Install required Azure SDK packages (@azure/arm-resourcegraph, @azure/arm-costmanagement, @azure/arm-resources). Set up DefaultAzureCredential for Managed Identity authentication. Create service classes for each API client with proper error handling and retry logic. Configure required Azure permissions for the managed identity to access resource data and cost information.",
            "status": "pending",
            "testStrategy": "Test authentication with Managed Identity, verify API client initialization, test error handling for authentication failures",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Azure Resource Graph integration",
            "description": "Build service to query Azure resources using Resource Graph API with filtering for relevant resource types",
            "dependencies": [
              1
            ],
            "details": "Create ResourceGraphService to query Azure resources. Implement KQL queries to filter for AI services (Foundry, OpenAI, AI Search), data storage (Cosmos DB, Blob Storage), compute resources (App Services, Container Apps), and security resources (Key Vault, Log Analytics). Extract resource properties: name, type, status, location, tags, resource group. Handle pagination for large result sets and implement proper error handling.",
            "status": "pending",
            "testStrategy": "Test KQL query execution, verify resource filtering accuracy, test pagination handling, validate extracted resource properties",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Cost Management API integration",
            "description": "Build service to retrieve cost data for Azure resources using Cost Management API",
            "dependencies": [
              1
            ],
            "details": "Create CostManagementService to query resource costs. Implement cost queries for individual resources and resource groups. Handle different cost metrics (actual, forecast, budgeted). Map cost data to corresponding resources from Resource Graph. Implement proper date range handling and currency formatting. Handle API rate limits and implement exponential backoff.",
            "status": "pending",
            "testStrategy": "Test cost data retrieval, verify cost-to-resource mapping, test date range filtering, validate currency formatting and rate limit handling",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement caching mechanism",
            "description": "Build caching system with different TTL for resource data (5 min) and cost data (1 hour)",
            "dependencies": [
              2,
              3
            ],
            "details": "Implement in-memory cache or Redis integration for storing Azure resource and cost data. Set up different cache keys and TTL values: 5 minutes for resource data, 1 hour for cost data. Implement cache invalidation strategies and cache warming. Add cache hit/miss metrics for monitoring. Handle cache failures gracefully with fallback to direct API calls.",
            "status": "pending",
            "testStrategy": "Test cache storage and retrieval, verify TTL expiration, test cache invalidation, validate fallback mechanisms",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Create Azure assets API endpoint",
            "description": "Build GET /api/resources/azure-assets endpoint to serve real-time Azure resource data",
            "dependencies": [
              4
            ],
            "details": "Create API endpoint that combines data from ResourceGraphService and CostManagementService. Implement response formatting to include resource cards with name, type, status, location, cost, and tags. Add query parameters for filtering by resource type, location, and resource group. Implement proper error handling and response caching headers. Add OpenAPI documentation for the endpoint.",
            "status": "pending",
            "testStrategy": "Test endpoint response format, verify data combination from multiple services, test filtering parameters, validate error responses and caching headers",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-11-25T16:57:49.661Z"
      },
      {
        "id": "9",
        "title": "Build Resources Library Frontend",
        "description": "Create Educational Resource Library with preview functionality - a document and link management system with rich preview capabilities supporting PDFs, markdown, Office docs, and web links",
        "status": "done",
        "dependencies": [],
        "priority": "medium",
        "details": "Build comprehensive resource library frontend with: 1) Document upload wizard with Azure Blob Storage integration and thumbnail generation, 2) OpenGraph metadata fetching for web links, 3) Preview modal system showing 80% of content without leaving page, 4) Full-text search across title/description/tags/content, 5) Category system (Agent, Governance, Technical, Licensing, AI Architect, Architecture, Budget, Security), 6) Multi-tag support with advanced filtering, 7) Usage analytics display (view count, last accessed). Backend COMPLETE: resource_library_service.py with Azure Blob Storage integration, OpenGraph metadata extraction, PDF text extraction/thumbnail generation. Routers/resources.py with 7 endpoints (POST /create, GET /list with filtering, GET /{id}, PUT /{id}, DELETE /{id}, POST /{id}/view, GET /categories/list, GET /tags/list). Frontend COMPLETE: frontend/app/resources/library/page.tsx with responsive grid layout, search/filter UI, category descriptions with tooltips, preview modal (max-w-6xl, split-pane), usage analytics. Resource model validated with ResourceCategory enum. 8 test resources created covering all categories with working OpenGraph metadata. REMAINING: Upload wizard UI implementation (backend ready, placeholder dialog exists at page.tsx:498-509).",
        "testStrategy": "Test document upload and thumbnail generation, verify OpenGraph metadata fetching for links, test preview modal functionality across different file types, validate full-text search capabilities, test category and tag filtering, verify usage analytics tracking, test file download and preview links",
        "subtasks": [
          {
            "id": 2,
            "title": "Implement Document Upload Wizard",
            "description": "Create multi-step upload dialog supporting file uploads and URL links with metadata input",
            "dependencies": [],
            "details": "Build upload wizard replacing placeholder dialog at frontend/app/resources/library/page.tsx:498-509. Backend ready: POST /api/resources endpoint (routers/resources.py:19-127) accepts multipart form-data (file: UploadFile, url: str, title: str, description: str, category: str, tags: str[], uploaded_by: str). Supports both file upload and URL link creation (mutually exclusive). Backend validates ResourceCategory enum (Agent|Governance|Technical|Licensing|AI Architect|Architecture|Budget|Security). File upload flow: resource_library_service.py uploads to Azure Blob Storage, generates thumbnails for PDFs, extracts preview text and page count. URL link flow: fetches OpenGraph metadata (og_title, og_description, og_image) via resource_service.fetch_og_metadata(). Frontend implementation needed: Multi-step wizard with 1) Type selection (Document Upload vs Web Link), 2) File picker/URL input, 3) Metadata form (title required, description, category dropdown with CATEGORY_DESCRIPTIONS tooltips, tags multi-input, uploaded_by defaulting to current user). Use Shadcn UI Dialog, Form, Input, Select, Textarea components. Show upload progress for files. Display OpenGraph preview for URLs. On success, refresh resources list and close dialog.\n<info added on 2025-11-25T17:16:15.157Z>\nI'll analyze the codebase to understand the current implementation and provide an accurate update based on the user request.Based on my analysis of the codebase, I can see the upload wizard has been fully implemented at `frontend/app/resources/library/page.tsx` (lines 590-758). The implementation includes all the features mentioned in the user request. Here's the update text:\n\nImplementation completed at frontend/app/resources/library/page.tsx:590-758. Wizard uses Shadcn Dialog component with Tabs for upload type selection (Web Link/File Upload via TabsList at line 603). File input (line 629-639) accepts .pdf,.doc,.docx,.txt,.png,.jpg,.jpeg with 50MB guidance. URL input (line 610-623) includes helper text for metadata extraction. Form fields include: required title (line 646-656), optional description (line 659-668), required category Select dropdown populated from API (line 672-684), tags with add/remove functionality using Badge components (line 687-722). Client-side validation checks: title required (line 219), category required (line 224), file presence for file uploads (line 229), URL presence for web links (line 234). Form submission creates FormData object (line 242-254) appending file/url, title, description, category, JSON-stringified tags array, and uploaded_by. POST request to /api/resources/ at line 256-259. Loading state managed via uploading boolean (line 105) displaying spinner on submit button (line 743-748). Error handling displays uploadError state in red alert box (line 726-730). Success flow: calls loadResources() to refresh list (line 266), closes modal via setUploadOpen(false) (line 267), resets form via resetUploadForm() (line 268). Cancel button and dialog close both trigger resetUploadForm() (lines 592-594, 737-740). FormData format matches backend multipart/form-data expectations from routers/resources.py endpoint.\n</info added on 2025-11-25T17:16:15.157Z>",
            "status": "done",
            "testStrategy": "Test file upload functionality, verify URL link processing, test form validation, verify Azure Blob Storage integration",
            "parentId": "undefined",
            "updatedAt": "2025-11-25T22:16:44.270Z"
          },
          {
            "id": 4,
            "title": "Implement OpenGraph Metadata Fetching",
            "description": "Add functionality to automatically fetch and display metadata for web links",
            "dependencies": [],
            "details": "Backend implementation COMPLETE at resource_library_service.py:130-166 with fetch_og_metadata() method using requests and BeautifulSoup to extract OpenGraph tags (og:title, og:description, og:image, og:site_name) from HTML meta tags. Integration COMPLETE in routers/resources.py:95-113 where POST /api/resources endpoint calls fetch_og_metadata(url) for link-type resources and stores og_title, og_description, og_image in database. Frontend display COMPLETE in library/page.tsx showing og_image in resource cards (lines 323-328) and preview modal (lines 424-429), og_description as fallback text (lines 347, 436-440). Working verification: 8 test resources include Microsoft Learn and GitHub links with successfully extracted OpenGraph metadata. Fallback handling implemented: uses basic URL and filename if OpenGraph data unavailable.",
            "status": "done",
            "testStrategy": "Test OpenGraph metadata extraction, verify fallback handling, test link preview display",
            "parentId": "undefined"
          },
          {
            "id": 8,
            "title": "Create Resource Management Actions",
            "description": "Implement edit, delete, and administrative actions for resources",
            "dependencies": [],
            "details": "Backend implementation COMPLETE with PUT /api/resources/{id} endpoint (routers/resources.py:190-247) for updating resource metadata (title, description, category, tags). DELETE /api/resources/{id} endpoint (lines 249-269) removes resource from database and calls resource_service.delete_resource_files() to clean up Azure Blob Storage files. Frontend implementation NOT STARTED - no edit dialog, delete confirmation, or administrative actions exist in library/page.tsx. Resource cards are view-only without edit/delete buttons. Permission-based access controls NOT implemented - all endpoints currently lack authentication/authorization. Bulk operations NOT implemented. Resource sharing and collaboration features NOT implemented. Implementation needed: Add edit/delete action buttons to resource cards or preview modal, create edit dialog with form pre-populated from selected resource, add delete confirmation dialog, integrate with PUT/DELETE endpoints, implement user role checking to show/hide admin actions.",
            "status": "done",
            "testStrategy": "Test edit functionality, verify delete confirmation, test permission controls, test bulk operations",
            "parentId": "undefined",
            "updatedAt": "2025-11-25T22:16:49.693Z"
          },
          {
            "id": 1,
            "title": "Create Resource Library Main Page Component",
            "description": "Build the main /app/resources/page.tsx with grid layout for resource cards, search bar, and category/tag filters",
            "dependencies": [],
            "details": "Create responsive grid layout displaying resource cards with thumbnails, title, description, category badges, tags, upload date, and uploaded by information. Implement search bar with live filtering and dropdown filters for categories and tags. Use Shadcn UI Card, Input, Select, and Badge components.\n<info added on 2025-11-25T02:01:46.677Z>\nI'll analyze the codebase first to understand the current implementation and provide an accurate update.Implementation completed in frontend/app/resources/library/page.tsx with functional React component featuring search and filter UI, resource card grid layout with responsive columns (md:grid-cols-2 lg:grid-cols-3), empty state handling, preview modal with metadata panel, and placeholder upload dialog. Component integrates Shadcn UI components including Card, Input, Select, Badge, Dialog, Tooltip with TooltipProvider wrapper for category descriptions. CATEGORY_DESCRIPTIONS object includes detailed explanations for 8 categories: Agent, Governance, Technical, Licensing, AI Architect, Architecture, Budget, Security. Resource interface defines comprehensive type schema supporting PDF/Document/Link/Video/Image types with document fields (file_type, file_size_bytes, blob_url, thumbnail_url, preview_text, page_count), link fields (url, og_title, og_description, og_image), and metadata (uploaded_by, uploaded_at, view_count). Filtering implemented via live search bar with Search icon, category dropdown with tooltips displaying descriptions on hover, tag filtering with selected tags displayed as removable badges, popular tags section showing top 10 tags with counts. Resource cards display thumbnail/og_image fallback to icon, title with type badge, description with line clamping, metadata (page count, file size, view count), and first 3 tags with +N indicator. 8 test resources created and loaded via /api/resources/ endpoint covering all categories. Full API integration implemented for GET /api/resources/, /api/resources/categories/list, /api/resources/tags/list endpoints with query parameter filtering (search, category, tags). Preview modal features split layout with preview area (left 2fr) and metadata panel (right 1fr), external link and download buttons, image display, and complete resource details display.\n</info added on 2025-11-25T02:01:46.677Z>",
            "status": "done",
            "testStrategy": "Test grid layout responsiveness, verify search functionality, test category and tag filtering",
            "parentId": "undefined",
            "updatedAt": "2025-11-25T02:02:13.253Z"
          },
          {
            "id": 3,
            "title": "Build Preview Modal System",
            "description": "Create preview modal that displays 80% of document content without leaving the main page",
            "dependencies": [],
            "details": "Implement modal component that can preview PDFs, markdown files, Office documents, and web link content. Show document preview with navigation controls, full-screen option, and download/view original buttons. Handle different file types with appropriate viewers and fallback options.\n<info added on 2025-11-25T02:05:40.078Z>\nI'll analyze the codebase to understand the current implementation and provide specific, contextual updates for the Preview Modal System.Based on my analysis of the codebase, I can see the preview modal is already implemented in `frontend/app/resources/library/page.tsx:382-496`. Here's the new implementation detail text to add:\n\nModal implementation completed at frontend/app/resources/library/page.tsx:382-496. Uses DialogContent with max-w-6xl width and h-[80vh] responsive height. Split-pane layout implemented using grid-cols-[2fr_1fr] at line 414 with left pane for content preview and right pane for metadata. Left pane displays thumbnail_url or og_image images (lines 417-430) and preview_text or og_description text content (lines 431-440). Right metadata panel (lines 444-491) shows resource details including type, category, pages, file size, view counts, and uploader information. Action buttons in DialogHeader (lines 389-410) include Open button for external URLs and Download button for blob_url files, both using ExternalLink and Download lucide-react icons. View count tracking implemented via handlePreview function (lines 158-168) which calls POST /api/resources/{id}/view endpoint when modal opens. Both panes use overflow-auto, border, rounded, and p-4 styling for consistent presentation.\n</info added on 2025-11-25T02:05:40.078Z>",
            "status": "done",
            "testStrategy": "Test preview functionality across different file types, verify modal navigation, test full-screen mode",
            "parentId": "undefined",
            "updatedAt": "2025-11-25T02:07:14.344Z"
          },
          {
            "id": 5,
            "title": "Add Full-Text Search Functionality",
            "description": "Implement comprehensive search across title, description, tags, and document content",
            "dependencies": [],
            "details": "Build search functionality that queries across all resource metadata and extracted document content. Implement search result highlighting, search suggestions, and advanced search filters. Integrate with backend search API for content indexing.",
            "status": "done",
            "testStrategy": "Test search across different content types, verify result highlighting, test search performance",
            "parentId": "undefined",
            "updatedAt": "2025-11-25T02:05:51.568Z"
          },
          {
            "id": 6,
            "title": "Implement Category and Tag Management",
            "description": "Create category system and multi-tag support with filtering capabilities",
            "dependencies": [],
            "details": "Implement predefined categories (Azure AI Foundry, Agent Framework, Governance, Licensing, Architecture, Meeting Materials, Research, Training) with color-coded badges. Add multi-tag support with tag autocomplete, tag creation, and advanced filtering options. Create tag management interface for administrators.",
            "status": "done",
            "testStrategy": "Test category assignment, verify tag autocomplete, test filtering combinations, test tag management interface",
            "parentId": "undefined",
            "updatedAt": "2025-11-25T02:06:08.575Z"
          },
          {
            "id": 7,
            "title": "Add Usage Analytics Display",
            "description": "Implement usage tracking display showing view counts and last accessed information",
            "dependencies": [],
            "details": "Add usage analytics components to resource cards and detail views showing view count, last accessed date, and trending indicators. Create analytics dashboard for administrators with usage statistics and popular resources. Implement privacy-compliant tracking.",
            "status": "done",
            "testStrategy": "Test analytics data display, verify tracking accuracy, test analytics dashboard functionality",
            "parentId": "undefined",
            "updatedAt": "2025-11-25T02:06:26.028Z"
          }
        ],
        "updatedAt": "2025-11-25T22:16:49.693Z"
      },
      {
        "id": "10",
        "title": "Implement Tech Radar System",
        "description": "Create technology tracking system with 4-category grid layout (Adopt, Trial, Assess, Hold)",
        "details": "Create backend endpoints: GET /api/tech-radar, POST /api/tech-radar, PUT /api/tech-radar/{id}, DELETE /api/tech-radar/{id}. Store in Cosmos DB 'tech_radar_items' collection: id, tool_name, category (Adopt|Trial|Assess|Hold), description, team_using, date_added, last_reviewed, link. Create /app/tech-radar/page.tsx with 4-column grid layout, one column per category. Each card shows tool name, description, team using, last reviewed date, external link. Add create dialog with tool name, category dropdown, description, team using, optional link. Display category descriptions: Adopt (proven), Trial (worth pursuing), Assess (worth exploring), Hold (proceed with caution).",
        "testStrategy": "Test CRUD operations for tech items, verify 4-column grid layout, test category filtering, validate external links, test item creation and editing",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [],
        "updatedAt": "2025-11-26T00:11:08.490Z"
      },
      {
        "id": "11",
        "title": "Build Architecture Lab Knowledge Base",
        "description": "Create code patterns and deployment guides management system with syntax highlighting",
        "details": "Create endpoints for code patterns: GET /api/code-patterns, POST /api/code-patterns, PUT /api/code-patterns/{id}, DELETE /api/code-patterns/{id}. Store in 'code_patterns' collection: id, title, pattern_type (Code-First|Low-Code|Hybrid), description, code_snippet, language, use_case. Create deployment guides endpoints with 'deployment_guides' collection: id, title, platform, guide_content (markdown), prerequisites[], steps[]. Create /app/architecture/page.tsx with two-tab interface. Code Patterns tab: grid of cards with syntax highlighting using react-syntax-highlighter, copy code button. Deployment Guides tab: markdown rendering using react-markdown, prerequisites checklist, step-by-step instructions.",
        "testStrategy": "Test code pattern CRUD operations, verify syntax highlighting, test markdown rendering, validate copy code functionality, test deployment guide creation",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "12",
        "title": "Add Edit/Delete Operations to All Entities",
        "description": "Implement full CRUD functionality across meetings, tasks, agents, and decisions",
        "details": "Add PUT endpoints for /api/meetings/{id}, /api/tasks/{id}, /api/agents/{id}, /api/decisions/{id} accepting partial updates. Add DELETE endpoints with soft delete for meetings/tasks, hard delete for others. Check dependencies before deletion. Update frontend components: add edit buttons to entity cards, reuse create dialogs with pre-filled data, add delete buttons with confirmation dialogs ('Are you sure you want to delete [entity name]?'). Implement optimistic UI updates, call PUT/DELETE endpoints, show success toasts. Add 'View Details' functionality with expanded dialogs showing all entity fields and related data.",
        "testStrategy": "Test edit functionality for all entity types, verify delete confirmations, test optimistic UI updates, validate dependency checking, test detail views",
        "priority": "medium",
        "dependencies": [
          "4",
          "7",
          "9",
          "10",
          "11"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "13",
        "title": "Implement Microsoft SSO Authentication with Entra ID to Replace Shared API Key and Enable User Identification, Task Assignment, and Role-Based Access Control",
        "description": "Integrate Microsoft Entra ID for SSO authentication using MSAL (Microsoft Authentication Library) with OAuth 2.0/OIDC, replacing the shared API key, and enable proper user identification, task assignment, and role-based access control.",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "details": "This is a foundational authentication task that can be worked on independently in parallel with other features. Implementation uses MSAL with OAuth 2.0/OIDC instead of SAML for modern authentication flow.\n\n**Key Components:**\n1. **Azure App Registration with MSAL Configuration**:\n   - Register application in Microsoft Entra admin center\n   - Configure OAuth 2.0/OIDC settings with appropriate redirect URIs\n   - Set up API permissions and scopes for user authentication\n   - Generate client ID and configure tenant settings\n\n2. **Backend: Python MSAL Integration with JWT Token Validation**:\n   - Implement MSAL authentication flow in Python backend\n   - Add JWT token validation middleware for all API endpoints\n   - Extract user claims (email, name, roles) from JWT tokens\n   - Replace shared API key authentication with Bearer token validation\n\n3. **Frontend: @azure/msal-react Integration**:\n   - Integrate @azure/msal-react library for React frontend\n   - Implement login/logout components and authentication context\n   - Handle token acquisition and renewal automatically\n   - Protect routes based on authentication state\n\n4. **User Model in Cosmos DB**:\n   - Create user collection in Cosmos DB to store user profiles\n   - Map Entra ID user identities to application users\n   - Store user roles and permissions for RBAC implementation\n   - Sync user data from Entra ID claims\n\n5. **Replace Shared API Key with Bearer Token Authentication**:\n   - Remove all shared API key references from codebase\n   - Update all API endpoints to require Bearer token authentication\n   - Implement proper error handling for authentication failures\n   - Ensure secure token storage and transmission\n\nRefer to detailed implementation guide at .taskmaster/docs/microsoft-sso-auth-prd.md for complete specifications.",
        "testStrategy": "1. Test OAuth 2.0/OIDC authentication flow with multiple user accounts and roles\n2. Verify JWT token validation and Bearer token authentication on all API endpoints\n3. Test @azure/msal-react integration including login/logout and token renewal\n4. Validate user model creation and synchronization with Entra ID\n5. Confirm complete removal of shared API key authentication\n6. Test role-based access control and task assignment with authenticated users\n7. Perform end-to-end authentication flow testing including error handling\n8. Test token expiration and refresh scenarios\n9. Validate secure token storage and transmission",
        "subtasks": [
          {
            "id": 1,
            "title": "Azure App Registration and MSAL Configuration",
            "description": "Register application in Microsoft Entra admin center and configure MSAL settings for OAuth 2.0/OIDC authentication flow.",
            "dependencies": [],
            "details": "Register new application in Microsoft Entra admin center with appropriate name and configuration. Set up OAuth 2.0/OIDC settings including redirect URIs for both development and production environments. Configure API permissions for user authentication and profile access. Generate client ID and configure tenant-specific settings. Document all configuration values needed for backend and frontend integration.\n<info added on 2025-11-24T21:18:30.763Z>\nI'll analyze the codebase to understand the project structure and provide context-specific information about this blocker.BLOCKED: Requires Azure Application Administrator role for Entra ID app registration. User does not have admin access. Need to coordinate with Azure admin or reassign task.\n\n---\n\n**Reference Documentation:**\n- Task 13.1 setup guide: `.taskmaster/docs/task-13.1-azure-setup-guide.md`\n- Troubleshooting note: Lines 196-207 confirm Azure Application Administrator or Cloud Application Administrator role required\n\n**Coordination Points:**\n- Developer B coordination doc: `.taskmaster/docs/developer-b-coordination.md`\n- Integration timeline: Phase 1 (Independent Development) - Task 13.1 must complete before 13.2 (Backend MSAL Integration)\n- Downstream impact: Tasks 13.2-13.6 dependent on app registration completion\n\n**Recommended Actions:**\n1. Contact Fourth Limited Azure administrator to request:\n   - Application Administrator role assignment (temporary)\n   - OR: Admin performs app registration following guide at `.taskmaster/docs/task-13.1-azure-setup-guide.md`\n   - Provide client ID, tenant ID, and client secret back to Developer B\n\n2. Alternative: Escalate to David Hayes (david.hayes@fourth.com) - listed as Admin role in microsoft-sso-auth-prd.md:266\n\n3. If blocker persists >24 hours: Consider reassigning Task 13 to team member with existing Azure admin privileges\n\n**Parallel Work Options:**\n- While blocked, Developer B could begin documentation work for Task 13.2 (Backend Python MSAL Integration)\n- Review MSAL library documentation\n- Prepare environment variable templates\n</info added on 2025-11-24T21:18:30.763Z>",
            "status": "in-progress",
            "testStrategy": "Verify app registration is accessible, test redirect URIs work correctly, validate API permissions are properly configured",
            "parentId": "undefined",
            "updatedAt": "2025-11-24T21:04:54.159Z"
          },
          {
            "id": 2,
            "title": "Backend Python MSAL Integration",
            "description": "Implement MSAL authentication in Python backend with JWT token validation middleware for all API endpoints.",
            "dependencies": [
              1
            ],
            "details": "Install and configure python MSAL library for backend authentication. Create JWT token validation middleware that validates Bearer tokens on all API requests. Implement token verification using Microsoft's public keys and validate token claims. Extract user information (email, name, roles) from validated JWT tokens. Create authentication decorators for protecting API endpoints. Handle token expiration and refresh scenarios appropriately.",
            "status": "pending",
            "testStrategy": "Test JWT token validation with valid and invalid tokens, verify middleware protects all endpoints, test user claim extraction accuracy",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Frontend @azure/msal-react Integration",
            "description": "Integrate @azure/msal-react library in React frontend for seamless authentication experience with login/logout components.",
            "dependencies": [
              1
            ],
            "details": "Install @azure/msal-react and @azure/msal-browser packages. Configure MSAL instance with client ID and tenant information from Azure app registration. Create authentication context provider to manage login state across the application. Implement login and logout components with proper error handling. Add route protection for authenticated-only pages. Handle token acquisition and automatic renewal. Implement proper loading states during authentication flows.",
            "status": "pending",
            "testStrategy": "Test login/logout functionality, verify token acquisition and renewal, test route protection, validate error handling scenarios",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "User Model and Cosmos DB Integration",
            "description": "Create user collection in Cosmos DB and implement user profile management with Entra ID synchronization.",
            "dependencies": [
              2
            ],
            "details": "Design user document schema for Cosmos DB including fields for user ID, email, name, roles, permissions, and Entra ID mapping. Create user collection with appropriate indexing for efficient queries. Implement user creation and update logic that syncs with Entra ID claims. Add user profile endpoints for retrieving and updating user information. Implement role-based access control data structures. Handle user first-time login scenarios with automatic profile creation.",
            "status": "pending",
            "testStrategy": "Test user profile creation and updates, verify Entra ID synchronization, test role assignment and RBAC functionality",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Replace Shared API Key Authentication",
            "description": "Remove all shared API key references and update all endpoints to use Bearer token authentication exclusively.",
            "dependencies": [
              2,
              4
            ],
            "details": "Audit entire codebase to identify and remove all shared API key authentication logic. Update all API endpoints to require Bearer token authentication instead of API key. Remove API key configuration from environment variables and deployment scripts. Update API documentation to reflect Bearer token authentication requirements. Implement proper HTTP 401 responses for unauthenticated requests. Ensure backward compatibility during transition period if needed.",
            "status": "pending",
            "testStrategy": "Verify no API key authentication remains in codebase, test all endpoints require Bearer tokens, validate proper error responses for unauthenticated requests",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "End-to-End Authentication Testing and Validation",
            "description": "Comprehensive testing of the complete authentication flow including edge cases and error scenarios.",
            "dependencies": [
              3,
              5
            ],
            "details": "Create comprehensive test suite covering the entire authentication flow from login to API access. Test multiple user accounts with different roles and permissions. Validate token expiration and refresh scenarios. Test error handling for network failures, invalid tokens, and authentication failures. Perform security testing to ensure tokens are properly secured. Test concurrent user sessions and token management. Validate performance impact of authentication middleware.",
            "status": "pending",
            "testStrategy": "Execute full authentication flow tests, validate security measures, test performance under load, verify error handling completeness",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Configure Custom Domain for Azure Static Web App",
            "description": "Set up custom domain (e.g., agent-arch.fourth.com) for the frontend Azure Static Web App to replace auto-generated URL.",
            "dependencies": [
              1
            ],
            "details": "**Requires IT/Admin action:**\n1. Request DNS CNAME record: agent-arch.fourth.com -> witty-coast-0e5f72203.3.azurestaticapps.net\n2. After DNS propagation, run: az staticwebapp hostname set --name agent-arch-web-prod --hostname agent-arch.fourth.com\n3. Azure provides free SSL certificate automatically\n4. Update frontend redirect URIs in Entra app registration\n\n**Current URL:** witty-coast-0e5f72203.3.azurestaticapps.net\n**Target:** agent-arch.fourth.com (or similar)",
            "status": "blocked",
            "testStrategy": "Verify DNS resolution, test HTTPS certificate, validate redirect URIs",
            "parentId": "undefined"
          },
          {
            "id": 8,
            "title": "Configure Production Redirect URIs in Entra App Registration",
            "description": "Add production Azure Static Web App domain(s) as valid redirect URIs in the Entra ID app registration.",
            "dependencies": [
              1,
              7
            ],
            "details": "**Requires IT/Admin action:**\n1. Microsoft Entra ID -> App registrations -> Fourth AI Portal - Agent Architecture Guide\n2. Authentication -> Add production redirect URIs:\n   - https://witty-coast-0e5f72203.3.azurestaticapps.net/auth/callback\n   - https://agent-arch.fourth.com/auth/callback (if custom domain)\n3. Click Save",
            "status": "blocked",
            "testStrategy": "Test OAuth login flow redirects correctly to production URLs",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-05T01:00:34.965Z"
      },
      {
        "id": "14",
        "title": "Coordinate Backend Restart on Port 8000 to Load New Transcript Routes",
        "description": "Coordinate with Developers B and C to restart the backend server on port 8000 to activate new transcript-related API routes after their current work is complete.",
        "details": "1. Confirm with Developers B and C that they have completed their current tasks and are ready for the backend restart to avoid disrupting ongoing work.\n2. Identify and terminate all processes currently using port 8000 to free the port. Use system tools such as 'netstat', 'lsof', or platform-specific commands to find and kill these processes safely.\n3. Start the backend server afresh on port 8000, ensuring it runs the updated version that includes the new transcript routes: /api/transcripts/upload and /api/transcripts/process.\n4. Verify the availability and correct functioning of the new transcript API routes by sending test requests and checking for expected responses.\n5. Notify the entire development team upon successful restart and route verification, so they can proceed with dependent tasks.\n\nConsiderations:\n- Coordinate timing carefully to minimize downtime.\n- Use logging to capture restart events and any errors.\n- Follow best practices for safely killing processes to avoid data loss.\n- Confirm environment consistency to ensure the backend version is updated before restart.",
        "testStrategy": "1. Before restart, confirm Developers B and C signal readiness.\n2. Verify no processes are listening on port 8000 after termination commands.\n3. After backend restart, perform automated API tests on /api/transcripts/upload and /api/transcripts/process endpoints to confirm they respond correctly.\n4. Check backend logs for successful startup messages and absence of errors.\n5. Confirm team notification is sent and acknowledged.\n6. Optionally, monitor backend stability for a short period post-restart to catch any issues early.",
        "status": "done",
        "dependencies": [
          "1"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-11-25T23:25:53.253Z"
      },
      {
        "id": "15",
        "title": "Add CSV Export Functionality for Meeting Data with Action Items and Decisions",
        "description": "Implement CSV export functionality allowing users to download meeting data including summaries, action items with assignees/status/priority, decisions, and topics in a structured spreadsheet format",
        "details": "Create comprehensive CSV export feature for meetings data that includes all AI-generated insights and related entities.\n\n**Backend Implementation:**\n1. Create new endpoint GET `/api/meetings/export` that accepts optional query parameters:\n   - `meeting_ids[]`: Array of specific meeting IDs to export (optional, exports all if not provided)\n   - `status`: Filter by meeting status (Scheduled|Completed|Cancelled)\n   - `start_date` and `end_date`: Date range filters\n   - `include_tasks`: Boolean to include full task details (default: true)\n   - `include_decisions`: Boolean to include decision details (default: true)\n\n2. Backend logic in `backend/src/routers/meetings.py`:\n   - Query meetings from Cosmos DB with filters applied\n   - For each meeting with action_items, fetch full task details using Promise.all pattern (similar to frontend implementation in meetings/page.tsx:87-96)\n   - For each meeting with decisions, fetch decision details from 'decisions' collection\n   - Generate CSV with proper escaping for multi-line fields (summary, descriptions)\n   - Set response headers: `Content-Type: text/csv; charset=utf-8` and `Content-Disposition: attachment; filename=meetings_export_{timestamp}.csv`\n\n3. CSV Structure with columns:\n   - Meeting ID, Title, Date, Type, Status, Facilitator, Attendees (pipe-separated), Summary (escaped newlines)\n   - Topics (pipe-separated), Transcript URL\n   - Action Items Count, Action Item 1 Title, Action Item 1 Assignee, Action Item 1 Status, Action Item 1 Priority, Action Item 1 Due Date\n   - (Repeat action item columns for items 2-5, with empty values if fewer items)\n   - Decisions Count, Decision 1 Title, Decision 1 Category, Decision 1 Maker\n   - (Repeat decision columns for items 2-3)\n\n4. Use Python csv module with DictWriter:\n```python\nimport csv\nfrom io import StringIO\nfrom fastapi.responses import StreamingResponse\n\ndef generate_csv(meetings: List[Meeting], tasks_map: Dict[str, List[Task]], decisions_map: Dict[str, List[Decision]]):\n    output = StringIO()\n    writer = csv.writer(output, quoting=csv.QUOTE_MINIMAL)\n    # Write headers and rows\n    output.seek(0)\n    return output\n```\n\n**Frontend Implementation (meetings/page.tsx):**\n1. Add \"Export to CSV\" button in the header actions area (near Plan Meeting and Upload Transcript buttons at line 254)\n   - Use Download icon from lucide-react\n   - Position as tertiary action with outline variant\n\n2. Implement export dialog with options:\n   - Checkbox: \"Export all meetings\" vs \"Export filtered meetings only\" (respect current search/filter state)\n   - Checkbox options: \"Include full task details\", \"Include decision details\" \n   - Date range picker for filtering export (optional)\n   - Status filter dropdown\n\n3. Create `handleExportCSV` async function:\n```typescript\nasync function handleExportCSV(exportOptions: ExportOptions) {\n  try {\n    const params = new URLSearchParams();\n    if (exportOptions.meetingIds) {\n      exportOptions.meetingIds.forEach(id => params.append('meeting_ids[]', id));\n    }\n    if (exportOptions.status) params.append('status', exportOptions.status);\n    \n    const response = await fetch(`${API_BASE_URL}/api/meetings/export?${params}`);\n    const blob = await response.blob();\n    const url = window.URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.href = url;\n    a.download = `meetings_export_${new Date().toISOString().split('T')[0]}.csv`;\n    document.body.appendChild(a);\n    a.click();\n    document.body.removeChild(a);\n    window.URL.revokeObjectURL(url);\n  } catch (error) {\n    console.error('Export failed:', error);\n    alert('Failed to export meetings data');\n  }\n}\n```\n\n4. Add export state management:\n   - `isExportDialogOpen` state\n   - `exportOptions` state with default values\n   - Loading state during export generation\n\n5. Update api.ts to add export method:\n```typescript\nexport const api = {\n  meetings: {\n    // ... existing methods\n    export: async (options?: ExportOptions): Promise<Blob> => {\n      const params = new URLSearchParams();\n      // Build query params from options\n      const response = await fetch(`${API_BASE_URL}/api/meetings/export?${params}`);\n      if (!response.ok) throw new Error('Export failed');\n      return response.blob();\n    }\n  }\n}\n```\n\n**CSV Handling Best Practices:**\n- Escape commas, quotes, and newlines in text fields (summary, descriptions)\n- Use consistent date format: YYYY-MM-DD HH:mm:ss UTC\n- Handle empty/null values gracefully with empty strings\n- Pipe-separate (|) list fields like attendees and topics\n- Limit action items and decisions to top 5 and 3 respectively per meeting to keep CSV manageable\n- Add row count and generation timestamp in first comment row\n\n**Performance Considerations:**\n- Implement streaming response for large datasets (100+ meetings)\n- Add pagination parameter for very large exports (max 500 meetings per export)\n- Cache task and decision lookups during single export operation\n- Use asyncio.gather() in Python for parallel task/decision fetching",
        "testStrategy": "1. Test CSV export with no meetings in database (should return empty CSV with headers)\n2. Test export with 1 meeting containing all fields populated (summary, action items, decisions, topics)\n3. Test export with meetings that have no action items or decisions (verify empty columns)\n4. Test export with 50+ meetings to verify performance and streaming\n5. Verify CSV file downloads with correct filename and timestamp\n6. Test filtering: export only Completed meetings, export date range, export specific meeting IDs\n7. Validate CSV format: open in Excel/Google Sheets and verify proper column separation, no broken newlines\n8. Test special characters in meeting summaries and descriptions (commas, quotes, newlines) are properly escaped\n9. Test with meetings containing 10+ action items (verify only top 5 are included)\n10. Verify action item and decision data accuracy by cross-referencing with UI display\n11. Test export dialog: toggle checkboxes, date range selection, filtered vs all meetings export\n12. Test error handling: backend unavailable, invalid meeting IDs, network timeout during export\n13. Verify CSV encoding (UTF-8) handles special characters and international names correctly",
        "status": "cancelled",
        "dependencies": [
          "4"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-11-25T23:26:05.204Z"
      },
      {
        "id": "16",
        "title": "AI Guide Intelligence Enhancement - Intent Classification, Smart Queries, and Contextual Actions",
        "description": "Transform AI Guide from basic keyword search to intelligent assistant with intent classification, targeted CosmosDB queries, aggregations, page-aware context, action suggestions, and session memory",
        "details": "**STATUS: CLOSED** - Core functionality complete (subtasks 1-2). Remaining features merged into Task 22.\n\n**Completed:**\n- Intent classification (subtask 1)\n- Query-specific CosmosDB queries (subtask 2)\n- Page context (done in Task 7.13-14)\n\n**Merged to Task 22:**\n- Date/time parser → Task 22.22\n- Session memory → Task 22.23\n- Proactive insights → Task 22.24\n- Quick actions → Task 22.25\n\n**Superseded by Task 22:**\n- Aggregation patterns (Task 22 Phase 1)\n- Action suggestions (Task 22.6-7)\n- Confidence scoring (Task 22 Phase 4)",
        "testStrategy": "1. Test intent classification accuracy with 20+ sample queries across all intent types\n2. Verify query-specific CosmosDB queries return filtered results (status, priority, assignee)\n3. Test aggregation queries return correct counts and groupings\n4. Verify page context is received and used in backend query construction\n5. Test action suggestions appear in responses for actionable queries\n6. Verify date parsing handles 'this week', 'yesterday', 'overdue' correctly\n7. Test session memory maintains context across conversation turns\n8. Verify proactive insights appear on widget open based on current page\n9. Performance test: response time <2s for standard queries\n10. Test fallback behavior when classification confidence is low",
        "status": "done",
        "dependencies": [
          "6"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Intent Classification System",
            "description": "Replace keyword matching with proper intent classification to determine query type before data fetching",
            "dependencies": [],
            "details": "Create _classify_intent() function in main.py that categorizes queries into: STATUS_CHECK ('what's blocked?', 'show overdue'), AGGREGATION ('how many tasks?', 'count by status'), SEARCH ('find tasks about API'), COMPARISON ('compare this week vs last'), ACTION_GUIDANCE ('what should I work on?'), EXPLANATION ('what is Tier 2 governance?'). Use regex patterns initially, optionally upgrade to AI-based classification. Return intent type and extracted parameters (e.g., status='Blocked', timeframe='this_week'). Update query_agent to use classified intent for downstream processing.",
            "status": "done",
            "testStrategy": "Test 5+ queries per intent type, verify correct classification and parameter extraction",
            "parentId": "undefined",
            "updatedAt": "2025-11-25T21:41:39.168Z"
          },
          {
            "id": 2,
            "title": "Build Query-Specific CosmosDB Query Generator",
            "description": "Replace generic SELECT queries with targeted queries based on intent and parameters",
            "dependencies": [
              1
            ],
            "details": "Create _build_cosmos_query() function that generates appropriate queries based on intent. STATUS_CHECK: 'SELECT * FROM c WHERE c.status = @status'. SEARCH: 'SELECT * FROM c WHERE CONTAINS(LOWER(c.title), @search) OR CONTAINS(LOWER(c.description), @search)'. Date-based: 'SELECT * FROM c WHERE c.due_date >= @start AND c.due_date <= @end'. Assignee: 'SELECT * FROM c WHERE c.assigned_to = @assignee'. Priority: 'SELECT * FROM c WHERE c.priority = @priority'. Support combining multiple filters. Use parameterized queries to prevent injection.\n\n**COMPLETED:** Implemented in backend/src/main.py lines 1242-1349. Supports status, priority, assignee, integration_status, date range, selected IDs, search term, and aggregation queries.",
            "status": "done",
            "testStrategy": "Test each query type returns correct filtered results, test combined filters",
            "parentId": "undefined",
            "updatedAt": "2025-11-26T15:00:00.000Z"
          },
          {
            "id": 3,
            "title": "Add Aggregation Query Support",
            "description": "Implement COUNT and GROUP BY queries for analytical questions",
            "dependencies": [
              2
            ],
            "details": "**SUPERSEDED BY TASK 22** - Intent patterns and aggregation handled in Task 22 Phase 1.",
            "status": "done",
            "testStrategy": "Test aggregation queries return correct counts, verify formatting is clear",
            "parentId": "undefined",
            "updatedAt": "2025-11-26T15:00:00.000Z"
          },
          {
            "id": 4,
            "title": "Implement Natural Date/Time Parser",
            "description": "Add date parsing for natural language time references in queries",
            "dependencies": [
              1
            ],
            "details": "**MERGED TO TASK 22** - Moved to Task 22 Phase 5 (subtask 22).",
            "status": "done",
            "testStrategy": "Test all date expressions resolve to correct date ranges, verify timezone handling",
            "parentId": "undefined",
            "updatedAt": "2025-11-26T15:00:00.000Z"
          },
          {
            "id": 5,
            "title": "Add Page Context to API Request",
            "description": "Update frontend to send current page context with queries for backend awareness",
            "dependencies": [],
            "details": "**ALREADY DONE** - Completed in Task 7 subtasks 13-14 (Phase 2.1 & 2.2).",
            "status": "done",
            "testStrategy": "Verify page context sent from frontend, test backend uses context appropriately",
            "parentId": "undefined",
            "updatedAt": "2025-11-26T15:00:00.000Z"
          },
          {
            "id": 6,
            "title": "Implement Action Suggestion System",
            "description": "Add intelligent next-step suggestions to AI Guide responses",
            "dependencies": [
              2,
              3
            ],
            "details": "**SUPERSEDED BY TASK 22** - Actionability improvements in Task 22 Phase 1 (subtasks 6-7).",
            "status": "done",
            "testStrategy": "Test suggestions are relevant and actionable, verify priority sorting",
            "parentId": "undefined",
            "updatedAt": "2025-11-26T15:00:00.000Z"
          },
          {
            "id": 7,
            "title": "Build Session-Based Conversation Memory",
            "description": "Track conversation context within session for follow-up questions",
            "dependencies": [
              1
            ],
            "details": "**MERGED TO TASK 22** - Moved to Task 22 Phase 5 (subtask 23).",
            "status": "done",
            "testStrategy": "Test follow-up queries use previous context, verify session isolation",
            "parentId": "undefined",
            "updatedAt": "2025-11-26T15:00:00.000Z"
          },
          {
            "id": 8,
            "title": "Add Proactive Insights on Widget Open",
            "description": "Surface relevant insights automatically when user opens AI Guide widget",
            "dependencies": [
              3,
              5
            ],
            "details": "**MERGED TO TASK 22** - Moved to Task 22 Phase 5 (subtask 24).",
            "status": "done",
            "testStrategy": "Test insights are relevant per page, verify caching works, test dismissal",
            "parentId": "undefined",
            "updatedAt": "2025-11-26T15:00:00.000Z"
          },
          {
            "id": 9,
            "title": "Implement Quick Actions System",
            "description": "Allow AI Guide to trigger read-only frontend actions",
            "dependencies": [
              5,
              6
            ],
            "details": "**MERGED TO TASK 22** - Moved to Task 22 Phase 5 (subtask 25).",
            "status": "done",
            "testStrategy": "Test each action type executes correctly, verify read-only constraint",
            "parentId": "undefined",
            "updatedAt": "2025-11-26T15:00:00.000Z"
          },
          {
            "id": 10,
            "title": "Add Confidence Scoring and Uncertainty Handling",
            "description": "Show confidence level in responses and handle uncertain queries gracefully",
            "dependencies": [
              1,
              6
            ],
            "details": "**SUPERSEDED BY TASK 22** - Response quality covered in Task 22 Phase 4 (response templates).",
            "status": "done",
            "testStrategy": "Test confidence assigned correctly per query type, verify UI displays confidence",
            "parentId": "undefined",
            "updatedAt": "2025-11-26T15:00:00.000Z"
          }
        ],
        "updatedAt": "2025-11-25T21:41:56.932Z"
      },
      {
        "id": "17",
        "title": "AI Guide Persona-Based Testing - 7 User Personas with Query Validation",
        "description": "Execute comprehensive persona-based testing of AI Guide using 7 defined user personas (Developer, Architect, AI Director, CTO, BizOps VP, IT Personnel, Security) with 35 test scenarios to validate intent classification, query handling, and response quality",
        "details": "Execute persona-based testing framework defined in .taskmaster/docs/ai-guide-persona-tests.md. Each persona represents a key stakeholder with distinct query patterns and expected behaviors.\n\n**Personas:**\n1. DEVELOPER (Michael Roberts) - Technical tasks, code patterns, assigned work\n2. ARCHITECT (Sarah Chen) - System design, tier classifications, governance\n3. AI ENABLEMENT DIRECTOR (Emma Wilson) - Portfolio metrics, adoption tracking, compliance\n4. CTO (James Anderson) - Executive summaries, strategic decisions, risk awareness\n5. BIZOPS VP (Jennifer Martinez) - Operational metrics, productivity, workload\n6. IT PERSONNEL (David Hayes) - Infrastructure, Azure resources, deployments\n7. SECURITY TEAM - Security decisions, compliance, audit trails\n\n**Test Categories per Persona (5 tests each):**\n- Intent classification accuracy\n- Query filter application\n- Aggregation correctness\n- Response relevance\n- Action suggestion appropriateness\n\n**Success Metrics:**\n- Query accuracy rate (correct intent classification)\n- Response relevance score (1-5)\n- Action suggestion appropriateness\n- Response time (<2s target)\n\n**Report Output:** Generate comprehensive test report with per-persona results, feature coverage, issues found, and recommendations.",
        "testStrategy": "Execute all 35 test scenarios via API calls to /api/agent/query endpoint. For each test: 1) Send query with persona context, 2) Validate intent classification matches expected, 3) Verify response contains relevant data, 4) Check suggestions are appropriate, 5) Measure response time. Generate pass/fail report with aggregated metrics.",
        "status": "done",
        "dependencies": [
          "16"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Execute Developer Persona Tests (DEV-01 to DEV-05)",
            "description": "Run 5 test queries for Developer persona: task lists, status checks, search, blockers, filtered queries",
            "dependencies": [],
            "details": "DEV-01: 'show my tasks' - expect LIST intent, task list with assignee context\nDEV-02: 'what tasks are in progress?' - expect STATUS_CHECK, in-progress tasks only\nDEV-03: 'find tasks about search' - expect SEARCH, tasks with 'search' in title/desc\nDEV-04: 'what's blocking development?' - expect STATUS_CHECK, blocked tasks with blockers\nDEV-05: 'show high priority tasks assigned to David' - expect LIST + FILTER, filtered results",
            "status": "pending",
            "testStrategy": "Call API, verify intent, validate response data structure and content",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Execute Architect Persona Tests (ARCH-01 to ARCH-05)",
            "description": "Run 5 test queries for Architect persona: agent tiers, development status, proposals, explanations",
            "dependencies": [],
            "details": "ARCH-01: 'list all agents by tier' - expect AGGREGATION, agent tier breakdown\nARCH-02: 'show agents in development' - expect LIST + FILTER, development-status agents\nARCH-03: 'what proposals are pending review?' - expect LIST + FILTER, pending proposals\nARCH-04: 'explain the agent tier system' - expect EXPLANATION, governance explanation\nARCH-05: 'recent architecture decisions' - expect LIST, decision list",
            "status": "pending",
            "testStrategy": "Call API, verify intent, validate response data structure and content",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Execute AI Director Persona Tests (AID-01 to AID-05)",
            "description": "Run 5 test queries for AI Enablement Director: status breakdown, completion metrics, portfolio summary",
            "dependencies": [],
            "details": "AID-01: 'breakdown agents by status' - expect AGGREGATION, status distribution\nAID-02: 'how many tasks completed this week?' - expect AGGREGATION, week's completion count\nAID-03: 'agent portfolio summary' - expect AGGREGATION, multi-metric summary\nAID-04: 'pending governance items' - expect LIST, pending proposals/decisions\nAID-05: 'what's the adoption trend?' - expect AGGREGATION, trend data if available",
            "status": "pending",
            "testStrategy": "Call API, verify intent, validate response data structure and content",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Execute CTO Persona Tests (CTO-01 to CTO-05)",
            "description": "Run 5 test queries for CTO: executive summary, blockers, portfolio, decisions, compliance",
            "dependencies": [],
            "details": "CTO-01: 'executive summary' - expect AGGREGATION, high-level overview\nCTO-02: 'what are the critical blockers?' - expect STATUS_CHECK, critical/blocked items\nCTO-03: 'technology portfolio status' - expect AGGREGATION, agent/tech summary\nCTO-04: 'major decisions this month' - expect LIST, recent decisions\nCTO-05: 'any security or compliance concerns?' - expect SEARCH, relevant items",
            "status": "pending",
            "testStrategy": "Call API, verify intent, validate response data structure and content",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Execute BizOps VP Persona Tests (BIZ-01 to BIZ-05)",
            "description": "Run 5 test queries for BizOps VP: workload, overdue, meetings, approvals, productivity",
            "dependencies": [],
            "details": "BIZ-01: 'workload by assignee' - expect AGGREGATION, assignee distribution\nBIZ-02: 'overdue tasks' - expect STATUS_CHECK, overdue items\nBIZ-03: 'meeting completion rate' - expect AGGREGATION, meeting metrics\nBIZ-04: 'pending approvals' - expect LIST, items needing approval\nBIZ-05: 'team productivity this week' - expect AGGREGATION, productivity metrics",
            "status": "pending",
            "testStrategy": "Call API, verify intent, validate response data structure and content",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Execute IT Personnel Persona Tests (IT-01 to IT-05)",
            "description": "Run 5 test queries for IT Personnel: infrastructure, deployments, Azure, changes, integrations",
            "dependencies": [],
            "details": "IT-01: 'infrastructure tasks' - expect SEARCH, infra-related tasks\nIT-02: 'blocked deployments' - expect STATUS_CHECK, blocked deployment tasks\nIT-03: 'azure resources' - expect LIST, resource information\nIT-04: 'recent system changes' - expect LIST, recent updates\nIT-05: 'integration issues' - expect SEARCH, integration problems",
            "status": "pending",
            "testStrategy": "Call API, verify intent, validate response data structure and content",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Execute Security Persona Tests (SEC-01 to SEC-05)",
            "description": "Run 5 test queries for Security Team: security decisions, compliance, governance, critical items, audit",
            "dependencies": [],
            "details": "SEC-01: 'security decisions' - expect SEARCH, security-related decisions\nSEC-02: 'compliance tasks' - expect SEARCH, compliance items\nSEC-03: 'governance policies' - expect EXPLANATION, policy information\nSEC-04: 'critical security items' - expect STATUS_CHECK, critical priority items\nSEC-05: 'audit trail' - expect LIST, recent activity",
            "status": "pending",
            "testStrategy": "Call API, verify intent, validate response data structure and content",
            "parentId": "undefined"
          },
          {
            "id": 8,
            "title": "Generate Comprehensive Test Report",
            "description": "Compile all test results into comprehensive report with per-persona scores, feature coverage, issues, and recommendations",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6,
              7
            ],
            "details": "Generate report following template in ai-guide-persona-tests.md. Include: Executive Summary (total pass rate), Per-Persona Results (X/5 passed, strengths, issues, recommendations), Feature Coverage (intent classification %, query filters %, aggregations %, search %, suggestions %), Issues Found, Recommendations. Save report to .taskmaster/docs/ai-guide-persona-test-report.md",
            "status": "pending",
            "testStrategy": "Verify report contains all sections, metrics are accurate, recommendations are actionable",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-03T03:52:42.110Z"
      },
      {
        "id": "18",
        "title": "P0: Security Critical - Audit Logging, Security Decisions & Approval Workflow",
        "description": "Implement critical security infrastructure identified in persona testing: audit logging system, security decision categorization, and task approval workflow tracking. These are blockers for enterprise deployment.",
        "details": "**Problem Context:**\nPersona testing revealed 3 critical gaps affecting Security Team and BizOps VP personas:\n1. SEC-05 'audit trail' query returned no data - no mechanism to track user actions\n2. SEC-01 'security decisions' returned nothing - decisions can't be categorized as security-related\n3. BIZ-04 'pending approvals' couldn't return meaningful data - no approval workflow tracking\n\n**Business Impact:**\n- Enterprise security requires audit trails for compliance (SOC2, GDPR, ISO 27001)\n- Security team can't investigate incidents without activity logs\n- BizOps can't track governance gates or approval processes\n- Missing visibility for compliance audits\n\n**Success Criteria:**\n- SEC-05 'audit trail' returns recent user activity\n- SEC-01 'security decisions' returns security-categorized decisions\n- BIZ-04 'pending approvals' returns tasks awaiting sign-off",
        "testStrategy": "Re-run Security Team persona tests (SEC-01 to SEC-05) and BizOps VP tests (BIZ-04). All should pass with relevant data returned.",
        "status": "done",
        "dependencies": [
          "17"
        ],
        "priority": "critical",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement AuditLog Model and CosmosDB Container",
            "description": "Create AuditLog pydantic model with fields: id, user_id, action (view/create/update/delete), entity_type, entity_id, timestamp, details. Create CosmosDB container for audit_logs.",
            "dependencies": [],
            "details": "Model fields:\n- id: str (UUID)\n- user_id: str (who performed action)\n- action: str enum ['view', 'create', 'update', 'delete', 'query']\n- entity_type: str ['task', 'agent', 'meeting', 'decision', 'proposal']\n- entity_id: str (ID of affected entity)\n- timestamp: datetime\n- ip_address: Optional[str]\n- user_agent: Optional[str]\n- details: Optional[dict] (additional context)\n\nCosmosDB: Create 'audit_logs' container with partition key '/entity_type'",
            "status": "pending",
            "testStrategy": "Verify model validates correctly, container created in CosmosDB",
            "parentId": "18"
          },
          {
            "id": 2,
            "title": "Add Audit Logging Middleware/Decorator",
            "description": "Create middleware or decorator to automatically log API calls. Log all create/update/delete operations and optionally read operations for sensitive entities.",
            "dependencies": [
              1
            ],
            "details": "Implementation options:\n1. FastAPI middleware for all routes\n2. Decorator for specific endpoints\n\nLog these operations:\n- All POST/PUT/PATCH/DELETE requests\n- GET requests for tasks, decisions, agents (configurable)\n- AI Guide queries (for usage analytics)\n\nInclude request context: user identity, timestamp, IP, endpoint, entity affected",
            "status": "pending",
            "testStrategy": "Make API calls, verify audit logs created in CosmosDB",
            "parentId": "18"
          },
          {
            "id": 3,
            "title": "Create Audit Log Query API Endpoint",
            "description": "Add GET /api/audit endpoint to retrieve audit logs with filtering by user, entity_type, action, and date range.",
            "dependencies": [
              1,
              2
            ],
            "details": "Endpoint: GET /api/audit\nQuery params:\n- user_id: filter by user\n- entity_type: filter by entity type\n- action: filter by action type\n- start_date, end_date: date range\n- limit: max results (default 100)\n\nResponse: List of AuditLog objects with pagination",
            "status": "pending",
            "testStrategy": "Query audit endpoint, verify filtering works correctly",
            "parentId": "18"
          },
          {
            "id": 4,
            "title": "Integrate Audit Trail into AI Guide",
            "description": "Update AI Guide to query audit_logs when user asks about 'audit trail', 'recent activity', 'who changed', etc.",
            "dependencies": [
              3
            ],
            "details": "Update _build_cosmos_query to:\n1. Detect audit-related queries: 'audit trail', 'activity log', 'who changed', 'recent changes'\n2. Query audit_logs container\n3. Format response showing recent activity\n\nExpected queries to work:\n- 'show audit trail' → recent audit logs\n- 'who changed task X' → filtered by entity\n- 'activity this week' → date-filtered logs",
            "status": "pending",
            "testStrategy": "Re-run SEC-05 'audit trail' test, verify returns actual data",
            "parentId": "18"
          },
          {
            "id": 5,
            "title": "Add ApprovalStatus Enum and Task Field",
            "description": "Add ApprovalStatus enum (NOT_REQUIRED, PENDING, APPROVED, REJECTED) and approval fields to Task model.",
            "dependencies": [],
            "details": "Add to models.py:\n\nclass ApprovalStatus(str, Enum):\n    NOT_REQUIRED = 'Not Required'\n    PENDING = 'Pending'\n    APPROVED = 'Approved'\n    REJECTED = 'Rejected'\n\nAdd to Task model:\n- approval_status: ApprovalStatus = ApprovalStatus.NOT_REQUIRED\n- approver: Optional[str] = None\n- approved_at: Optional[datetime] = None\n- approval_notes: Optional[str] = None",
            "status": "pending",
            "testStrategy": "Verify Task model accepts new fields, existing tasks migrate cleanly",
            "parentId": "18"
          },
          {
            "id": 6,
            "title": "Create Approval Workflow API Endpoints",
            "description": "Add endpoints for approval workflow: request approval, approve/reject task, list pending approvals.",
            "dependencies": [
              5
            ],
            "details": "New endpoints:\n- POST /api/tasks/{id}/request-approval - set status to PENDING, assign approver\n- POST /api/tasks/{id}/approve - set APPROVED, record timestamp\n- POST /api/tasks/{id}/reject - set REJECTED with notes\n- GET /api/tasks/pending-approvals - list all PENDING tasks\n\nBusiness rules:\n- Only certain task categories require approval\n- Approver cannot be same as assignee\n- Approval changes should be audit logged",
            "status": "pending",
            "testStrategy": "Test full approval workflow, verify state transitions",
            "parentId": "18"
          },
          {
            "id": 7,
            "title": "Integrate Approval Status into AI Guide",
            "description": "Update AI Guide to filter and report on approval status when queried about 'pending approvals', 'awaiting sign-off', etc.",
            "dependencies": [
              6
            ],
            "details": "Update _extract_filters to:\n1. Detect approval queries: 'pending approvals', 'awaiting approval', 'needs sign-off'\n2. Filter tasks by approval_status = 'Pending'\n3. Include approver info in response\n\nUpdate _generate_action_suggestions to suggest:\n- 'Approve task' action for pending items\n- 'Request approval' for tasks without approval",
            "status": "pending",
            "testStrategy": "Re-run BIZ-04 'pending approvals' test, verify returns filtered tasks",
            "parentId": "18"
          },
          {
            "id": 8,
            "title": "Populate Security Decisions Test Data",
            "description": "Add sample security-categorized decisions to CosmosDB to enable SEC-01 testing. DecisionCategory.SECURITY already exists.",
            "dependencies": [],
            "details": "Add 2-3 security decisions:\n1. 'Implement Role-Based Access Control' - Security decision about RBAC implementation\n2. 'Data Encryption Standards' - Decision on encryption requirements\n3. 'PII Detection Requirements' - Decision on PII handling\n\nEnsure category = DecisionCategory.SECURITY for all",
            "status": "pending",
            "testStrategy": "Re-run SEC-01 'security decisions' test, verify returns security-categorized decisions",
            "parentId": "18"
          },
          {
            "id": 9,
            "title": "Re-run Security & BizOps Persona Tests",
            "description": "Execute Security Team (SEC-01 to SEC-05) and BizOps VP (BIZ-04) tests to validate all P0 fixes.",
            "dependencies": [
              4,
              7,
              8
            ],
            "details": "Tests to re-run:\n- SEC-01: 'security decisions' - should return security-categorized decisions\n- SEC-05: 'audit trail' - should return recent audit logs\n- BIZ-04: 'pending approvals' - should return tasks with approval_status=PENDING\n\nSuccess criteria: All 3 tests pass with relevant data",
            "status": "pending",
            "testStrategy": "Run API tests, document results, update test report",
            "parentId": "18"
          }
        ],
        "updatedAt": "2025-11-26T00:10:18.608Z"
      },
      {
        "id": "19",
        "title": "P1: Functionality Improvements - Intent Classification, Date Filtering & Integration Status",
        "description": "Improve AI Guide functionality identified in persona testing: better aggregation intent classification, relative date filtering ('this month/week'), and agent integration status tracking.",
        "details": "**Problem Context:**\nPersona testing revealed 3 functionality gaps:\n1. Aggregation queries like 'breakdown agents by status' return correct data but intent shows 'list' instead of 'aggregation'\n2. CTO-04 'major decisions this month' returned empty despite November decisions existing\n3. IT-05 'integration issues' returned inference-based response due to no explicit integration tracking\n\n**Business Impact:**\n- Aggregation intent affects UI presentation (could trigger charts/graphs)\n- Date filtering is common executive query pattern\n- IT team needs visibility into agent integration health\n\n**Success Criteria:**\n- Aggregation queries return intent='aggregation'\n- 'This month/week' queries return correct date-filtered results\n- Integration status visible for agents",
        "testStrategy": "Re-run affected persona tests: AID-01, CTO-04, IT-05. Verify intent classification and data accuracy.",
        "status": "done",
        "dependencies": [
          "18"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Enhance Aggregation Intent Classification",
            "description": "Update _classify_intent() to better recognize aggregation keywords and patterns.",
            "dependencies": [],
            "details": "Current issue: Queries like 'breakdown agents by status' return intent='list' instead of 'aggregation'\n\nAdd aggregation keyword patterns:\n- 'breakdown', 'breakdown by', 'distribution'\n- 'count by', 'group by', 'categorize by'\n- 'how many by', 'percentage', 'ratio'\n- 'summary by', 'stats by', 'metrics by'\n\nUpdate _classify_intent() logic:\n```python\naggregation_patterns = [\n    r'breakdown\\s+(by|of)',\n    r'distribution\\s+(by|of)',\n    r'count\\s+by',\n    r'group\\s+by',\n    r'how\\s+many\\s+.*\\s+by',\n    r'percentage',\n    r'summary\\s+(by|of)'\n]\nif any(re.search(p, query_lower) for p in aggregation_patterns):\n    return 'aggregation'\n```",
            "status": "pending",
            "testStrategy": "Test queries: 'breakdown agents by status', 'count tasks by priority', 'distribution by assignee'. All should return intent='aggregation'",
            "parentId": "19"
          },
          {
            "id": 2,
            "title": "Implement Relative Date Parsing",
            "description": "Enhance _extract_filters() to properly handle relative date expressions like 'this month', 'this week', 'recent', 'today'.",
            "dependencies": [],
            "details": "Date expression mappings:\n- 'today' → current date\n- 'yesterday' → current date - 1 day\n- 'this week' → Monday to Sunday of current week\n- 'last week' → previous Monday to Sunday\n- 'this month' → 1st to last day of current month\n- 'last month' → previous month range\n- 'recent' → last 30 days\n- 'past X days' → last X days\n\nImplementation:\n```python\nfrom datetime import datetime, timedelta\n\ndef _parse_relative_date(expression: str) -> tuple[datetime, datetime]:\n    today = datetime.utcnow().replace(hour=0, minute=0, second=0)\n    if 'this week' in expression:\n        start = today - timedelta(days=today.weekday())\n        end = start + timedelta(days=6)\n    elif 'this month' in expression:\n        start = today.replace(day=1)\n        # end = last day of month\n    # ... etc\n    return start, end\n```",
            "status": "pending",
            "testStrategy": "Test 'decisions this month', 'tasks this week', 'meetings today'. Verify correct date ranges applied.",
            "parentId": "19"
          },
          {
            "id": 3,
            "title": "Integrate Relative Dates into Query Builder",
            "description": "Update _build_cosmos_query() to apply parsed relative dates as filters on created_at, due_date, decision_date fields.",
            "dependencies": [
              2
            ],
            "details": "When relative date detected:\n1. Parse to start_date, end_date range\n2. Apply appropriate filter based on entity type:\n   - Tasks: filter on created_at or due_date\n   - Decisions: filter on decision_date\n   - Meetings: filter on date field\n   - Audit logs: filter on timestamp\n\nCosmosDB query addition:\n```sql\nAND c.decision_date >= @start_date AND c.decision_date <= @end_date\n```",
            "status": "pending",
            "testStrategy": "Re-run CTO-04 'major decisions this month', verify returns November 2025 decisions",
            "parentId": "19"
          },
          {
            "id": 4,
            "title": "Add IntegrationStatus Enum and Agent Field",
            "description": "Add IntegrationStatus enum and integration fields to Agent model for tracking connectivity health.",
            "dependencies": [],
            "details": "Add to models.py:\n\nclass IntegrationStatus(str, Enum):\n    NOT_CONFIGURED = 'Not Configured'\n    CONNECTED = 'Connected'\n    DEGRADED = 'Degraded'\n    FAILED = 'Failed'\n\nAdd to Agent model:\n- integration_status: IntegrationStatus = IntegrationStatus.NOT_CONFIGURED\n- integration_endpoint: Optional[str] = None\n- last_health_check: Optional[datetime] = None\n- integration_error: Optional[str] = None",
            "status": "pending",
            "testStrategy": "Verify Agent model accepts new fields, existing agents migrate cleanly",
            "parentId": "19"
          },
          {
            "id": 5,
            "title": "Integrate Integration Status into AI Guide",
            "description": "Update AI Guide to filter and report on agent integration status when queried about 'integration issues', 'connectivity problems', etc.",
            "dependencies": [
              4
            ],
            "details": "Update _extract_filters to detect:\n- 'integration issues', 'integration problems'\n- 'connectivity', 'connection status'\n- 'failed integrations', 'degraded services'\n\nFilter logic:\n- 'integration issues' → integration_status IN ['Failed', 'Degraded']\n- 'integration status' → list all with their status\n\nUpdate response to include:\n- Status indicator\n- Last health check time\n- Error details if failed",
            "status": "pending",
            "testStrategy": "Re-run IT-05 'integration issues', verify returns agents with Failed/Degraded status",
            "parentId": "19"
          },
          {
            "id": 6,
            "title": "Re-run Affected Persona Tests",
            "description": "Execute tests affected by P1 changes to validate improvements.",
            "dependencies": [
              1,
              3,
              5
            ],
            "details": "Tests to re-run:\n- AID-01: 'breakdown agents by status' - verify intent='aggregation'\n- CTO-04: 'major decisions this month' - verify returns current month decisions\n- IT-05: 'integration issues' - verify returns agents with integration problems\n\nAdditional validation:\n- Test various relative date expressions\n- Test aggregation keyword variations",
            "status": "pending",
            "testStrategy": "Run API tests, compare before/after behavior, update test report",
            "parentId": "19"
          }
        ],
        "updatedAt": "2025-12-03T03:53:30.016Z"
      },
      {
        "id": "20",
        "title": "P2: Future Enhancements - Workload Balancing, Trends, Security Shortcuts & Deduplication",
        "description": "Nice-to-have enhancements for improved user experience: proactive workload balancing suggestions, trend analysis, security-focused query shortcuts, and task deduplication detection.",
        "details": "**Problem Context:**\nPersona testing observations for future improvement:\n1. BIZ-01 showed uneven workload (Michael: 6 tasks, Bob: 1 task) - no proactive suggestion\n2. AID-05 'adoption trend' could only provide qualitative observations - no historical data\n3. Security persona requires multiple queries for comprehensive view\n4. BIZ-02 revealed duplicate tasks in database\n\n**Business Impact:**\n- Workload balancing improves team efficiency\n- Trend analysis enables data-driven decisions\n- Security shortcuts improve security team productivity\n- Deduplication reduces data noise\n\n**Note:** These are lower priority enhancements after P0/P1 are complete.",
        "testStrategy": "Feature acceptance testing with relevant personas. Validate suggestions appear, trends calculated, shortcuts work, duplicates detected.",
        "status": "pending",
        "dependencies": [
          "19"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Workload Balancing Suggestions",
            "description": "Add proactive suggestions when AI Guide detects uneven workload distribution across team members.",
            "dependencies": [],
            "details": "When processing workload queries:\n1. Calculate task count per assignee\n2. Detect imbalance: if max/min ratio > 3x\n3. Generate suggestion: 'Consider reassigning tasks from [overloaded] to [underloaded]'\n\nThreshold configuration:\n- Warning: 3x difference\n- Critical: 5x difference\n\nResponse addition:\n```\n⚠️ Workload imbalance detected:\n- Michael Roberts: 6 tasks (overloaded)\n- Bob Smith: 1 task (capacity available)\nConsider redistributing for better balance.\n```",
            "status": "pending",
            "testStrategy": "Query 'workload by assignee' with imbalanced data, verify suggestion appears",
            "parentId": "20"
          },
          {
            "id": 2,
            "title": "Add Historical Metrics Tracking",
            "description": "Create metrics_history container to track daily snapshots of key metrics for trend analysis.",
            "dependencies": [],
            "details": "New CosmosDB container: metrics_history\nPartition key: /metric_type\n\nDaily snapshot model:\n```python\nclass MetricSnapshot(BaseModel):\n    id: str\n    metric_type: str  # 'agent_count', 'task_completion', 'meeting_frequency'\n    date: datetime\n    value: float\n    breakdown: Optional[dict] = None  # e.g., {status: count}\n```\n\nScheduled job (daily):\n- Count agents by status → store snapshot\n- Count tasks completed today → store snapshot\n- Count meetings held → store snapshot",
            "status": "pending",
            "testStrategy": "Verify daily snapshots created, queryable by date range",
            "parentId": "20"
          },
          {
            "id": 3,
            "title": "Implement Trend Analysis Queries",
            "description": "Enable AI Guide to answer trend questions using historical metrics data.",
            "dependencies": [
              2
            ],
            "details": "Detect trend queries:\n- 'adoption trend', 'growth trend'\n- 'how has X changed', 'X over time'\n- 'velocity', 'completion rate trend'\n\nQuery metrics_history:\n- Get snapshots for requested period\n- Calculate trend direction (increasing/decreasing/stable)\n- Calculate percentage change\n\nResponse format:\n```\nAgent adoption trend (last 30 days):\n- Start: 1 agent\n- Current: 3 agents\n- Growth: +200%\n- Trend: ↗️ Increasing\n```",
            "status": "pending",
            "testStrategy": "Re-run AID-05 'adoption trend' with historical data, verify quantitative response",
            "parentId": "20"
          },
          {
            "id": 4,
            "title": "Create Security-Focused Query Shortcuts",
            "description": "Implement compound queries that combine multiple security-relevant data points for Security persona efficiency.",
            "dependencies": [],
            "details": "Security shortcuts:\n\n1. 'security posture' → combines:\n   - Security-categorized decisions\n   - Critical priority tasks\n   - Compliance-related tasks\n   - Recent security audit logs\n\n2. 'risk assessment' → combines:\n   - Blocked items\n   - Overdue security tasks\n   - Failed integrations\n   - Pending security approvals\n\n3. 'compliance status' → combines:\n   - Compliance tasks by status\n   - Governance decisions\n   - Audit log summary\n\nImplementation: Detect shortcut phrases, execute multiple queries, merge results",
            "status": "pending",
            "testStrategy": "Test 'security posture', verify comprehensive multi-source response",
            "parentId": "20"
          },
          {
            "id": 5,
            "title": "Implement Task Deduplication Detection",
            "description": "Add logic to detect and flag potential duplicate tasks based on title similarity and assignee.",
            "dependencies": [],
            "details": "Duplicate detection triggers:\n1. On task creation: check for similar existing tasks\n2. On query: highlight duplicates in response\n3. Periodic scan: identify duplicate clusters\n\nSimilarity criteria:\n- Title similarity > 80% (fuzzy match)\n- Same assignee\n- Same category\n- Created within 7 days\n\nResponse addition:\n```\n⚠️ Potential duplicates detected:\n- 'Draft Governance Policy' (task-123) and 'Draft Governance Policy' (task-456)\n  Consider merging these tasks.\n```\n\nUse: difflib.SequenceMatcher or rapidfuzz for similarity",
            "status": "pending",
            "testStrategy": "Create similar tasks, verify warning appears in query response",
            "parentId": "20"
          },
          {
            "id": 6,
            "title": "Add Deduplication Merge Suggestion",
            "description": "When duplicates detected, suggest merge action with preview of combined task.",
            "dependencies": [
              5
            ],
            "details": "Add action suggestion:\n```json\n{\n  \"label\": \"Merge duplicates\",\n  \"action_type\": \"merge\",\n  \"params\": {\n    \"task_ids\": [\"task-123\", \"task-456\"],\n    \"preview\": \"Merged: Draft Governance Policy (combined notes)\"\n  }\n}\n```\n\nMerge logic:\n- Keep older task ID\n- Combine descriptions/notes\n- Preserve all assignees\n- Sum estimated effort\n- Delete duplicate after merge",
            "status": "pending",
            "testStrategy": "Click merge suggestion, verify tasks combined correctly",
            "parentId": "20"
          }
        ],
        "updatedAt": "2025-11-25T23:30:00.000Z"
      },
      {
        "id": "21",
        "title": "AI Guide Persona Use Case Testing",
        "description": "Execute realistic persona-based test scenarios that mimic how actual users would interact with the AI Guide. Each persona represents a distinct role with specific goals, workflows, and question patterns.",
        "details": "**Purpose:** Validate AI Guide accuracy and usefulness through realistic user scenarios.\n\n**Personas:**\n1. **Project Manager (PM)** - Needs status overviews, blockers, deadlines, team workload\n2. **Engineering Lead** - Needs technical details, agent status, integration health\n3. **Executive/Stakeholder** - Needs high-level summaries, KPIs, decision support\n4. **New Team Member** - Needs onboarding info, context, learning resources\n5. **DevOps/Platform** - Needs infrastructure status, deployments, resource health\n\n**Success Criteria:**\n- Response relevance: Does it answer the actual question?\n- Data accuracy: Are counts/statuses correct?\n- Actionability: Can user take action based on response?\n- Context awareness: Does it understand the user's role?",
        "testStrategy": "Execute each persona's use cases, score responses on relevance/accuracy/actionability, identify failure patterns, generate comprehensive report",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "PM Persona: Daily Standup Prep",
            "description": "Project Manager preparing for daily standup needs quick status overview",
            "dependencies": [],
            "details": "**Scenario:** It's 9am, PM needs to run standup in 15 minutes.\n\n**Use Cases:**\n1. 'What's blocking the team today?' - Expect: List of blocked tasks with reasons\n2. 'Show me tasks due this week' - Expect: Tasks with due dates in next 7 days\n3. 'Who has the most work assigned?' - Expect: Workload distribution by assignee\n4. 'Any meetings scheduled today?' - Expect: Today's meetings with times\n5. 'What was completed yesterday?' - Expect: Recently completed tasks\n\n**Success Criteria:**\n- Quick, scannable responses\n- Accurate task counts\n- Actionable blockers list",
            "status": "done",
            "testStrategy": "Execute 5 queries, score each 1-5 on relevance/accuracy/actionability",
            "parentId": "21"
          },
          {
            "id": 2,
            "title": "PM Persona: Sprint Planning",
            "description": "Project Manager planning upcoming sprint needs capacity and backlog info",
            "dependencies": [],
            "details": "**Scenario:** Sprint planning meeting, need to understand capacity and priorities.\n\n**Use Cases:**\n1. 'What high priority tasks are still pending?' - Expect: Prioritized backlog\n2. 'How many tasks did we complete last sprint?' - Expect: Completion metrics\n3. 'Show team capacity - who has bandwidth?' - Expect: Tasks per person\n4. 'What decisions are pending approval?' - Expect: Awaiting decisions\n5. 'Any tasks without assignees?' - Expect: Unassigned work\n\n**Success Criteria:**\n- Accurate counts for planning\n- Clear priority ordering\n- Identifies capacity gaps",
            "status": "done",
            "testStrategy": "Execute 5 queries, validate against actual database counts",
            "parentId": "21"
          },
          {
            "id": 3,
            "title": "Engineering Lead: Technical Health Check",
            "description": "Engineering Lead checking system and agent health",
            "dependencies": [],
            "details": "**Scenario:** Weekly technical review, need agent status and integration health.\n\n**Use Cases:**\n1. 'What agents are in production?' - Expect: Production agents with status\n2. 'Any agents with integration issues?' - Expect: Integration problems flagged\n3. 'Show me agents in development' - Expect: Development pipeline\n4. 'What's the agent deployment timeline?' - Expect: Agents by stage\n5. 'Any critical technical tasks overdue?' - Expect: Overdue high-priority items\n\n**Success Criteria:**\n- Technical accuracy on agent states\n- Integration status visibility\n- Clear deployment pipeline view",
            "status": "done",
            "testStrategy": "Execute 5 queries, verify technical details match database",
            "parentId": "21"
          },
          {
            "id": 4,
            "title": "Engineering Lead: Architecture Decisions",
            "description": "Engineering Lead researching past architecture decisions",
            "dependencies": [],
            "details": "**Scenario:** Need to understand why certain technical decisions were made.\n\n**Use Cases:**\n1. 'What architecture decisions have been made?' - Expect: Decision list\n2. 'Why did we choose Azure OpenAI?' - Expect: Decision rationale\n3. 'Show recent technical decisions' - Expect: Last 30 days decisions\n4. 'What proposals are pending technical review?' - Expect: Awaiting decisions\n5. 'Who made the CosmosDB decision?' - Expect: Decision owner/date\n\n**Success Criteria:**\n- Accurate decision history\n- Rationale included when available\n- Correct attribution",
            "status": "done",
            "testStrategy": "Execute 5 queries, verify decision details match records",
            "parentId": "21"
          },
          {
            "id": 5,
            "title": "Executive: Weekly Status Report",
            "description": "Executive preparing for leadership meeting needs high-level metrics",
            "dependencies": [],
            "details": "**Scenario:** Executive needs 5-minute status summary for leadership.\n\n**Use Cases:**\n1. 'Give me a summary of project status' - Expect: High-level overview\n2. 'What are the key risks or blockers?' - Expect: Risk summary\n3. 'How many agents are operational vs planned?' - Expect: Pipeline metrics\n4. 'What decisions need my attention?' - Expect: Pending approvals\n5. 'Show progress this month' - Expect: Monthly achievements\n\n**Success Criteria:**\n- Executive-friendly summaries (not technical jargon)\n- Key metrics highlighted\n- Decision items clear",
            "status": "done",
            "testStrategy": "Execute 5 queries, evaluate clarity and executive-appropriateness",
            "parentId": "21"
          },
          {
            "id": 6,
            "title": "Executive: Resource Allocation",
            "description": "Executive reviewing resource utilization and team allocation",
            "dependencies": [],
            "details": "**Scenario:** Budget review, need to understand where resources are focused.\n\n**Use Cases:**\n1. 'How is the team workload distributed?' - Expect: Work per person\n2. 'What are the biggest initiatives?' - Expect: Major work streams\n3. 'Any bottlenecks in delivery?' - Expect: Blockers/delays\n4. 'Show me tasks by category' - Expect: Work breakdown\n5. 'What's the completion rate trend?' - Expect: Productivity metrics\n\n**Success Criteria:**\n- Resource visibility\n- Clear bottleneck identification\n- Actionable insights",
            "status": "done",
            "testStrategy": "Execute 5 queries, evaluate business value of responses",
            "parentId": "21"
          },
          {
            "id": 7,
            "title": "New Team Member: Onboarding",
            "description": "New team member trying to understand the project and get oriented",
            "dependencies": [],
            "details": "**Scenario:** Day 1 on the project, need to understand what's happening.\n\n**Use Cases:**\n1. 'What is this project about?' - Expect: Project overview\n2. 'Who are the key people I should know?' - Expect: Team/stakeholders\n3. 'What are the main agents we're building?' - Expect: Agent overview\n4. 'What should I focus on first?' - Expect: Onboarding guidance\n5. 'Where can I find documentation?' - Expect: Resource pointers\n\n**Success Criteria:**\n- Helpful onboarding context\n- Clear explanations (not assumed knowledge)\n- Welcoming tone",
            "status": "done",
            "testStrategy": "Execute 5 queries, evaluate newcomer-friendliness",
            "parentId": "21"
          },
          {
            "id": 8,
            "title": "New Team Member: Task Pickup",
            "description": "New team member looking for tasks to work on",
            "dependencies": [],
            "details": "**Scenario:** Finished onboarding, ready to pick up first task.\n\n**Use Cases:**\n1. 'What tasks are available to pick up?' - Expect: Unassigned tasks\n2. 'Show me low priority tasks good for learning' - Expect: Starter tasks\n3. 'What did the team work on recently?' - Expect: Recent activity\n4. 'Who should I ask about the AI Search task?' - Expect: Task owner\n5. 'What skills are needed for pending tasks?' - Expect: Requirements\n\n**Success Criteria:**\n- Identifies starter-friendly tasks\n- Points to right people\n- Manageable scope tasks",
            "status": "done",
            "testStrategy": "Execute 5 queries, evaluate onboarding usefulness",
            "parentId": "21"
          },
          {
            "id": 9,
            "title": "DevOps: Deployment Status",
            "description": "DevOps engineer checking deployment pipeline and infrastructure",
            "dependencies": [],
            "details": "**Scenario:** Deployment day, need to verify everything is ready.\n\n**Use Cases:**\n1. 'What agents are ready for deployment?' - Expect: Deployment-ready items\n2. 'Any infrastructure tasks pending?' - Expect: Infra backlog\n3. 'Show me recent deployment decisions' - Expect: Deployment-related decisions\n4. 'What's blocking production releases?' - Expect: Release blockers\n5. 'Status of Azure resource setup?' - Expect: Infrastructure status\n\n**Success Criteria:**\n- Deployment readiness clear\n- Infrastructure status accurate\n- Blockers identified",
            "status": "done",
            "testStrategy": "Execute 5 queries, verify operational accuracy",
            "parentId": "21"
          },
          {
            "id": 10,
            "title": "DevOps: Incident Response",
            "description": "DevOps responding to a reported issue",
            "dependencies": [],
            "details": "**Scenario:** User reports something is broken, need to investigate.\n\n**Use Cases:**\n1. 'Any known issues with the AI Guide?' - Expect: Known problems\n2. 'Show me recent changes to the system' - Expect: Recent updates\n3. 'What integrations might be affected?' - Expect: Integration list\n4. 'Who owns the backend service?' - Expect: Ownership info\n5. 'Any related incidents or bugs?' - Expect: Related issues\n\n**Success Criteria:**\n- Quick issue context\n- Points to owners\n- Shows related problems",
            "status": "done",
            "testStrategy": "Execute 5 queries, evaluate incident response usefulness",
            "parentId": "21"
          },
          {
            "id": 11,
            "title": "Cross-Persona: Complex Multi-Part Queries",
            "description": "Test complex queries that combine multiple concerns",
            "dependencies": [],
            "details": "**Scenario:** Users often ask compound questions.\n\n**Use Cases:**\n1. 'Show blocked tasks and who owns them' - Expect: Blocked + assignee\n2. 'High priority tasks due this week assigned to David' - Expect: Filtered list\n3. 'Compare agent count by status vs last month' - Expect: Comparison\n4. 'Tasks created from recent meetings' - Expect: Meeting-task link\n5. 'Pending decisions that affect the architecture' - Expect: Filtered decisions\n\n**Success Criteria:**\n- Handles compound queries\n- Applies multiple filters correctly\n- Doesn't miss parts of question",
            "status": "done",
            "testStrategy": "Execute 5 complex queries, verify all parts addressed",
            "parentId": "21"
          },
          {
            "id": 12,
            "title": "Generate Comprehensive Test Report",
            "description": "Compile results from all persona tests into actionable report",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6,
              7,
              8,
              9,
              10,
              11
            ],
            "details": "**Report Structure:**\n1. Executive Summary - Overall pass rate, key findings\n2. Persona Results - Score by persona, best/worst scenarios\n3. Query Analysis - Which query types succeed/fail\n4. Failure Patterns - Common issues identified\n5. Recommendations - Priority fixes needed\n6. Data Accuracy - Count verification results\n7. Response Quality - Relevance/actionability scores\n\n**Scoring:**\n- Relevance: 1-5 (Does it answer the question?)\n- Accuracy: 1-5 (Is the data correct?)\n- Actionability: 1-5 (Can user act on it?)\n- Overall: Average of above\n\n**Output:** Markdown report with tables and recommendations",
            "status": "done",
            "testStrategy": "Generate report after all tests complete",
            "parentId": "21"
          }
        ],
        "updatedAt": "2025-11-26T12:00:00.000Z"
      },
      {
        "id": "22",
        "title": "AI Guide Relevance and Actionability Improvements",
        "description": "Implement improvements to address low relevance (7 queries, 12.7%) and low actionability (17 queries, 30.9%) issues identified in persona testing. Target: Relevance 3.95→4.5+, Actionability 3.38→4.2+",
        "details": "**Root Cause Analysis:**\n- Accuracy is strong (4.87/5) but understanding (3.95/5) and actionability (3.38/5) need improvement\n- Intent patterns missing: risks, blockers, bottlenecks, ownership, learning-friendly\n- AI system prompt lacks actionability guidance\n- Data model missing: complexity, owner, timeline fields\n- No historical data for comparisons\n\n**Implementation Phases:**\n- Phase 1 (1-2 days): Quick wins - expand intent patterns, enhance AI prompt (subtasks 1-8)\n- Phase 2 (3-5 days): Data model enhancements - complexity, owner fields (subtasks 9-13)\n- Phase 3 (5-7 days): Historical snapshots for trend queries (subtasks 14-17)\n- Phase 4 (3-5 days): Response templates by intent (subtasks 18-21)\n- Phase 5 (Enhancement): Date parser, session memory, proactive insights, quick actions (subtasks 22-25) - merged from Task 16\n\n**Files to Modify:**\n- backend/src/main.py (lines 732-798, 1242-1349, 1691-1792)\n- backend/src/ai_client.py (lines 218-231)\n- backend/src/models.py\n\n**Reference:** See AI_Guide_Improvement_Plan.md for detailed analysis",
        "testStrategy": "Re-run all 55 persona tests after each phase. Phase 1 Success: 0 queries with R<=2, <10 queries with Act<=2. Phase 2 Success: Learning/timeline queries score R>=4. Phase 3 Success: Comparison queries score R>=4, Act>=3.",
        "status": "done",
        "dependencies": [
          "21"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Phase 1.1: Expand STATUS_CHECK Intent Patterns",
            "description": "Add missing patterns for risks, blockers, bottlenecks to STATUS_CHECK intent",
            "dependencies": [],
            "details": "**DONE:** Added patterns to STATUS_CHECK in main.py:749-751\n\n**Patterns added:**\n- r\"\\b(risks?|blockers?|bottlenecks?|impediments?)\\b\"\n- r\"\\bwhat('s| is| are).*\\b(blocking|stuck|delayed|holding up)\\b\"\n\n**Queries Fixed:**\n- 'What are the key risks or blockers?' (R=1→4)\n- 'What's blocking the team today?' (R=2→4)\n- 'Any bottlenecks in delivery?' (R=2→4)",
            "status": "done",
            "testStrategy": "Run targeted tests for the 3 affected queries, verify R>=4",
            "parentId": "22"
          },
          {
            "id": 2,
            "title": "Phase 1.2: Add OWNERSHIP Intent Detection",
            "description": "Create new OWNERSHIP intent type with patterns for ownership queries",
            "dependencies": [],
            "details": "**DONE:** Added OWNERSHIP intent to QueryIntent enum (main.py:709) and patterns (main.py:790-795)\n\n**Patterns added:**\n- r\"\\b(who owns|owner of|responsible for|who manages)\\b\"\n- r\"\\bwho.*\\b(contact|ask about|handles|maintains)\\b\"\n- r\"\\b(ownership|responsible|in charge of)\\b\"\n\n**Query Fixed:**\n- 'Who owns the backend service?' (R=2→4)",
            "status": "done",
            "testStrategy": "Run ownership query, verify R>=4",
            "parentId": "22"
          },
          {
            "id": 3,
            "title": "Phase 1.3: Add ACTION_GUIDANCE Patterns",
            "description": "Add patterns for learning-friendly and beginner task queries",
            "dependencies": [],
            "details": "**DONE:** Added learning/starter patterns to ACTION_GUIDANCE in main.py:776-779\n\n**Patterns added:**\n- r\"\\b(good for|suitable for|appropriate for)\\s*(learning|beginners?|new|onboarding)\\b\"\n- r\"\\b(starter|beginner|easy|simple|introductory)\\s*(tasks?|work|items?)\\b\"\n- r\"\\bfirst\\s*(task|issue|thing|item)\\s*(to|for)\\b\"\n\n**Query Fixed:**\n- 'Show me low priority tasks good for learning' (R=2→4)",
            "status": "done",
            "testStrategy": "Run learning query, verify R>=4",
            "parentId": "22"
          },
          {
            "id": 4,
            "title": "Phase 1.4: Add COMPARISON Intent Patterns",
            "description": "Add patterns for historical and trend comparison queries",
            "dependencies": [],
            "details": "**DONE:** Added historical/trend patterns to COMPARISON in main.py:768-771\n\n**Patterns added:**\n- r\"\\bvs\\.?\\s*(last|previous)\\s*(month|quarter|year|week)\\b\"\n- r\"\\b(historical|trend|over time|progress)\\b\"\n- r\"\\b(how has|what changed|growth|decline)\\b\"\n\n**Query Fixed (partial - needs Phase 3 data):**\n- 'Compare agent count by status vs last month' (R=2→3, needs snapshot data for R=4)",
            "status": "done",
            "testStrategy": "Verify intent is correctly classified even without historical data",
            "parentId": "22"
          },
          {
            "id": 5,
            "title": "Phase 1.5: Enhance System Prompt for Query Relevance",
            "description": "Expand the AI system prompt with detailed context about data model, entities, and query interpretation to improve response relevance",
            "dependencies": [],
            "details": "**DONE:** Enhanced system prompt in ai_client.py:218-268\n\n**Added sections:**\n- DATA MODEL UNDERSTANDING: Entity types (TASKS, AGENTS, MEETINGS, DECISIONS) with field descriptions\n- QUERY INTERPRETATION RULES: 7 rules for blockers, timeline, ownership, capacity, learning, dates, overdue\n- PERSONA AWARENESS: 4 persona detection patterns (PM, technical, executive, new member)\n\n**Impact:** Should improve relevance scores from avg 3.95 to 4.3+ by helping AI understand what users are really asking",
            "status": "done",
            "testStrategy": "Run the 7 low-relevance queries, verify R improves from <=2 to >=3",
            "parentId": "22"
          },
          {
            "id": 6,
            "title": "Phase 1.6: Enhance AI System Prompt for Actionability",
            "description": "Add actionability guidelines to the AI system prompt",
            "dependencies": [],
            "details": "**DONE:** Added actionability guidelines to system prompt in ai_client.py:252-265\n\n**Added sections:**\n- ACTIONABILITY GUIDELINES: 6 rules for next steps, contacts, highlighting urgent items, action required\n- RESPONSE FORMAT: 5 formatting rules for bullets, dates, assignees, recommendations\n\n**Impact:** Should improve actionability scores from avg 3.38 to 4.0+",
            "status": "done",
            "testStrategy": "Re-run all 55 tests, verify actionability avg improves to 4.0+",
            "parentId": "22"
          },
          {
            "id": 7,
            "title": "Phase 1.7: Add Action Hints to AI Context",
            "description": "Pass action suggestions to AI as context hints",
            "dependencies": [
              6
            ],
            "details": "**DONE:** Added _generate_action_hints() function (main.py:1103-1151) and integrated into query flow (main.py:1833-1839)\n\n**Implementation:**\n- New function generates intent-specific hints based on query type and data count\n- Hints include: contact suggestions, pattern highlighting, urgency guidance\n- Hints added to full_context before AI call as [Suggested Response Actions]\n\n**Purpose:** Help AI include actionable next steps in responses",
            "status": "done",
            "testStrategy": "Verify AI responses include relevant action suggestions",
            "parentId": "22"
          },
          {
            "id": 8,
            "title": "Phase 1 Validation: Re-run All 55 Persona Tests",
            "description": "Validate Phase 1 improvements with full test suite",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6,
              7
            ],
            "details": "**DONE:** Phase 1 validation complete!\n\n**Results (Before → After):**\n| Metric | Before | After | Improvement |\n|--------|--------|-------|-------------|\n| Relevance | 3.95 | 4.18 | +5.8% |\n| Accuracy | 4.87 | 4.76 | -2.3% (within variance) |\n| Actionability | 3.38 | 4.45 | +31.7% |\n| Overall | 4.07 | 4.47 | +9.8% |\n\n**Key improvements:**\n- 'What are the key risks or blockers?' R: 1→4\n- 'Any bottlenecks in delivery?' R: 2→5\n- 'Who owns the backend service?' R: 2→5\n- Actionability dramatically improved due to Next Steps guidance\n\n**Report:** AI_Guide_Persona_Test_Report.md",
            "status": "done",
            "testStrategy": "Full persona test suite execution",
            "parentId": "22"
          },
          {
            "id": 9,
            "title": "Phase 2.1: Add Task Complexity Fields to Data Model",
            "description": "Add complexity, learning_friendly, skills_required fields to Task model",
            "dependencies": [
              8
            ],
            "details": "**DONE:** Added to models.py:115-147\n\n**Added:**\n- TaskComplexity enum: beginner, intermediate, advanced\n- Task.complexity: Optional[TaskComplexity]\n- Task.learning_friendly: bool = False\n- Task.skills_required: List[str] = []\n\n**Test Result:** 'Show me low priority tasks good for learning' R: 2→5",
            "status": "done",
            "testStrategy": "Verify model accepts new fields, migration completes without errors",
            "parentId": "22"
          },
          {
            "id": 10,
            "title": "Phase 2.2: Add Agent Timeline Fields to Data Model",
            "description": "Add target_deployment_date, deployment_blockers, next_milestone fields to Agent model",
            "dependencies": [
              8
            ],
            "details": "**DONE:** Added to models.py:161-167\n\n**Added to Agent model:**\n- target_deployment_date: Optional[datetime]\n- deployment_blockers: List[str] = []\n- next_milestone: Optional[str]\n- owner_contact: Optional[str]\n- team: Optional[str]\n\n**Test Result:** 'What's the agent deployment timeline?' R: 2→4",
            "status": "done",
            "testStrategy": "Verify model accepts new fields, test timeline queries",
            "parentId": "22"
          },
          {
            "id": 11,
            "title": "Phase 2.3: Add Owner/Contact Fields to All Models",
            "description": "Add owner_name, owner_contact, team fields to relevant models",
            "dependencies": [
              8
            ],
            "details": "**DONE:** Added to models.py\n\n**Task model (lines 142-145):**\n- owner_name: Optional[str]\n- owner_contact: Optional[str]\n- team: Optional[str]\n\n**Agent model (lines 165-167):**\n- owner_contact: Optional[str]\n- team: Optional[str]\n\n**Test Result:** 'Who owns the backend service?' R: 2→4",
            "status": "done",
            "testStrategy": "Verify ownership queries return contact info",
            "parentId": "22"
          },
          {
            "id": 12,
            "title": "Phase 2.4: Update Query Builder for New Fields",
            "description": "Update _build_cosmos_query to filter/sort by new fields",
            "dependencies": [
              9,
              10,
              11
            ],
            "details": "**DONE:** Updated main.py\n\n**_build_cosmos_query (lines 1394-1414):**\n- learning_friendly filter\n- complexity filter\n- owner filter (searches owner, owner_name, assigned_to)\n- team filter\n\n**_classify_intent (lines 883-911):**\n- Learning pattern extraction → learning_friendly, complexity params\n- Ownership subject extraction\n- Timeline flag extraction\n\n**_generate_action_hints (lines 1155-1191):**\n- Enhanced hints for learning queries\n- Enhanced hints for ownership queries\n- Timeline hints for agent lists",
            "status": "done",
            "testStrategy": "Verify queries return correctly filtered results",
            "parentId": "22"
          },
          {
            "id": 13,
            "title": "Phase 2 Validation: Re-run Targeted Tests",
            "description": "Validate Phase 2 improvements with targeted tests",
            "dependencies": [
              12
            ],
            "details": "**DONE:** Phase 2 validation complete!\n\n**Key Query Improvements:**\n| Query | Before | After |\n|-------|--------|-------|\n| 'Show me low priority tasks good for learning' | R=2 | R=5 |\n| 'What's the agent deployment timeline?' | R=2 | R=4 |\n| 'Who owns the backend service?' | R=2 | R=4 |\n\n**Overall Metrics (Post Phase 2):**\n- Relevance: 4.13/5\n- Accuracy: 4.58/5\n- Actionability: 4.51/5\n- Overall: 4.41/5\n- All 55 tests passed (100%)",
            "status": "done",
            "testStrategy": "Run targeted tests for affected queries",
            "parentId": "22"
          },
          {
            "id": 14,
            "title": "Phase 3.1: Create Snapshot Service",
            "description": "Build service to capture daily/weekly snapshots for trend analysis",
            "dependencies": [
              13
            ],
            "details": "**DONE:** Created backend/src/snapshot_service.py (280 lines)\n\n**SnapshotService class features:**\n- capture_daily_snapshot(): Captures metrics for tasks, agents, decisions, meetings\n- _get_task_metrics(): Counts by status, priority, overdue\n- _get_agent_metrics(): Counts by status, tier, production/development\n- _get_decision_metrics(): Counts by category\n- _get_meeting_metrics(): Counts by status\n- get_snapshot_by_id(): Retrieve specific snapshot\n- get_snapshot(days_ago): Get historical snapshot\n- get_snapshots_range(): Get snapshots in date range\n- clear_all_snapshots(): Delete all snapshots (for test→prod transition)\n- format_comparison(): Format comparison text for AI context\n\n**CosmosDB:** Auto-creates 'snapshots' container with partition key /snapshot_type",
            "status": "done",
            "testStrategy": "Verify snapshots are created and stored correctly",
            "parentId": "22"
          },
          {
            "id": 15,
            "title": "Phase 3.2: Implement COMPARISON Intent Handler",
            "description": "Add handler to fetch and format historical comparisons",
            "dependencies": [
              14
            ],
            "details": "**DONE:** Added COMPARISON intent handler to main.py:1879-1912\n\n**Implementation:**\n- Detects time period from query: 'last week' (7 days), 'last month' (30 days), 'yesterday' (1 day)\n- Calls snapshot_service.capture_daily_snapshot() for current state\n- Calls snapshot_service.get_snapshot(days_ago) for historical state\n- Uses snapshot_service.format_comparison() to create AI context\n- Returns comparison_status: 'full', 'partial', or 'none'\n\n**Context Added to AI:**\n- **Comparison with Previous Period:** section\n- Task totals with deltas (+/-)\n- Agent status breakdown with changes\n- Production/development counts comparison",
            "status": "done",
            "testStrategy": "Create test snapshots, verify comparison output",
            "parentId": "22"
          },
          {
            "id": 16,
            "title": "Phase 3.3: Snapshot API Endpoints",
            "description": "Add REST API endpoints for snapshot management",
            "dependencies": [
              14
            ],
            "details": "**DONE:** Added snapshot endpoints to main.py:1994-2086\n\n**Endpoints Created:**\n1. POST /api/snapshots/capture - Manual snapshot capture (returns full snapshot data)\n2. GET /api/snapshots/{snapshot_id} - Get specific snapshot by ID\n3. GET /api/snapshots/compare/{days_ago} - Compare current vs N days ago\n4. GET /api/snapshots - List recent snapshots (default limit 30)\n5. DELETE /api/snapshots - Clear all snapshots (for test→prod transition)\n\n**Note:** Implemented as on-demand API instead of scheduled task. Snapshots captured:\n- Automatically on COMPARISON queries\n- Manually via POST /api/snapshots/capture\n- Can schedule externally via cron/Azure Functions calling the API",
            "status": "done",
            "testStrategy": "Verify endpoints return correct data via curl/Postman",
            "parentId": "22"
          },
          {
            "id": 17,
            "title": "Phase 3 Validation: Test Comparison Queries + UI",
            "description": "Validate historical comparison functionality and management UI",
            "dependencies": [
              15,
              16
            ],
            "details": "**DONE:** Phase 3 validation complete!\n\n**API Testing:**\n- POST /api/snapshots/capture - Working, returns full snapshot data\n- GET /api/snapshots/compare/7 - Returns partial status (no historical data yet)\n- DELETE /api/snapshots - Working, returns deleted_count\n\n**Frontend UI Added (audit/page.tsx):**\n- Historical Snapshots card on Audit Trail page\n- Shows snapshot count and latest capture date\n- Clear Snapshots button with confirmation dialog\n- AlertDialog explains when to clear (test→prod transition)\n\n**New Components:**\n- Created components/ui/alert-dialog.tsx (shadcn/ui AlertDialog)\n- Added @radix-ui/react-alert-dialog dependency\n\n**Fixes Applied:**\n- HTML hydration fix: AlertDialogDescription uses asChild with <div> wrapper\n\n**Note:** Full comparison scoring requires historical snapshot accumulation over time",
            "status": "done",
            "testStrategy": "Run comparison queries, verify accuracy",
            "parentId": "22"
          },
          {
            "id": 18,
            "title": "Phase 4.1: Create Response Templates by Intent",
            "description": "Build structured response templates for each intent type",
            "dependencies": [
              17
            ],
            "details": "**DONE:** Added RESPONSE_TEMPLATES dict to main.py:1133-1226\n\n**Templates Created (9 total):**\n- STATUS_CHECK: Status summary, top 3 items, attention section, next steps\n- AGGREGATION: Headline number, breakdown table, key insight, recommendation\n- LIST: Count, formatted items, summary, next steps\n- ACTION_GUIDANCE: Recommendation, rationale, who to contact, getting started\n- OWNERSHIP: Direct answer, contact details, team, reach suggestion\n- COMPARISON: Headline change, +/- indicators, trend, implication\n- EXPLANATION: Definition, context, examples, related concepts\n- AUDIT: Time context, chronological activities, patterns, notable items\n- DETAIL: Entity name, organized sections, related items, actions\n\n**Integration:** main.py:2125-2134 - templates injected into AI context",
            "status": "done",
            "testStrategy": "Verify templates produce consistent, actionable responses",
            "parentId": "22"
          },
          {
            "id": 19,
            "title": "Phase 4.2: Add Contact Information Enhancement",
            "description": "Auto-enhance responses with relevant contact information",
            "dependencies": [
              18
            ],
            "details": "**DONE:** Added _extract_contact_info() function to main.py:1300-1383\n\n**Features:**\n- Extracts contacts from: assigned_to, owner, owner_contact, facilitator, made_by\n- Prioritizes contacts for blocked/urgent items (marked NEEDS ATTENTION)\n- Shows up to 5 contacts with role, contact details, team affiliation\n- Limits output to top 3 urgent + 2 others\n- Groups by name to avoid duplicates\n\n**Integration:** main.py:2128-2136 - contact info injected into AI context\n\n**Test Results:** Queries now include relevant contacts in Next Steps section",
            "status": "done",
            "testStrategy": "Verify responses include relevant contacts",
            "parentId": "22"
          },
          {
            "id": 20,
            "title": "Phase 4.3: Enhanced Test Suite Implementation",
            "description": "Implement comprehensive test suite for production readiness",
            "dependencies": [
              18,
              19
            ],
            "details": "**DONE:** Created 7 new test files in backend/tests/\n\n**Files Created:**\n1. conftest.py - Shared fixtures (query_guide, helpers, assertions)\n2. test_template_compliance.py - 7 test classes, validates response format by intent\n3. test_contact_extraction.py - 4 test classes, contact surfacing accuracy\n4. test_comparison_queries.py - 5 test classes, historical/trend query handling\n5. test_edge_cases.py - 6 test classes, error handling, ambiguity, security\n6. test_fourth_terminology.py - 5 test classes, company-specific terminology\n7. test_response_quality.py - 5 test classes, length/structure/tone/consistency\n\n**Total New Tests:** ~80 test methods across 32 test classes\n\n**Coverage Areas:**\n- Response template compliance by intent type\n- Contact extraction and prioritization\n- Historical comparison handling\n- Edge cases (empty results, long queries, typos, injection)\n- Fourth terminology (tiers, lifecycle, governance)\n- Response quality metrics (length, structure, tone)",
            "status": "done",
            "testStrategy": "pytest backend/tests/ --verbose",
            "parentId": "22"
          },
          {
            "id": 20,
            "title": "Phase 4 Final Validation: Run Full Test Suite",
            "description": "Execute complete test suite to validate all improvements",
            "dependencies": [
              20
            ],
            "details": "**Target Scores:**\n| Metric | Before | After | Improvement |\n|--------|--------|-------|-------------|\n| Relevance | 3.95 | 4.5+ | +14% |\n| Accuracy | 4.87 | 4.9+ | +1% (maintain) |\n| Actionability | 3.38 | 4.2+ | +24% |\n| Overall | 4.07 | 4.5+ | +11% |\n\n**Success Criteria:**\n- 0 queries with R<=2\n- 0 queries with Act<=2\n- All personas avg 4.0+ overall",
            "status": "pending",
            "testStrategy": "Run all persona tests + new enhanced tests, generate final report",
            "parentId": "22"
          },
          {
            "id": 21,
            "title": "Generate Final Improvement Report",
            "description": "Document all changes and final test results",
            "dependencies": [
              20
            ],
            "details": "**Report Contents:**\n1. Executive Summary - Before/after metrics\n2. Changes Implemented - By phase\n3. Test Results - All 55 queries with scores\n4. Code Changes - Files modified with line numbers\n5. Data Model Changes - New fields added\n6. Future Recommendations - Phase 5+ ideas\n\n**Output:** AI_Guide_Improvement_Final_Report.md",
            "status": "pending",
            "testStrategy": "Review report for completeness and accuracy",
            "parentId": "22"
          },
          {
            "id": 22,
            "title": "Phase 5.1: Implement Natural Date/Time Parser",
            "description": "Add date parsing for natural language time references in queries",
            "dependencies": [
              20
            ],
            "details": "**DONE:** Enhanced _extract_date_references() in main.py:1386-1540\n\n**Patterns Added:**\n- last month, this quarter, last quarter\n- end of week (eow, by friday)\n- end of month (eom, by month end)\n- Specific weekday ('on Monday', 'by Tuesday')\n\n**Existing Patterns (already implemented):**\n- this week, last week, next week\n- today, yesterday, tomorrow\n- this month\n- overdue, due soon\n- last N days, next N days\n\n**Returns:** start_date, end_date, timeframe (ISO format, UTC)",
            "status": "done",
            "testStrategy": "Test all date expressions resolve to correct date ranges, verify timezone handling",
            "parentId": "22"
          },
          {
            "id": 23,
            "title": "Phase 5.2: Build Session-Based Conversation Memory",
            "description": "Track conversation context within session for follow-up questions",
            "dependencies": [
              20
            ],
            "details": "**DONE:** Implemented conversation memory system\n\n**Backend (main.py:695-862):**\n- ConversationTurn dataclass: query, response, intent, entities, extracted_context\n- ConversationMemory class: sessions dict, 30min TTL, max 5 turns\n- _resolve_pronouns(): handles 'those', 'them', 'it' + 'which are overdue?' patterns\n- Integration in query_agent(): session_id handling, context injection, turn storage\n\n**Frontend (GuideProvider.tsx):**\n- generateSessionId(): unique session per widget open\n- sessionIdRef: persists during session, resets on close/clear\n- api.guide.query() updated with session_id parameter\n\n**Models (models.py:389):**\n- AgentQueryRequest.session_id: Optional[str]\n\n**API (api.ts):**\n- GuideQueryRequest.session_id added\n- api.guide.query() accepts sessionId parameter",
            "status": "done",
            "testStrategy": "Test follow-up queries use previous context, verify session isolation",
            "parentId": "22"
          },
          {
            "id": 24,
            "title": "Phase 5.3: Add Proactive Insights on Widget Open",
            "description": "Surface relevant insights automatically when user opens AI Guide widget",
            "dependencies": [
              20
            ],
            "details": "**DONE:** Implemented proactive insights system\n\n**Backend (main.py:2445-2659):**\n- GET /api/agent/insights?page=X endpoint\n- InsightItem model: id, type (warning/info/action), title, description, count, action_label, action_query\n- Page-specific insights:\n  - tasks/dashboard: overdue, blocked, unassigned high-priority, due this week\n  - agents: integration issues, in development count\n  - meetings: next meeting, unprocessed transcripts\n  - governance: pending proposals, recent decisions\n\n**Frontend:**\n- api.ts: InsightItem, InsightsResponse types, api.guide.getInsights()\n- GuideProvider.tsx: insights state, 60s cache, fetchInsights on widget open\n- GuideInsightsBar.tsx: displays insights with dismiss/action buttons\n- GuideWidgetPanel.tsx: shows insights above empty state\n\n**Features:**\n- 60 second cache to reduce API calls\n- Dismissible cards\n- Action buttons that run suggested queries",
            "status": "done",
            "testStrategy": "Test insights are relevant per page, verify caching works, test dismissal",
            "parentId": "22"
          },
          {
            "id": 25,
            "title": "Phase 5.4: Implement Quick Actions System",
            "description": "Allow AI Guide to trigger read-only frontend actions",
            "dependencies": [
              20
            ],
            "details": "**DONE:** Implemented quick actions system\n\n**Backend (already existed - main.py:1147-1280):**\n- _generate_action_suggestions() generates context-aware actions\n- ActionSuggestion model with action_type and params\n- Action types: query, filter, navigate, show_detail, export, create\n\n**Frontend:**\n- api.ts: ActionSuggestion type, ChatMessage.suggestions, GuideQueryResponse.suggestions\n- GuideProvider.tsx: executeAction() handles all action types:\n  - query: runs sendMessage()\n  - navigate: router.push()\n  - filter/show_detail/export: dispatches CustomEvents\n  - create: logged (read-only enforcement)\n- GuideActionSuggestions.tsx: displays action buttons with icons\n- GuideChatMessage.tsx: shows action suggestions below responses\n- GuideWidgetPanel/GuideWidget: passes executeAction through component tree\n\n**Read-Only Enforcement:**\n- create actions filtered out in UI\n- Only query, navigate, filter, show_detail, export are actionable",
            "status": "done",
            "testStrategy": "Test each action type executes correctly, verify read-only constraint",
            "parentId": "22"
          }
        ],
        "updatedAt": "2025-12-03T03:52:53.287Z"
      },
      {
        "id": "23",
        "title": "Agent Framework Migration Evaluation",
        "description": "Evaluate and potentially migrate AI Guide from custom pipeline to Microsoft Agent Framework (Semantic Kernel, AutoGen, or Azure AI Agent Service)",
        "details": "**Context:** Current AI Guide uses custom pipeline:\n- Pattern-based intent classification\n- Direct CosmosDB queries\n- Azure AI Search for RAG\n- Azure OpenAI GPT-4 for response generation\n\n**Why Evaluate Migration:**\n1. Reduced maintenance - framework handles conversation flow\n2. Multi-turn memory - built-in conversation state management\n3. Tool/plugin ecosystem - pre-built integrations\n4. Enterprise features - observability, guardrails, evaluation\n5. Future-proofing - align with Microsoft AI stack\n\n**Framework Options:**\n\n| Framework | Pros | Cons | Fit |\n|-----------|------|------|-----|\n| Semantic Kernel | .NET/Python, Microsoft supported, Tool calling, Memory plugins | Learning curve, Abstraction overhead | HIGH |\n| AutoGen | Multi-agent patterns, Async conversations, Code execution | Complex for single-agent, Beta features | MEDIUM |\n| Azure AI Agent Service | Managed service, Built-in memory, Easy deployment | Preview, Limited customization, Vendor lock-in | HIGH |\n| LangChain | Largest ecosystem, Many connectors | Python-focused, Frequent breaking changes | MEDIUM |\n\n**Recommended Approach:**\n1. Start with Semantic Kernel (same Azure stack)\n2. Create proof-of-concept with existing guide functionality\n3. Compare performance, latency, cost\n4. Gradual migration if successful",
        "testStrategy": "POC comparison: same 55 persona queries on both architectures, measure latency/accuracy/cost",
        "priority": "medium",
        "dependencies": [
          "22"
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Research and Document Framework Options",
            "description": "Deep-dive into Semantic Kernel, AutoGen, and Azure AI Agent Service capabilities",
            "dependencies": [],
            "details": "**Research Areas:**\n- Multi-turn conversation memory\n- Tool/plugin architecture\n- Azure OpenAI integration\n- CosmosDB connector availability\n- RAG pattern support\n- Observability and tracing\n- Cost implications\n\n**Deliverable:** Framework comparison document with recommendations",
            "status": "pending",
            "testStrategy": "Review documentation, try sample code, document findings",
            "parentId": "23"
          },
          {
            "id": 2,
            "title": "Build Semantic Kernel POC",
            "description": "Create proof-of-concept AI Guide using Semantic Kernel",
            "dependencies": [
              1
            ],
            "details": "**POC Scope:**\n- Same intent classification via SK native functions\n- CosmosDB query tool as SK plugin\n- Azure AI Search as SK memory store\n- GPT-4 via SK chat completion\n- Session memory via SK conversation history\n\n**Files:** backend/src/sk_guide_poc.py (new)",
            "status": "pending",
            "testStrategy": "Run 10 representative queries, compare to current implementation",
            "parentId": "23"
          },
          {
            "id": 3,
            "title": "Performance and Cost Comparison",
            "description": "Benchmark POC against current implementation",
            "dependencies": [
              2
            ],
            "details": "**Metrics:**\n- Response latency (p50, p95, p99)\n- Token usage per query\n- Response quality (manual scoring)\n- Code complexity/maintainability\n- Operational cost projection\n\n**Deliverable:** Comparison report with migration recommendation",
            "status": "pending",
            "testStrategy": "Run full test suite on both, aggregate metrics",
            "parentId": "23"
          },
          {
            "id": 4,
            "title": "Migration Decision and Roadmap",
            "description": "Make go/no-go decision and create migration plan if proceeding",
            "dependencies": [
              3
            ],
            "details": "**Decision Criteria:**\n- Quality >= current (no regression)\n- Latency <= current + 20%\n- Cost <= current + 30%\n- Maintainability improvement\n\n**If GO:** Create phased migration plan\n**If NO-GO:** Document learnings, revisit in 6 months",
            "status": "pending",
            "testStrategy": "Stakeholder review and sign-off",
            "parentId": "23"
          }
        ],
        "updatedAt": "2025-11-28T10:30:00.000Z"
      },
      {
        "id": "24",
        "title": "Azure Deployment Preparation - First Production Release",
        "description": "Comprehensive preparation for deploying the Agent Architecture Guide platform to Azure for team use",
        "details": "**Objective:** Deploy the complete platform (Frontend + Backend + Infrastructure) to Azure so the Fourth team can begin using it.\n\n**Architecture Overview:**\n- Frontend: Next.js app → Azure Static Web Apps or App Service\n- Backend: FastAPI (Python) → Azure App Service or Container Apps\n- Database: Azure Cosmos DB (already provisioned)\n- Search: Azure AI Search (already provisioned)\n- AI: Azure OpenAI GPT-4 (already provisioned)\n- Auth: Microsoft Entra ID (SSO)\n- Storage: Azure Blob Storage (transcripts)\n\n**Key Considerations:**\n1. Single-tenant auth (Fourth Limited only)\n2. Production redirect URIs\n3. Environment variable management (Key Vault)\n4. CORS configuration for production domains\n5. SSL/TLS certificates\n6. Monitoring and logging setup\n7. CI/CD pipeline configuration",
        "testStrategy": "Pre-deployment checklist validation, smoke tests in staging, SSO login verification, API endpoint health checks, end-to-end user journey test",
        "priority": "high",
        "dependencies": [
          "22"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Infrastructure Assessment and Resource Planning",
            "description": "Audit existing Azure resources and plan additional infrastructure needed",
            "dependencies": [],
            "details": "**DONE:** See .taskmaster/docs/task-24-infrastructure-assessment.md\n\n**Existing Resources Verified:**\n- Cosmos DB: agent-architecture-cosmos (active)\n- OpenAI: swedencentral.api.cognitive.microsoft.com (gpt-4o, ada-002)\n- AI Foundry: mb-ai-foundry.openai.azure.com\n- AI Search: agent-demo-search.search.windows.net\n- Blob Storage: agentarchstorage\n- Subscription: c9bed202-c4a1-43e9-922d-9a38d966f83e\n\n**New Resources Needed:**\n- App Service Plan (B2/P1v2): ~$55-75/mo\n- Backend App Service\n- Static Web App (Free/Standard): $0-9/mo\n- Key Vault: ~$0.03/10k ops\n- Application Insights: ~$2-10/mo\n\n**17 secrets identified for Key Vault migration**\n**Naming: agent-arch-{component}-prod**",
            "status": "done",
            "testStrategy": "Verify access to all existing resources via Azure Portal and CLI",
            "parentId": "24"
          },
          {
            "id": 2,
            "title": "Environment Configuration and Secrets Management",
            "description": "Set up Azure Key Vault and migrate all secrets from local .env files",
            "dependencies": [
              1
            ],
            "details": "**Key Vault Setup:**\n1. Create Azure Key Vault in same resource group\n2. Configure access policies (App Service managed identity)\n3. Migrate secrets from backend/.env:\n   - COSMOS_ENDPOINT, COSMOS_KEY\n   - AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY\n   - AZURE_SEARCH_ENDPOINT, AZURE_SEARCH_KEY\n   - AZURE_STORAGE_CONNECTION_STRING\n   - ENTRA_CLIENT_ID, ENTRA_CLIENT_SECRET, ENTRA_TENANT_ID\n   - JWT_SECRET_KEY\n4. Create environment-specific configs (dev, staging, prod)\n\n**Files to Update:**\n- backend/src/config.py - add Key Vault integration\n- Create azure-keyvault-secrets.json reference\n\n**Security:**\n- Enable soft-delete and purge protection\n- Set up access logging",
            "status": "pending",
            "testStrategy": "Test secret retrieval from Key Vault using managed identity",
            "parentId": "24"
          },
          {
            "id": 3,
            "title": "Backend Deployment Configuration",
            "description": "Prepare FastAPI backend for Azure App Service deployment",
            "dependencies": [
              2
            ],
            "details": "**CAN DO NOW - Deployment Artifacts:**\n1. Create requirements.txt freeze (production deps only)\n2. Create startup.sh script for App Service\n3. Configure gunicorn/uvicorn for production\n4. Set up health check endpoint (/health)\n5. Configure CORS for production frontend domain\n\n**Files to Create/Update:**\n- backend/startup.sh\n- backend/gunicorn.conf.py\n- backend/.deployment (Azure deployment config)\n- backend/web.config (IIS configuration if needed)\n\n**App Service Configuration:**\n- Python 3.11 runtime\n- B2 or P1v2 plan (minimum for production)\n- Always On: enabled\n- HTTPS Only: enabled\n- Managed Identity: enabled\n\n**Environment Variables (from Key Vault):**\n- @Microsoft.KeyVault(VaultName=...;SecretName=...)\n\n**Entra Placeholder Approach:**\n- Use existing dev Entra values for now\n- Key Vault references ready for prod values when available\n- Auth will work with localhost redirects until Task 13 complete",
            "status": "pending",
            "testStrategy": "Local docker build test, deploy to staging slot first",
            "parentId": "24"
          },
          {
            "id": 4,
            "title": "Frontend Build and Deployment Configuration",
            "description": "Prepare Next.js frontend for Azure Static Web Apps or App Service deployment",
            "dependencies": [
              2
            ],
            "details": "**CAN DO NOW - Build Configuration:**\n1. Update next.config.js for production\n2. Configure environment variables for production API URL\n3. Set up static export or SSR mode\n4. Optimize bundle size (analyze with @next/bundle-analyzer)\n\n**Environment Variables:**\n- NEXT_PUBLIC_API_URL=https://api.{yourdomain}.azurewebsites.net\n- NEXT_PUBLIC_ENTRA_CLIENT_ID (use dev value for now)\n- NEXT_PUBLIC_ENTRA_TENANT_ID (use dev value for now)\n\n**Static Web Apps Configuration:**\n- Create staticwebapp.config.json\n- Configure routing rules\n- Set up custom domain (if available)\n\n**Or App Service Configuration:**\n- Node.js 20 LTS runtime\n- PM2 ecosystem file for process management\n- Custom startup command\n\n**Entra Placeholder Approach:**\n- Build and deploy work with any valid Entra config\n- Auth redirect URIs updated in Task 5 when admin provides access",
            "status": "pending",
            "testStrategy": "Production build locally, test all routes work with static export",
            "parentId": "24"
          },
          {
            "id": 5,
            "title": "Microsoft Entra ID Production Configuration",
            "description": "Update app registration with production redirect URIs and verify SSO",
            "dependencies": [
              3,
              4
            ],
            "details": "**⚠️ BLOCKED: Requires Task 13 completion (Admin/IT access to Entra ID)**\n\n**When unblocked - App Registration Updates:**\n1. Add production redirect URIs:\n   - https://{frontend-domain}/auth/callback\n   - https://{frontend-domain}/api/auth/callback\n2. Update logout URLs\n3. Review and tighten API permissions\n4. Generate new client secret for production (shorter expiry)\n\n**Conditional Access (optional):**\n- Require MFA for all users\n- Block non-corporate IPs\n- Require compliant devices\n\n**Testing:**\n- Test SSO flow end-to-end\n- Verify token claims include required user info\n- Test session expiry and refresh\n\n**Prerequisite:** Task 13 - Azure App Registration Setup (awaiting IT admin)",
            "status": "pending",
            "testStrategy": "Full SSO login/logout cycle in staging environment",
            "parentId": "24"
          },
          {
            "id": 6,
            "title": "CI/CD Pipeline Setup",
            "description": "Configure GitHub Actions or Azure DevOps for automated deployments",
            "dependencies": [
              3,
              4
            ],
            "details": "**GitHub Actions Workflows:**\n\n1. Backend CI/CD (.github/workflows/backend-deploy.yml):\n   - Trigger: push to main (backend/** changes)\n   - Jobs: lint, test, build, deploy to App Service\n   - Use azure/webapps-deploy@v2\n   - Secrets: AZURE_WEBAPP_PUBLISH_PROFILE\n\n2. Frontend CI/CD (.github/workflows/frontend-deploy.yml):\n   - Trigger: push to main (frontend/** changes)\n   - Jobs: lint, build, deploy to Static Web Apps\n   - Use Azure/static-web-apps-deploy@v1\n\n**Deployment Slots:**\n- staging slot for preview deployments\n- Swap to production after validation\n\n**Branch Protection:**\n- Require PR reviews for main\n- Require status checks to pass",
            "status": "pending",
            "testStrategy": "Trigger test deployment via PR, verify staging deployment works",
            "parentId": "24"
          },
          {
            "id": 7,
            "title": "Monitoring and Observability Setup",
            "description": "Configure Application Insights, logging, and alerting",
            "dependencies": [
              3,
              4
            ],
            "details": "**Application Insights:**\n1. Create App Insights resource\n2. Add instrumentation to backend (opentelemetry-instrumentation-fastapi)\n3. Add frontend monitoring (applicationinsights-react)\n4. Configure sampling rate\n\n**Logging:**\n- Structured logging (JSON format)\n- Log levels: INFO in prod, DEBUG in staging\n- Correlation IDs for request tracing\n\n**Alerts:**\n- 5xx error rate > 1%\n- Response time p95 > 3s\n- Failed dependencies\n- App Service health degradation\n\n**Dashboard:**\n- Create Azure Dashboard with key metrics\n- Request rate, error rate, latency\n- Cosmos DB RU consumption\n- OpenAI token usage",
            "status": "pending",
            "testStrategy": "Verify telemetry flows to App Insights, test alert triggers",
            "parentId": "24"
          },
          {
            "id": 8,
            "title": "Security Hardening (Non-Auth)",
            "description": "Apply security best practices that don't require Entra ID access",
            "dependencies": [
              2
            ],
            "details": "**Network Security (can do now):**\n- Enable Azure Private Endpoints for Cosmos DB, Storage\n- Configure App Service access restrictions (if VPN/corporate network)\n- Enable DDoS protection (basic included)\n\n**Application Security (can do now):**\n- Verify all secrets in Key Vault (no hardcoded secrets)\n- Enable HTTPS redirect\n- Configure security headers (CSP, HSTS, X-Frame-Options)\n- Review CORS policy (production domains only)\n\n**Data Protection (can do now):**\n- Verify Cosmos DB encryption at rest (default)\n- Enable TLS 1.2+ only\n- Review PII handling in logs\n\n**Compliance (can do now):**\n- Document data residency (UK South region)\n- Review for GDPR compliance\n- Audit logging enabled\n\n**NOTE:** Auth-specific security (Conditional Access, MFA policies) deferred to subtask 5",
            "status": "pending",
            "testStrategy": "Security scan with Azure Security Center, run OWASP ZAP baseline",
            "parentId": "24"
          },
          {
            "id": 9,
            "title": "Pre-Deployment Checklist and Documentation (Draft)",
            "description": "Create deployment runbook - can draft most without Entra ID",
            "dependencies": [
              1,
              2,
              3,
              4,
              6,
              7,
              8
            ],
            "details": "**CAN DO NOW - Deployment Runbook:**\n1. Pre-deployment checklist (draft)\n2. Step-by-step deployment procedure\n3. Rollback procedure\n4. Post-deployment verification steps\n5. Contact escalation matrix\n\n**CAN DO NOW - Documentation Updates:**\n- Update README with production URLs (placeholders ok)\n- Create operations guide\n- Document known issues/workarounds\n- Create user onboarding guide (draft)\n\n**Pre-Deployment Checklist (mark what's ready):**\n- [ ] All Azure resources provisioned\n- [ ] Key Vault secrets populated (non-Entra)\n- [ ] ⏳ Entra ID configured with prod URIs (BLOCKED - Task 13)\n- [ ] CI/CD pipelines tested\n- [ ] Monitoring configured\n- [ ] Security review complete (non-auth parts)\n- [ ] Backup/DR plan documented\n- [ ] Stakeholder sign-off obtained\n\n**NOTE:** Final sign-off requires Entra ID completion",
            "status": "pending",
            "testStrategy": "Walkthrough checklist with team, simulate deployment in staging",
            "parentId": "24"
          },
          {
            "id": 10,
            "title": "Production Deployment and Verification",
            "description": "Execute deployment to production and verify all functionality",
            "dependencies": [
              9
            ],
            "details": "**Deployment Steps:**\n1. Deploy backend to App Service (via CI/CD or manual)\n2. Verify backend health endpoint\n3. Deploy frontend to Static Web Apps\n4. Update DNS/custom domains if applicable\n5. Swap staging to production\n\n**Post-Deployment Verification:**\n- [ ] Frontend loads correctly\n- [ ] SSO login works\n- [ ] AI Guide responds to queries\n- [ ] Data displays correctly (tasks, agents, meetings)\n- [ ] All navigation works\n- [ ] No console errors\n- [ ] Monitoring data flows to App Insights\n\n**Smoke Test Scenarios:**\n1. Login via Microsoft SSO\n2. View dashboard\n3. Ask AI Guide a question\n4. Navigate to Agents page\n5. View a task detail\n6. Logout\n\n**Rollback Trigger Criteria:**\n- Critical functionality broken\n- Authentication failures\n- Data not loading\n- Error rate > 5%",
            "status": "pending",
            "testStrategy": "Execute smoke tests, monitor error rates for 1 hour post-deploy",
            "parentId": "24"
          },
          {
            "id": 11,
            "title": "Team Onboarding and Handoff",
            "description": "Onboard team members and establish support procedures",
            "dependencies": [
              10
            ],
            "details": "**Team Onboarding:**\n1. Send access instructions to team\n2. Provide production URL\n3. Quick start guide / demo video\n4. Known limitations document\n\n**Support Procedures:**\n- Define support channels (Teams, email)\n- Create issue reporting template\n- Establish SLA expectations\n- Document common troubleshooting steps\n\n**Feedback Collection:**\n- Set up feedback form or channel\n- Schedule 2-week check-in\n- Plan iteration based on feedback\n\n**Success Metrics:**\n- Daily active users\n- AI Guide query volume\n- Error rate trends\n- User feedback scores",
            "status": "pending",
            "testStrategy": "Verify all team members can login and complete basic tasks",
            "parentId": "24"
          }
        ],
        "updatedAt": "2025-12-03T03:53:04.666Z"
      },
      {
        "id": "25",
        "title": "Build Internal Ticketing & Idea Submission System",
        "description": "Create comprehensive internal feedback hub for staff to submit bug reports, feature requests, and improvement ideas with upvoting, status tracking, admin triage, and task system integration",
        "details": "## Backend Implementation (FastAPI)\n\n### 1. Database Schema (Cosmos DB - 'submissions' container)\n```python\nclass SubmissionCategory(str, Enum):\n    BUG_REPORT = \"Bug Report\"\n    FEATURE_REQUEST = \"Feature Request\"\n    IMPROVEMENT_IDEA = \"Improvement Idea\"\n    QUESTION = \"Question\"\n\nclass SubmissionStatus(str, Enum):\n    SUBMITTED = \"Submitted\"\n    UNDER_REVIEW = \"Under Review\"\n    IN_PROGRESS = \"In Progress\"\n    COMPLETED = \"Completed\"\n    DECLINED = \"Declined\"\n\nclass SubmissionPriority(str, Enum):\n    CRITICAL = \"Critical\"\n    HIGH = \"High\"\n    MEDIUM = \"Medium\"\n    LOW = \"Low\"\n\nclass Comment(BaseModel):\n    id: str\n    user: str\n    content: str\n    created_at: datetime\n\nclass Submission(BaseModel):\n    id: Optional[str]\n    title: str\n    description: str\n    category: SubmissionCategory\n    priority: SubmissionPriority\n    status: SubmissionStatus = SubmissionStatus.SUBMITTED\n    submitted_by: str  # User name\n    submitted_at: datetime\n    upvotes: List[str] = []  # List of user names who upvoted\n    upvote_count: int = 0\n    comments: List[Comment] = []\n    assigned_to: Optional[str] = None\n    tags: List[str] = []\n    linked_task_id: Optional[str] = None  # Reference to created task\n    resolved_at: Optional[datetime] = None\n    resolution_notes: Optional[str] = None\n```\n\n### 2. API Endpoints (src/routers/submissions.py)\n```python\n# Create router\nrouter = APIRouter(prefix=\"/api/submissions\", tags=[\"submissions\"])\n\n# Endpoints:\nPOST /api/submissions - Create new submission\nGET /api/submissions - List with filters (status, category, assignee, sort)\nGET /api/submissions/{id} - Get single submission\nPUT /api/submissions/{id} - Update submission (admin only for status/assignee)\nDELETE /api/submissions/{id} - Delete submission (admin only)\nPOST /api/submissions/{id}/upvote - Toggle upvote\nPOST /api/submissions/{id}/comments - Add comment\nPUT /api/submissions/{id}/comments/{comment_id} - Edit comment\nDELETE /api/submissions/{id}/comments/{comment_id} - Delete comment\nPOST /api/submissions/{id}/convert-to-task - Create task from submission\nGET /api/submissions/stats - Dashboard statistics\n```\n\n### 3. Key Backend Functions\n- **Upvote System**: Toggle user in upvotes array, update upvote_count atomically\n- **Notification Service**: Send email on status changes using Azure Communication Services or SendGrid\n- **Task Conversion**: Create Task object with submission details, link submission.linked_task_id\n- **Search Integration**: Index submissions in Azure AI Search for full-text search\n- **Access Control**: Admin-only routes for status updates, assignment, deletion\n\n### 4. Database Container Addition\nAdd to database.py container_definitions:\n```python\n(\"submissions\", \"/id\"),\n```\n\n## Frontend Implementation (Next.js/React)\n\n### 5. Submission Form Component (app/feedback/submit/page.tsx)\n- Form with title (Input), description (Textarea), category (Select), priority (Select)\n- Tags input (multi-select or comma-separated)\n- Submitted_by auto-populated from user context (TeamMemberSelect)\n- Validation: Required fields, max lengths\n- Success toast and redirect to submission detail\n\n### 6. Submissions List View (app/feedback/page.tsx)\n- Table/card view with columns: Title, Category, Priority, Status, Upvotes, Submitted By, Date\n- Filters: Category dropdown, Status dropdown, Priority dropdown, Search bar\n- Sort options: Most upvoted, Most recent, Priority, Status\n- Click row to open detail view\n- \"Submit New\" button → submission form\n- Pagination for large lists\n\n### 7. Submission Detail Page (app/feedback/[id]/page.tsx)\n- Display all submission details\n- Upvote button with count (disable if already upvoted)\n- Comments section with add/edit/delete\n- Status badge with color coding\n- Admin actions (conditional render):\n  - Status dropdown to change status\n  - Assign to team member (TeamMemberSelect)\n  - \"Convert to Task\" button\n  - Delete button with confirmation\n- Activity timeline showing status changes\n\n### 8. Admin Dashboard (app/feedback/admin/page.tsx or admin section)\n- Statistics cards: Total submissions, By status, By category, Avg resolution time\n- Triage queue: Submissions in \"Submitted\" status sorted by priority/upvotes\n- Bulk actions: Assign multiple, Change status\n- Filter by assignee to show \"My Assigned\"\n- Quick actions: Approve → In Progress, Decline with reason\n\n### 9. Notification System\n- Backend: Create notification_service.py with send_status_update_email()\n- Trigger on status change: Notify submitter and upvoters\n- Email template with submission title, new status, link to detail page\n- Optional: In-app notifications (toast on page load if user has updates)\n\n### 10. Integration with Existing Task System\n- \"Convert to Task\" button in admin view\n- Create Task with:\n  - title = submission.title\n  - description = submission.description + link to original submission\n  - priority = map submission priority to task priority\n  - category = inferred from submission tags or admin selected\n  - created_from_submission = submission.id\n- Update submission.linked_task_id and status to \"In Progress\"\n- Display linked task in submission detail with clickable link\n\n### 11. Search & Analytics\n- Integrate with Azure AI Search (index submissions like other entities)\n- Search bar in submissions list for full-text search\n- Analytics page:\n  - Submissions over time (line chart)\n  - Category breakdown (pie chart)\n  - Top requested features (by upvotes)\n  - Average time to resolution by category\n  - Most active submitters leaderboard\n\n## Technical Details\n\n### UI Components (Radix UI pattern)\n- SubmissionCard component (reusable)\n- SubmissionFilters component\n- CommentThread component\n- UpvoteButton component (with animation)\n- StatusBadge component (color-coded)\n- ConvertToTaskDialog component\n\n### State Management\n- Use React hooks (useState, useEffect)\n- API calls via lib/api.ts extension:\n```typescript\nsubmissions: {\n  list: (filters?) => GET /api/submissions\n  get: (id) => GET /api/submissions/{id}\n  create: (data) => POST /api/submissions\n  update: (id, data) => PUT /api/submissions/{id}\n  upvote: (id) => POST /api/submissions/{id}/upvote\n  addComment: (id, comment) => POST /api/submissions/{id}/comments\n  convertToTask: (id) => POST /api/submissions/{id}/convert-to-task\n}\n```\n\n### Styling\n- Follow existing card-based layout (similar to tasks/resources pages)\n- Color coding for categories: Bug (red), Feature (blue), Improvement (green), Question (yellow)\n- Status badges: Submitted (gray), Under Review (yellow), In Progress (blue), Completed (green), Declined (red)\n- Upvote button with heart/thumbs-up icon and count\n\n### Security & Permissions\n- All users can submit and upvote\n- Only admins can change status, assign, delete, convert to task\n- Backend validates user role from auth context\n- Rate limiting on upvotes to prevent abuse",
        "testStrategy": "1. Backend API Testing: Test submission CRUD endpoints with valid/invalid data, verify enum validations, test upvote toggle idempotency, test comment operations, verify admin-only routes return 403 for non-admins, test task conversion creates correct Task object and links submission. 2. Database Testing: Verify submissions container creation, test atomic upvote_count updates, validate query performance with filters. 3. Frontend Form Testing: Test submission form validation, verify required fields, test category/priority selections, verify success redirect. 4. List View Testing: Test filtering by category/status/priority, verify search functionality, test sorting options, validate pagination. 5. Detail Page Testing: Test upvote button toggles correctly, verify comments display and CRUD, test admin actions visibility based on role, verify linked task displays. 6. Notification Testing: Trigger status changes and verify emails sent, test email content includes correct details, verify only submitter and upvoters notified. 7. Integration Testing: Convert submission to task and verify task created with correct fields, verify submission.linked_task_id updated, test task link clickable from submission detail. 8. Search Testing: Create submissions and verify indexed in Azure AI Search, test full-text search returns relevant results. 9. Analytics Testing: Submit various entries and verify statistics calculations, test charts render correctly. 10. Access Control Testing: Verify non-admin users cannot change status/assign/delete, test admin users have full access",
        "status": "done",
        "dependencies": [
          "6"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Submissions Database Schema and Models",
            "description": "Define Pydantic models for submissions system with enums for Category, Status, and Priority, plus Comment and Submission models",
            "dependencies": [],
            "details": "Create models in backend/src/models.py: SubmissionCategory enum (Bug Report, Feature Request, Improvement Idea, Question), SubmissionStatus enum (Submitted, Under Review, In Progress, Completed, Declined), SubmissionPriority enum (Critical, High, Medium, Low). Create Comment BaseModel with id, user, content, created_at. Create Submission BaseModel with all fields from task details. Add 'submissions' container to database.py container_definitions with partition key '/id'. Pattern matches existing Task/Agent/Resource models in models.py and database container setup.",
            "status": "pending",
            "testStrategy": "Verify enum values match requirements, test Pydantic model validation with valid/invalid data, confirm container creation in Cosmos DB, test partition key configuration",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Build Backend API Endpoints for Submissions",
            "description": "Create FastAPI router with all submission CRUD endpoints, upvote system, comments, and task conversion",
            "dependencies": [
              1
            ],
            "details": "Create backend/src/routers/submissions.py following pattern from resources.py. Implement endpoints: POST /api/submissions (create), GET /api/submissions (list with filters), GET /api/submissions/{id} (get single), PUT /api/submissions/{id} (update, admin-only status/assignee), DELETE /api/submissions/{id} (admin-only), POST /api/submissions/{id}/upvote (toggle upvote), POST /api/submissions/{id}/comments (add comment), PUT /api/submissions/{id}/comments/{comment_id} (edit), DELETE /api/submissions/{id}/comments/{comment_id} (delete), POST /api/submissions/{id}/convert-to-task (create linked task), GET /api/submissions/stats (dashboard stats). Use db.get_container('submissions') pattern. Implement upvote toggle by checking if user in upvotes array, add/remove atomically. Register router in main.py.",
            "status": "pending",
            "testStrategy": "Test all CRUD operations with Postman/curl, verify filter/sort/search functionality, test upvote idempotency (toggle on/off), test comment CRUD, verify admin-only routes return 403 for non-admins, test task conversion creates Task with correct fields and links submission.linked_task_id, test query parameters and pagination",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Create Frontend TypeScript API Client and Types",
            "description": "Extend lib/api.ts with submissions interfaces and API methods following existing pattern",
            "dependencies": [
              2
            ],
            "details": "Add to frontend/lib/api.ts: Create Submission interface matching backend model (id, title, description, category, priority, status, submitted_by, submitted_at, upvotes, upvote_count, comments, assigned_to, tags, linked_task_id, resolved_at, resolution_notes). Create Comment interface. Add api.submissions object with methods: list(filters?), get(id), create(data), update(id, data), upvote(id), addComment(id, comment), deleteComment(id, commentId), convertToTask(id), getStats(). Follow pattern from api.tasks, api.agents, api.resources using apiFetch helper. All methods return Promise<T> with proper typing.",
            "status": "pending",
            "testStrategy": "Verify TypeScript types compile without errors, test API methods connect to backend endpoints correctly, validate request/response type safety, test error handling",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Build Submissions List and Detail Pages",
            "description": "Create submission list view with filters/search and detail page with upvote/comments following tasks page pattern",
            "dependencies": [
              3
            ],
            "details": "Create frontend/app/feedback/page.tsx for list view with table/card layout showing title, category badge (color-coded: Bug=red, Feature=blue, Improvement=green, Question=yellow), priority, status badge (Submitted=gray, Under Review=yellow, In Progress=blue, Completed=green, Declined=red), upvotes, submitted_by, date. Add filter dropdowns for category, status, priority. Add search bar for title/description. Add sort options (most upvoted, most recent, priority). Add 'Submit New' button opening create dialog. Create frontend/app/feedback/[id]/page.tsx for detail view showing all fields, upvote button (heart icon with count, disabled if already upvoted), comments section with add/edit/delete, status badge, admin-only actions (status dropdown, TeamMemberSelect for assign, Convert to Task button, delete with confirmation). Use Radix UI components (Card, Dialog, Select, Badge) matching app/tasks/page.tsx pattern. Implement real-time upvote count updates.",
            "status": "pending",
            "testStrategy": "Test filter combinations work correctly, verify search filters results, test sort order changes, verify create dialog validation, test upvote button toggles correctly and updates count, test comment CRUD operations, verify admin actions only show for admins, test status badge color coding, verify responsive layout on mobile/desktop, test pagination if implemented",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement Admin Dashboard and Task Conversion Integration",
            "description": "Create admin triage dashboard with stats and implement convert-to-task functionality linking submissions to task system",
            "dependencies": [
              4
            ],
            "details": "Create frontend/app/feedback/admin section (or integrate into existing admin area) with statistics cards: total submissions, by status, by category, avg resolution time. Add triage queue showing 'Submitted' status items sorted by priority/upvotes. Add bulk actions for assign/status change. Create ConvertToTaskDialog component that calls api.submissions.convertToTask(id), which backend implements by creating Task with title=submission.title, description=submission.description + link to submission, priority mapped from submission priority, category from tags/admin selection, created_from_submission=submission.id. Backend updates submission.linked_task_id and status to 'In Progress'. Display linked task in submission detail with clickable link to /tasks. Add notification system (optional): create notification_service.py to send emails on status changes using Azure Communication Services/SendGrid, trigger on status updates to notify submitter and upvoters.",
            "status": "pending",
            "testStrategy": "Test admin dashboard displays correct statistics, verify triage queue filters and sorts correctly, test bulk actions update multiple submissions, test convert-to-task creates Task with correct fields and links submission, verify linked task appears in submission detail with working link, test navigation between submission and linked task, if notifications implemented: test email triggers on status change and reaches correct recipients",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-03T03:53:14.032Z"
      },
      {
        "id": "26",
        "title": "MSFT Ecosystem Integration for Agent-Arch Platform with Azure AI Foundry and Microsoft Services",
        "description": "Integrate the agent-arch platform with Azure AI Foundry and Microsoft services to enable end-to-end agent lifecycle management, including enhanced agent request intake, communication, auto-provisioning, and metrics reporting.",
        "details": "1. Enhanced Agent Request System: Develop structured intake forms for staff and department submissions capturing requester information, problem statement, business impact, and approval workflow fields. Implement validation and role-based access controls.\n\n2. Built-in Communication/Messaging System: Create a messaging interface supporting back-and-forth discussions between requesters and AI architects. Features include @mentions, threaded conversations, and optional Microsoft Teams notifications leveraging Microsoft Graph API.\n\n3. Azure AI Foundry Auto-Provisioning: Implement automation triggered by agent status changes to 'Development' that programmatically creates Azure AI Foundry projects, configures necessary data connections, and links these projects back to the agent-arch platform. Use Azure SDKs and Foundry APIs with managed identities for secure provisioning.\n\n4. Metrics & Reporting Dashboard: Build a dashboard integrated into agent detail pages that aggregates usage statistics, cost data, performance metrics, and health status by pulling data from Azure Monitor and Microsoft Fabric APIs. Ensure real-time updates and role-based data visibility.\n\nImplementation should follow Microsoft best practices for security, scalability, and compliance, leveraging Azure services such as Logic Apps, Azure Functions, and Microsoft Entra ID for authentication and authorization. Use Microsoft Agent Framework and Model Context Protocol (MCP) standards for interoperability and governance. Ensure thorough logging and observability for all integrations.",
        "testStrategy": "1. Validate structured intake forms for completeness, field validation, and correct workflow transitions.\n2. Test messaging system for correct delivery of messages, @mentions functionality, thread integrity, and Teams notification triggers.\n3. Simulate agent status changes to 'Development' and verify automatic provisioning of Azure AI Foundry projects, correct configuration of data connections, and linkage back to agent-arch.\n4. Verify metrics dashboard accuracy by comparing displayed data against Azure Monitor and Fabric source data; test real-time updates and access control.\n5. Conduct end-to-end integration tests covering the full agent lifecycle from request submission through provisioning to monitoring.\n6. Perform security and compliance audits on all new integrations.\n7. Use automated unit, integration, and UI tests to ensure robustness and regression prevention.",
        "status": "pending",
        "dependencies": [
          "22",
          "24"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Develop Enhanced Agent Request Intake System",
            "description": "Create structured intake forms for staff and department submissions capturing requester information, problem statement, business impact, and approval workflow fields with validation and role-based access controls.",
            "dependencies": [],
            "details": "Design and implement intake forms with fields for requester details, problem description, business impact, and approval workflow. Implement client-side and server-side validation. Enforce role-based access control to restrict form access and submission capabilities based on user roles.\n<info added on 2025-12-06T15:22:08.251Z>\nFor the intake form implementation, reference the shadcn-builder visual form builder (https://github.com/iduspara/shadcn-builder/) as inspiration for creating the structured intake form UI. This tool provides drag-and-drop form creation with shadcn/ui components, which aligns with our existing frontend stack (shadcn UI + Tailwind CSS). Consider leveraging similar patterns for building the intake form with fields for requester information, problem statement, business impact, and approval workflow. The visual builder approach can streamline form field configuration and validation rule setup while maintaining consistency with our component library.\n</info added on 2025-12-06T15:22:08.251Z>",
            "status": "pending",
            "testStrategy": "Validate form field completeness and correctness, test validation rules, verify role-based access restrictions, and simulate approval workflow transitions.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Built-in Communication and Messaging System",
            "description": "Create a messaging interface supporting threaded conversations, @mentions, and optional Microsoft Teams notifications using Microsoft Graph API for communication between requesters and AI architects.",
            "dependencies": [
              1
            ],
            "details": "Develop a messaging UI with support for back-and-forth threaded discussions and @mentions. Integrate Microsoft Graph API to send optional notifications to Microsoft Teams channels or users. Ensure message delivery reliability and thread integrity.",
            "status": "pending",
            "testStrategy": "Test message sending and receiving, verify @mentions functionality, validate thread structure, and confirm Microsoft Teams notifications trigger correctly.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Create Azure AI Foundry Management API Client Service",
            "description": "Develop a client service to interact with Azure AI Foundry APIs for project creation, configuration, and management using Azure SDKs and Foundry APIs with managed identities for secure authentication.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement API client service to authenticate using managed identities, create and configure Azure AI Foundry projects, manage data connections based on agent data_sources, and handle project metadata storage. Follow Microsoft security best practices.",
            "status": "pending",
            "testStrategy": "Unit test API client methods, validate authentication flows, simulate project creation and configuration, and verify error handling.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement Auto-Provisioning Workflow Triggered by Agent Status Change",
            "description": "Automate Azure AI Foundry project provisioning triggered when an agent's status changes to 'Development', including linking projects back to the agent-arch platform and tracking deployment status in the UI.",
            "dependencies": [
              3
            ],
            "details": "Develop event-driven workflow that listens for agent status changes to 'Development'. Trigger Azure AI Foundry project creation via API client service. Configure data connections dynamically. Store Foundry project IDs and URLs in agent records. Add UI components to display deployment status and progress.",
            "status": "pending",
            "testStrategy": "Simulate agent status changes, verify auto-provisioning triggers, confirm project creation and linkage, and test deployment status UI updates.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Build Metrics and Reporting Dashboard with Real-Time Updates",
            "description": "Develop a dashboard integrated into agent detail pages that aggregates usage statistics, cost data, performance metrics, and health status by querying Azure Monitor, Microsoft Fabric, and Azure Cost Management APIs with role-based data visibility.",
            "dependencies": [
              4
            ],
            "details": "Create data models and Cosmos DB containers for AgentMetrics. Implement polling and caching services for metrics retrieval. Integrate Azure Monitor Query API and Azure Cost Management API. Build dashboard UI with charts and alerts. Ensure real-time updates and enforce role-based access controls for data visibility.",
            "status": "pending",
            "testStrategy": "Test metrics data retrieval and caching, validate dashboard visualizations, verify cost tracking accuracy, test health status alerts, and confirm role-based data access enforcement.",
            "parentId": "undefined"
          }
        ]
      },
      {
        "id": "27",
        "title": "Research and Redesign PERSONAS.md as Programmatic Subagent System",
        "description": "Transform PERSONAS.md from passive guidance profiles into actionable subagent definitions that can be reliably triggered via hooks/auto-activation and integrated with the SuperClaude command system for autonomous task delegation.",
        "details": "## Research Phase\n\n### 1. Investigate Current Persona Activation Mechanisms\n- **Manual Activation Analysis**: Document current `/persona:<name>` command pattern from PERSONAS.md, identifying why it requires explicit user invocation\n- **Hook System Investigation**: Examine .claude/settings.local.json permissions structure (lines 2-10) to understand current hook capabilities for auto-activation\n- **Trigger Pattern Analysis**: Analyze existing auto-activation patterns in PERSONAS.md (lines 173-180 in context) - file-based (*.tsx→frontend), keyword-based (bug|error→debugger), context-based (TypeError→trace)\n\n### 2. Benchmark Against ClaudeKit/Claude Agent SDK Code Review Agents\n- **Agent Architecture Research**: Use WebSearch to find Claude Agent SDK documentation on subagent patterns, specifically the 6-agent code review system mentioned in user query\n- **Comparison Matrix**: Create comparison table mapping current 9 personas (architect, frontend, backend, analyzer, security, mentor, refactorer, performance, qa) to ClaudeKit's agent specializations\n- **Identify Gaps**: Document differences in activation mechanisms, context sharing, and delegation patterns between current passive personas and SDK subagents\n\n### 3. Analyze Integration with SuperClaude Command System\n- **Command Integration Points**: Review CLAUDE.md auto-activation rules (files, keywords, context triggers) and how personas currently integrate with /user:<command> patterns\n- **MCP Integration**: Examine MCP.md persona preferences (lines mentioning architect→Sequential, frontend→Magic, etc.) to understand tool orchestration expectations\n- **Task Management Hooks**: Investigate how TodoWrite/TodoRead could trigger persona-based task delegation\n\n## Design Phase\n\n### 4. Define Programmatic Subagent Structure\nCreate new YAML-based schema for each persona in PERSONAS.md:\n\n```yaml\npersona_id: architect\nactivation:\n  auto_triggers:\n    file_patterns: [\"**/architecture/**/*\", \"**/*design*/**\"]\n    keywords: [\"design system\", \"architecture\", \"tier classification\", \"ddd\", \"bounded context\"]\n    context_signals: [\"architectural decision\", \"system design\", \"scalability\"]\n  manual_command: /persona:architect\n  hook_integration:\n    pre_task_hook: check_for_design_patterns\n    post_task_hook: suggest_architectural_improvements\ndelegation:\n  can_spawn_subagents: [analyzer, security, performance]\n  requires_approval: false\n  escalation_threshold: complexity_score >= 8\ntool_access:\n  mcp_tools: [Sequential, Context7]\n  native_tools: [Read, Grep, Glob, Edit]\n  tool_orchestration: \"long_form_sequential_analysis\"\noutput_format:\n  communication_style: \"diagrams, trade-offs, future scenarios\"\n  response_template: \"architectural_decision_record\"\n  context_retention: high\n```\n\n### 5. Define Hook-Based Activation System\n- **Pre-execution Hooks**: Create hooks that analyze incoming user requests and activate appropriate personas before task execution\n- **Context-based Routing**: Implement scoring system (0-100) for each persona based on query keywords, file context, and task complexity\n- **Multi-persona Coordination**: Design handoff protocol for sequential persona activation (e.g., analyzer→refactorer→performance from PERSONAS.md lines 131-135)\n\n### 6. Map Personas to Task Delegation Patterns\n- **Complexity-Based Delegation**: Define when personas should use Task tool with subagent_type parameter vs. executing directly\n- **Autonomous Decision Tree**: Create decision flowchart for personas to determine when to:\n  - Execute directly with native tools\n  - Spawn specialized subagent via Task tool\n  - Request user clarification\n  - Delegate to another persona\n- **Checkpoint Integration**: Define when personas should create git checkpoints before risky operations\n\n## Implementation Plan\n\n### 7. Restructure PERSONAS.md\n- **Header Section**: Add programmatic activation schema documentation with examples\n- **Per-Persona Sections**: Rewrite each of the 9 personas with new structure including:\n  - `activation` block with auto_triggers, hook_integration\n  - `delegation` block with subagent spawning rules\n  - `tool_access` block with MCP and native tool preferences\n  - `decision_logic` block with autonomous operation rules\n- **Collaboration Protocols**: Enhance existing collaboration section (lines 124-135) with programmatic handoff rules\n- **Integration Examples**: Add code examples showing how hooks trigger personas\n\n### 8. Create Hook Configuration Files\n- **New File**: `.claude/hooks/persona-router.json` defining pre-execution routing logic\n- **Schema**:\n```json\n{\n  \"pre_task_hooks\": {\n    \"persona_activation\": {\n      \"enabled\": true,\n      \"scoring_weights\": {\n        \"file_context\": 0.4,\n        \"keyword_match\": 0.3,\n        \"task_complexity\": 0.3\n      },\n      \"activation_threshold\": 70,\n      \"multi_persona_mode\": \"sequential\"\n    }\n  }\n}\n```\n\n### 9. Define Command System Integration\n- **Update CLAUDE.md**: Add persona auto-activation to command flags (--persona:architect, --auto-persona)\n- **Update MCP.md**: Enhance persona preferences with explicit MCP tool routing rules for each persona\n- **Task Tool Integration**: Define subagent_type mappings for each persona when using Task tool\n\n### 10. Create Validation Framework\n- **Test Scenarios**: Adapt test cases from .taskmaster/docs/ai-guide-persona-tests.md for validating persona auto-activation\n- **Metrics**: Define success criteria:\n  - Activation accuracy: correct persona triggered ≥90% of time\n  - Response time: persona activation adds <500ms overhead\n  - User satisfaction: reduction in manual /persona: commands ≥70%\n- **A/B Testing**: Create test harness comparing manual vs. auto-activation across 35 test scenarios\n\n## Documentation\n\n### 11. Update Related Documentation\n- **RULES.md**: Add persona auto-activation to Core Protocols section with severity [H:8]\n- **CLAUDE.md**: Add Auto-Activation section for personas with examples\n- **Create**: `.claude/docs/persona-subagent-guide.md` with implementation guide for custom personas\n\n### 12. Migration Guide\n- **Backward Compatibility**: Ensure /persona:<name> manual commands still work\n- **Gradual Rollout**: Define phases for enabling auto-activation (opt-in → default → mandatory)\n- **User Training**: Create quick reference showing new auto-activation behaviors",
        "testStrategy": "### Validation Testing\n1. **Activation Accuracy Tests**: Execute 35 test scenarios from ai-guide-persona-tests.md, measuring correct persona activation rate (target: ≥90%)\n2. **Hook Performance Tests**: Measure overhead of persona-router hook on task execution time (target: <500ms added latency)\n3. **Multi-Persona Coordination Tests**: Test sequential activation patterns (analyzer→refactorer→perf) with 10 complex refactoring scenarios\n4. **Tool Orchestration Tests**: Verify each persona uses correct MCP tools (architect→Sequential+C7, frontend→Magic+Puppeteer+C7, etc.) across 20 tasks\n\n### Integration Testing\n5. **Command System Tests**: Verify /user:build, /user:analyze, /user:design commands trigger appropriate personas automatically\n6. **File Context Tests**: Open files matching persona patterns (*.tsx→frontend, *.test.*→qa) and verify auto-activation\n7. **Keyword Detection Tests**: Submit queries with persona-specific keywords and measure routing accuracy\n8. **Fallback Tests**: Test scenarios with ambiguous context to verify graceful degradation to user clarification\n\n### Regression Testing\n9. **Manual Override Tests**: Ensure explicit /persona:<name> commands still work and override auto-activation\n10. **Backward Compatibility**: Test existing workflows without persona system enabled\n11. **Performance Baseline**: Compare task completion time before/after persona system implementation (target: no degradation)\n\n### User Acceptance Testing\n12. **Reduction in Manual Activation**: Track /persona: command usage before/after (target: ≥70% reduction)\n13. **User Satisfaction Survey**: Collect feedback on autonomous persona activation helpfulness (target: ≥4/5 rating)\n14. **Error Rate Monitoring**: Track persona mis-activations requiring user correction (target: <5% of tasks)",
        "status": "pending",
        "dependencies": [
          "17",
          "21"
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": "28",
        "title": "Transform PERSONAS.md into Programmatic Subagent System with Hook-Based Auto-Activation",
        "description": "Redesign PERSONAS.md from passive guidance profiles into actionable subagent definitions that integrate with Claude Code's hook system, settings.json trigger patterns, and the SuperClaude command infrastructure for autonomous activation during development workflows.",
        "details": "## Research Phase\n\n### 1. Analyze Current Hook & Activation Systems\n**Goal:** Understand how .claude/settings.json hooks work and how they can trigger persona activation\n\n**Investigation Tasks:**\n- Examine `.claude/settings.json:2-46` hook structure (UserPromptSubmit, PreToolUse, Stop hooks)\n- Study existing skill trigger patterns in `settings.json:33-46` (systematic-debugging, databases, code-review skills)\n- Map `settings.json` matcher patterns to persona activation conditions\n- Document SuperClaude command system integration points from CLAUDE.md/MCP.md\n- Review Task 17's 35 persona test scenarios in `.taskmaster/docs/ai-guide-persona-tests.md` for activation patterns\n\n**Key Questions:**\n1. Can hooks execute `/persona:<name>` commands or must they invoke Task tool subagents?\n2. What's the latency of hook-based activation vs. manual command execution?\n3. How to pass context from hook matcher to persona invocation?\n\n### 2. ClaudeKit 6-Agent Code Review Mapping\n**Goal:** Map PERSONAS.md archetypes to specialized code review subagent roles\n\n**Mapping Exercise:**\n```yaml\nClaudeKit_Agent_1_Syntax: → refactorer (code quality, complexity reduction)\nClaudeKit_Agent_2_Logic: → analyzer (root cause, evidence-based reasoning)\nClaudeKit_Agent_3_Security: → security (threat models, vulnerability scanning)\nClaudeKit_Agent_4_Performance: → performance (bottleneck identification, optimization)\nClaudeKit_Agent_5_Architecture: → architect (system design, long-term maintainability)\nClaudeKit_Agent_6_Testing: → qa (edge cases, test coverage, defect detection)\n```\n\n**Analysis:**\n- Identify gaps between 9 PERSONAS.md archetypes and 6 ClaudeKit agents\n- Determine if frontend/backend/mentor personas need separate invocation patterns\n- Map MCP tool preferences per persona (from PERSONAS.md lines 185-218)\n\n### 3. Optimal Persona Structure for Programmatic Invocation\n**Goal:** Design persona definition schema that enables autonomous delegation\n\n**Current PERSONAS.md Structure (Passive):**\n```yaml\npersona_name:\n  Core_Belief: [philosophical statement]\n  Primary_Question: [guiding question]\n  Decision_Pattern: [trade-off preferences]\n  Risk_Tolerance: [approach to uncertainty]\n  Success_Metric: [outcome measurement]\n  Communication_Style: [output formatting]\n  Problem_Solving: [methodology]\n  MCP_Tools: [tool preferences]\n```\n\n**Proposed Structure (Actionable):**\n```yaml\npersona_name:\n  # Invocation Configuration\n  triggers:\n    keywords: [bug|error|fix] # Regex patterns for auto-activation\n    file_patterns: [\"*.test.*\", \"*spec*\"] # Glob patterns\n    context_signals: [\"TypeError\", \"Module error\"] # Error signatures\n    command_flags: [\"--qa\", \"--test\"] # Explicit flags\n  \n  # Subagent Behavior\n  task_delegation:\n    agent_type: \"Explore\" | \"Plan\" | \"general-purpose\" # Task tool subagent_type\n    thinking_mode: \"think\" | \"think-hard\" | \"ultrathink\" # Complexity level\n    model_preference: \"haiku\" | \"sonnet\" | \"opus\" # Cost optimization\n  \n  # Execution Parameters\n  behavior_profile:\n    core_belief: [...]\n    decision_pattern: [...]\n    problem_solving_approach: [...]\n    mcp_tool_preferences: [Sequential, Context7] # From MCP.md\n  \n  # Integration Hooks\n  pre_execution_checks: [\"read skill file\", \"validate context\"]\n  post_execution_actions: [\"update audit log\", \"checkpoint work\"]\n  parallel_personas: [\"security\", \"qa\"] # Which personas can run concurrently\n```\n\n### 4. Hook System Integration Patterns\n\n**Pattern A: Direct Persona Invocation via Hooks**\n```json\n// .claude/settings.json enhancement\n{\n  \"hooks\": {\n    \"UserPromptSubmit\": [\n      {\n        \"matcher\": \"optimize|performance|slow|bottleneck\",\n        \"command\": \"/persona:performance --auto-invoked\"\n      },\n      {\n        \"matcher\": \"security|vulnerability|auth|xss|injection\",\n        \"command\": \"/persona:security --context=${MATCHED_TEXT}\"\n      }\n    ]\n  }\n}\n```\n\n**Pattern B: Persona Router Hook (Recommended)**\n```json\n{\n  \"hooks\": {\n    \"UserPromptSubmit\": [\n      {\n        \"matcher\": \".*\", // Match all prompts\n        \"command\": \"claude-code persona-router --analyze-intent\"\n      }\n    ]\n  }\n}\n```\n\n**Persona Router Logic:**\n1. Analyze user prompt intent\n2. Check context signals (files, errors, recent commands)\n3. Select optimal persona(s) from PERSONAS.md\n4. Invoke via Task tool or direct command\n5. Pass control to persona subagent\n\n**Pattern C: Multi-Persona Orchestration**\n```yaml\n# For complex tasks requiring multiple perspectives\ncode_review_workflow:\n  sequential: [architect, refactorer, security, qa]\n  parallel: [frontend, backend]\n  handoff_protocol: \"Share findings → Checkpoint → Cumulative → Document\"\n```\n\n## Implementation Phase\n\n### 5. PERSONAS.md Restructuring\n\n**File Structure:**\n```markdown\n# PERSONAS.md - Programmatic Subagent Definitions\n\n## Core Archetypes (9 personas)\n\n### architect\ninvocation:\n  triggers:\n    keywords: \"design|architecture|system|scale|maintainability\"\n    file_patterns: [\"docs/architecture/**\", \"*.architecture.md\"]\n    context_signals: [\"breaking change\", \"API design\"]\n    command_flags: [\"--design\", \"--architecture\"]\n  \n  delegation:\n    subagent_type: \"Plan\"\n    thinking_mode: \"think-hard\"\n    model: \"sonnet\"\n  \n  behavior:\n    [existing Core_Belief, Decision_Pattern, etc.]\n  \n  mcp_integration:\n    primary_tools: [\"Sequential\", \"Context7\"]\n    research_first: true  # Always lookup patterns before implementation\n  \n  collaboration:\n    works_with: [\"security\", \"performance\", \"qa\"]\n    handoff_to: [\"frontend\", \"backend\"]\n\n[Repeat for all 9 personas...]\n```\n\n### 6. Settings Integration\n\n**Create `.claude/personas/router.sh`:**\n```bash\n#!/bin/bash\n# Persona Router Hook Script\n\nUSER_PROMPT=\"$1\"\nCONTEXT_FILES=\"$2\"\nRECENT_ERRORS=\"$3\"\n\n# Analyze prompt and select persona\nif echo \"$USER_PROMPT\" | grep -qiE 'bug|error|debug|broken'; then\n    echo \"[PERSONA ACTIVATION] analyzer + systematic-debugging skill\"\n    exit 0\nfi\n\nif echo \"$USER_PROMPT\" | grep -qiE 'optimize|performance|slow'; then\n    echo \"[PERSONA ACTIVATION] performance persona\"\n    exit 0\nfi\n\n# Default: no persona override\nexit 0\n```\n\n**Update `.claude/settings.json`:**\n```json\n{\n  \"hooks\": {\n    \"UserPromptSubmit\": [\n      {\n        \"matcher\": \".*\",\n        \"command\": \"bash .claude/personas/router.sh '${USER_PROMPT}' '${CONTEXT_FILES}' '${RECENT_ERRORS}'\"\n      }\n    ]\n  },\n  \"personas\": {\n    \"architect\": {\n      \"triggers\": [\"design\", \"architecture\", \"scale\"],\n      \"path\": \"~/.claude/PERSONAS.md#architect\"\n    },\n    \"analyzer\": {\n      \"triggers\": [\"bug\", \"error\", \"debug\", \"broken\"],\n      \"path\": \"~/.claude/PERSONAS.md#analyzer\"\n    },\n    [... all 9 personas ...]\n  }\n}\n```\n\n### 7. SuperClaude Command Enhancement\n\n**Add to CLAUDE.md:**\n```yaml\nPersona_Invocation:\n  Manual: /persona:<name> [--flags]\n  Auto: Triggered by hooks based on context\n  Multi: /persona:sequence architect,security,qa\n  Parallel: /persona:parallel frontend,backend\n  \nFlags:\n  --auto: Invoked by hook (skip confirmation)\n  --context=<text>: Pass specific context\n  --with=<persona>: Parallel invocation\n  --after=<persona>: Sequential chaining\n```\n\n### 8. Task Tool Integration\n\n**Modify Task tool invocation patterns:**\n```python\n# When Task tool is called, check if persona should be activated\nif task_involves_architecture():\n    activate_persona(\"architect\")\n    # architect persona guides Task tool with:\n    # - Long-term thinking mode\n    # - Sequential MCP for system design\n    # - DDD patterns from Context7\n\nif task_involves_debugging():\n    activate_persona(\"analyzer\")\n    load_skill(\"systematic-debugging\")\n    # analyzer persona ensures root cause analysis\n```\n\n## Validation Phase\n\n### 9. Testing Framework\n\n**Create `.taskmaster/docs/persona-activation-tests.md`:**\n```markdown\n# Persona Activation Test Suite\n\n## Test Scenarios (35 total from Task 17 + 20 new activation tests)\n\n### Auto-Activation Tests\n1. User types \"why is this slow?\" → performance persona activates\n2. User types \"review this code\" → qa + refactorer personas activate\n3. User opens *.test.ts file → qa persona activates\n4. Error: \"SQL injection vulnerability detected\" → security persona activates\n\n### Multi-Persona Tests\n5. /persona:sequence architect,security,qa → Sequential execution\n6. /persona:parallel frontend,backend → Parallel execution with merge\n\n### Hook Performance Tests\n7. Measure persona-router latency (target: <500ms)\n8. Test hook activation accuracy (target: 90%+ correct persona)\n```\n\n**Acceptance Criteria:**\n- ✅ 90%+ correct persona activation from natural language prompts\n- ✅ <500ms overhead from persona-router hook\n- ✅ Seamless integration with existing Task tool workflows\n- ✅ MCP tool preferences honored (Sequential for architect, etc.)\n- ✅ Multi-persona orchestration works (sequential + parallel)\n\n## Documentation Phase\n\n### 10. Migration Guide\n\n**Create `.claude/docs/persona-migration-guide.md`:**\n```markdown\n# Migrating from Manual to Programmatic Personas\n\n## Before (Manual Activation)\nUser: /persona:architect\nUser: Design a new API\n\n## After (Auto-Activation)\nUser: Design a new API\n[Hook detects \"design\" keyword]\n[Persona-router activates architect persona]\n[Architect persona invokes Task tool with Plan subagent]\n[Sequential thinking + Context7 lookup for API patterns]\n[Implementation proceeds with architect guidance]\n\n## Configuration\n1. Update .claude/settings.json with persona triggers\n2. Install persona-router hook script\n3. Test with persona-activation-tests.md scenarios\n```\n\n## Technical Specifications\n\n### File Changes Required:\n1. **PERSONAS.md**: Add `invocation`, `delegation`, `mcp_integration` sections to each persona\n2. **.claude/settings.json**: Add `personas` config block + persona-router hook\n3. **.claude/personas/router.sh**: New persona selection script\n4. **CLAUDE.md**: Add Persona_Invocation command documentation\n5. **MCP.md**: Add Persona_Preferences integration (line 10-15)\n6. **.taskmaster/docs/persona-activation-tests.md**: New test framework\n\n### Integration Points:\n- `settings.json:2-10` hooks → Persona activation\n- `settings.json:33-46` skills → Persona skill loading\n- `MCP.md:10-50` Decision Matrix → Persona MCP preferences\n- `Task tool` subagent_type → Persona delegation patterns\n- `.taskmaster/docs/ai-guide-persona-tests.md` → Validation scenarios",
        "testStrategy": "### Validation Testing\n\n1. **Activation Accuracy Tests**: Execute 35 test scenarios from ai-guide-persona-tests.md, measuring correct persona activation rate (target: ≥90%)\n\n2. **Hook Performance Tests**: Measure overhead of persona-router hook on task execution time (target: <500ms added latency)\n\n3. **Multi-Persona Orchestration Tests**:\n   - Sequential workflow: architect → security → qa (verify handoff protocol)\n   - Parallel workflow: frontend + backend (verify context merging)\n   - Verify collaboration patterns from PERSONAS.md:235-244\n\n4. **MCP Tool Preference Tests**: Validate each persona uses correct MCP tools (Sequential for architect/analyzer, Magic for frontend, Puppeteer for qa)\n\n5. **Command System Integration Tests**:\n   - `/persona:architect --design --think-hard` → Verify flags propagate to Task tool\n   - Test SuperClaude command parsing from CLAUDE.md\n   - Verify --uc (UltraCompressed) mode works with persona responses\n\n6. **Skill Integration Tests**: Verify personas auto-load skills from settings.json:33-46 (security loads code-review skill, analyzer loads systematic-debugging)\n\n7. **Regression Testing**: Run all 35 persona-based queries from `.taskmaster/docs/ai-guide-persona-tests.md` to ensure no functionality breaks\n\n8. **Real-World Scenario Tests**:\n   - User requests \"optimize database queries\" → Performance + databases skill activate\n   - User requests \"review PR #123\" → qa + refactorer + security activate in sequence\n   - User opens .test.tsx file → qa + frontend personas activate\n\n9. **Error Handling Tests**: Verify graceful degradation when persona-router fails, falling back to manual activation\n\n10. **Documentation Verification**: Ensure all persona triggers are documented in updated PERSONAS.md and activation examples work as described",
        "status": "cancelled",
        "dependencies": [
          "17",
          "21"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2025-12-03T03:53:38.665Z"
      },
      {
        "id": "29",
        "title": "Feature Bundle: Task Delete, Decisions Modal, Proposal Creation Fix",
        "description": "Implement task deletion functionality with confirmation dialog, create enhanced decision details modal with view/edit/delete capabilities, and fix 422 validation error occurring during proposal creation.",
        "details": "## 1. Task Delete Functionality\n\n### Frontend Implementation (frontend/app/tasks/page.tsx)\n- Add delete button to task detail modal (line 933-945 area, alongside Edit button)\n- Create confirmation dialog using AlertDialog component from Radix UI:\n  - Title: \"Delete Task?\"\n  - Content: \"Are you sure you want to delete '{task.title}'? This action cannot be undone.\"\n  - Two buttons: Cancel (outline) and Delete (destructive variant)\n- Implement handleDeleteTask function:\n  - Call api.tasks.delete(taskId)\n  - Close detail modal on success\n  - Reload task list via loadTasks()\n  - Show error alert on failure\n- Add loading state during deletion (isDeleting boolean)\n\n### Backend Implementation (backend/src/main.py)\n- DELETE endpoint already exists at line 334-342: `/api/tasks/{task_id}`\n- Performs hard delete from Cosmos DB\n- Returns {\"success\": True}\n\n### API Client (frontend/lib/api.ts)\n- Add delete method to api.tasks object (line 292-305):\n```typescript\ndelete: (id: string) =>\n  apiFetch<void>(`/api/tasks/${id}`, {\n    method: 'DELETE',\n  }),\n```\n\n---\n\n## 2. Enhanced Decisions Modal\n\n### Current State Analysis\n- Decision details modal exists (line 883-1062 in decisions/page.tsx)\n- Has view/edit modes and delete functionality already implemented\n- Modal is opened via openDecisionDetails() function (line 111-116)\n- Edit mode toggles between view and form (line 917-1005)\n- Delete implemented with confirmation (line 130-142)\n\n### Improvements Needed\n- **Make modal larger**: Change max-w-2xl to max-w-4xl (already done at line 884)\n- **Wire \"View Details\" button**: Currently at line 630-632, button exists but doesn't call openDecisionDetails()\n  - Update onClick handler: `onClick={() => openDecisionDetails(decision)}`\n- **Enhance content display**:\n  - Add stakeholders section if available in Decision model\n  - Add timeline/key dates section\n  - Format rationale and impact with better typography (whitespace-pre-wrap already present)\n  - Add metadata section showing created/updated timestamps (already present at line 1053-1056)\n\n### Frontend Changes\n1. Connect \"View Details\" button in decisions list card (line 630) to openDecisionDetails()\n2. Add stakeholder field to Decision interface if needed (frontend/lib/api.ts line 90-103)\n3. Enhance modal content sections with richer formatting\n\n---\n\n## 3. Fix Proposal Creation 422 Error\n\n### Investigation Steps\n1. **Check backend Proposal model** (backend/src/models.py):\n   - Verify all required fields match frontend payload\n   - Check field validation rules (string lengths, enum values, etc.)\n   \n2. **Check frontend payload** (frontend/app/decisions/page.tsx line 144-156):\n   - Current fields sent: title, description, proposer, department, team_member, category, status, rationale, impact\n   - Verify all required fields are marked correctly in create dialog\n   \n3. **Common 422 causes**:\n   - Missing required field (proposer, department, or department validation)\n   - Invalid category enum value\n   - Invalid status enum value (hardcoded to 'Proposed')\n   - Empty string vs undefined/null handling (team_member uses `|| undefined`)\n\n### Backend Endpoint Analysis (backend/src/main.py line 446-464)\n- POST /api/proposals accepts Proposal model\n- Auto-generates ID if not provided\n- Sets created_at and updated_at timestamps\n- Creates in Cosmos DB \"proposals\" container\n\n### Likely Fix\n- **Issue**: Frontend validation (line 764) requires all fields: `!newProposal.title || !newProposal.description || !newProposal.proposer || !newProposal.department`\n- **But**: department is a text Input field that could be empty string instead of falsy\n- **Solution**: Add `.trim()` checks or verify backend model allows empty strings\n- Add error response logging to see exact validation message from API\n- Check that all enum values match exactly between frontend and backend\n\n### Implementation\n1. Add try-catch error logging in handleCreateProposal (line 144):\n   - Log full error response body\n   - Display specific validation errors to user\n2. Verify Proposal model required fields in backend\n3. Add defensive validation in frontend before API call\n4. Consider adding field-level validation feedback",
        "testStrategy": "## Testing Plan\n\n### 1. Task Delete Functionality\n- **Unit Tests**:\n  - Test api.tasks.delete() API client method\n  - Test confirmation dialog opens with correct task title\n  - Test dialog cancel closes without deletion\n  - Test successful deletion closes modal and reloads list\n- **Integration Tests**:\n  - Create test task, open detail modal, click delete\n  - Verify confirmation dialog appears\n  - Cancel and verify task still exists\n  - Delete and verify task removed from list and database\n  - Verify audit log records deletion event\n- **Edge Cases**:\n  - Test delete with network error\n  - Test delete of task with dependencies\n  - Test concurrent deletion (two users deleting same task)\n\n### 2. Enhanced Decisions Modal\n- **Functional Tests**:\n  - Click \"View Details\" button on decision card\n  - Verify modal opens with full content displayed\n  - Test edit mode toggle and save changes\n  - Test delete functionality with confirmation\n  - Verify modal dimensions (max-w-4xl) on various screen sizes\n- **Content Tests**:\n  - Verify all decision fields display correctly (title, description, rationale, impact)\n  - Check date formatting for decision_date, created_at, updated_at\n  - Test with decisions that have missing optional fields (rationale, impact)\n  - Verify related meeting/proposal links if present\n- **UI/UX Tests**:\n  - Test modal scroll behavior with long content\n  - Verify edit form validation\n  - Test modal close on outside click and ESC key\n\n### 3. Proposal Creation Fix\n- **Diagnosis Tests**:\n  - Attempt proposal creation, capture exact 422 error message\n  - Log request payload before API call\n  - Use browser DevTools Network tab to inspect request/response\n  - Test with minimal required fields only\n- **Fix Validation**:\n  - Create proposal with all fields populated\n  - Create proposal with only required fields\n  - Test each category enum value\n  - Test with empty vs undefined optional fields\n  - Verify team_member field with empty string\n- **Regression Tests**:\n  - After fix, test proposal creation workflow end-to-end\n  - Test proposal → decision conversion flow\n  - Verify all proposal status transitions work\n  - Test proposal listing and filtering\n  \n### 4. Cross-Feature Integration\n- Test task delete from task created via meeting transcript\n- Test decision delete that was created from proposal\n- Verify audit logs capture all CRUD operations\n- Test all functionality in both light/dark modes\n- Verify responsive behavior on mobile/tablet/desktop",
        "status": "done",
        "dependencies": [
          "4",
          "12"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-05T01:04:08.607Z"
      },
      {
        "id": "30",
        "title": "Budget & Cost Tracking System with Azure Cost Management Integration",
        "description": "Implement comprehensive AI tooling cost tracking system integrating Azure Cost Management API for real-time cost data from 7 Resource Groups, with backend service, budget CRUD endpoints, license management, and frontend dashboard UI.",
        "details": "## Architecture Overview\nBuild full-stack cost tracking system connecting Azure Cost Management API to provide real-time budget visibility for AI infrastructure spend across multiple subscriptions and resource groups.\n\n## Backend Implementation\n\n### 1. Data Models (backend/src/models.py)\nAdd new Pydantic models following existing patterns (lines 1-727):\n\n```python\nclass BudgetCategory(str, Enum):\n    AZURE_SERVICE = \"Azure Service\"\n    SOFTWARE_LICENSE = \"Software License\"\n    CUSTOM_ALLOCATION = \"Custom Allocation\"\n\nclass BudgetStatus(str, Enum):\n    ON_TRACK = \"On Track\"\n    WARNING = \"Warning\"\n    CRITICAL = \"Critical\"\n    EXCEEDED = \"Exceeded\"\n\nclass Budget(BaseModel):\n    id: Optional[str] = None\n    name: str\n    category: BudgetCategory\n    resource_group: Optional[str] = None  # For Azure services\n    azure_service_type: Optional[str] = None  # OpenAI, CosmosDB, AI Search, Blob Storage\n    amount: float\n    spent: float = 0.0\n    currency: str = \"USD\"\n    period: str = \"monthly\"  # monthly, quarterly, annual\n    status: BudgetStatus = BudgetStatus.ON_TRACK\n    threshold_warning: float = 75.0  # Percent\n    threshold_critical: float = 90.0\n    start_date: datetime\n    end_date: Optional[datetime] = None\n    notes: Optional[str] = None\n    created_at: datetime = Field(default_factory=datetime.utcnow)\n    updated_at: datetime = Field(default_factory=datetime.utcnow)\n\nclass License(BaseModel):\n    id: Optional[str] = None\n    name: str  # M365 Copilot, GitHub Copilot\n    license_type: str  # Subscription, Pay-as-you-go\n    seats: Optional[int] = None\n    cost_per_seat: Optional[float] = None\n    total_monthly_cost: float\n    renewal_date: Optional[datetime] = None\n    vendor: str\n    status: str = \"active\"  # active, expiring, expired\n    notes: Optional[str] = None\n    created_at: datetime = Field(default_factory=datetime.utcnow)\n    updated_at: datetime = Field(default_factory=datetime.utcnow)\n\nclass CostDataPoint(BaseModel):\n    date: str\n    cost: float\n    resource_group: str\n    service_type: Optional[str] = None\n```\n\n### 2. Azure Cost Management Service (backend/src/cost_management_service.py)\nCreate new service following azure_resources_service.py pattern (lines 1-85):\n\n```python\nfrom azure.identity import DefaultAzureCredential\nfrom azure.mgmt.costmanagement import CostManagementClient\nfrom azure.mgmt.costmanagement.models import QueryDefinition, QueryTimePeriod, QueryDataset, QueryAggregation, QueryGrouping\nfrom datetime import datetime, timedelta\nimport logging\nimport os\n\nlogger = logging.getLogger(__name__)\n\nclass CostManagementService:\n    def __init__(self):\n        self.credential = DefaultAzureCredential()\n        self.subscription_id = os.getenv(\"AZURE_SUBSCRIPTION_ID\")\n        self.client = CostManagementClient(self.credential)\n        self.resource_groups = [\n            \"rg-agent-architecture\",\n            \"mb-ai-foundry\",\n            \"mb-ai-foundry-hub\",\n            \"fourth-ai-prod\",\n            \"ai-dev-education\",\n            \"personal-agent-rg\",\n            \"boyan-ai-experiments\"\n        ]\n        self._cache = {}\n        self._cache_timestamp = None\n        self._cache_duration = timedelta(hours=6)  # 6-hour cache\n\n    async def get_resource_group_costs(self, resource_group: str, start_date: datetime, end_date: datetime):\n        \"\"\"Query costs for a specific resource group within date range.\"\"\"\n        scope = f\"/subscriptions/{self.subscription_id}/resourceGroups/{resource_group}\"\n        \n        query = QueryDefinition(\n            type=\"ActualCost\",\n            timeframe=\"Custom\",\n            time_period=QueryTimePeriod(from_property=start_date.isoformat(), to=end_date.isoformat()),\n            dataset=QueryDataset(\n                granularity=\"Daily\",\n                aggregation={\n                    \"totalCost\": QueryAggregation(name=\"Cost\", function=\"Sum\")\n                },\n                grouping=[\n                    QueryGrouping(type=\"Dimension\", name=\"ServiceName\"),\n                    QueryGrouping(type=\"Dimension\", name=\"ResourceId\")\n                ]\n            )\n        )\n        \n        try:\n            result = self.client.query.usage(scope=scope, parameters=query)\n            return self._parse_cost_response(result, resource_group)\n        except Exception as e:\n            logger.error(f\"Error querying costs for {resource_group}: {e}\")\n            return []\n\n    def _parse_cost_response(self, response, resource_group: str):\n        \"\"\"Parse Azure Cost Management API response into structured data.\"\"\"\n        costs = []\n        if hasattr(response, 'rows') and response.rows:\n            for row in response.rows:\n                costs.append({\n                    \"cost\": row[0],\n                    \"service_name\": row[1] if len(row) > 1 else \"Unknown\",\n                    \"resource_id\": row[2] if len(row) > 2 else \"\",\n                    \"date\": row[3] if len(row) > 3 else datetime.now().isoformat(),\n                    \"resource_group\": resource_group\n                })\n        return costs\n\n    async def get_all_costs(self, force_refresh: bool = False):\n        \"\"\"Get costs for all configured resource groups with caching.\"\"\"\n        if not force_refresh and self._cache_timestamp and (datetime.now() - self._cache_timestamp) < self._cache_duration:\n            return self._cache.get(\"all_costs\", [])\n\n        end_date = datetime.now()\n        start_date = end_date.replace(day=1)  # Start of current month\n        \n        all_costs = []\n        for rg in self.resource_groups:\n            rg_costs = await self.get_resource_group_costs(rg, start_date, end_date)\n            all_costs.extend(rg_costs)\n        \n        self._cache[\"all_costs\"] = all_costs\n        self._cache_timestamp = datetime.now()\n        return all_costs\n\n    async def get_service_type_totals(self):\n        \"\"\"Aggregate costs by Azure service type (OpenAI, CosmosDB, etc.).\"\"\"\n        all_costs = await self.get_all_costs()\n        service_totals = {}\n        \n        for cost_entry in all_costs:\n            service = cost_entry[\"service_name\"]\n            if service not in service_totals:\n                service_totals[service] = 0.0\n            service_totals[service] += cost_entry[\"cost\"]\n        \n        return service_totals\n\ncost_management_service = CostManagementService()\n```\n\n### 3. Budget Router (backend/src/routers/budget.py)\nCreate new router following existing patterns (see meetings.py, tasks.py):\n\n```python\nfrom fastapi import APIRouter, HTTPException, Depends\nfrom typing import List, Optional\nfrom src.models import Budget, License, CostDataPoint\nfrom src.database import db\nfrom src.cost_management_service import cost_management_service\nfrom datetime import datetime\nimport logging\n\nlogger = logging.getLogger(__name__)\nrouter = APIRouter(prefix=\"/api/budget\", tags=[\"budget\"])\n\n# Budget CRUD endpoints\n@router.get(\"/budgets\", response_model=List[Budget])\nasync def list_budgets():\n    \"\"\"List all budgets from Cosmos DB.\"\"\"\n    try:\n        query = \"SELECT * FROM c WHERE c.type = 'budget' ORDER BY c.created_at DESC\"\n        budgets = await db.query_items(\"budgets\", query)\n        return [Budget(**b) for b in budgets]\n    except Exception as e:\n        logger.error(f\"Error listing budgets: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@router.post(\"/budgets\", response_model=Budget)\nasync def create_budget(budget: Budget):\n    \"\"\"Create new budget.\"\"\"\n    try:\n        budget_dict = budget.dict()\n        budget_dict[\"type\"] = \"budget\"\n        result = await db.create_item(\"budgets\", budget_dict)\n        return Budget(**result)\n    except Exception as e:\n        logger.error(f\"Error creating budget: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@router.put(\"/budgets/{budget_id}\", response_model=Budget)\nasync def update_budget(budget_id: str, budget: Budget):\n    \"\"\"Update existing budget.\"\"\"\n    try:\n        existing = await db.get_item(\"budgets\", budget_id)\n        if not existing:\n            raise HTTPException(status_code=404, detail=\"Budget not found\")\n        \n        budget_dict = budget.dict()\n        budget_dict[\"id\"] = budget_id\n        budget_dict[\"updated_at\"] = datetime.utcnow().isoformat()\n        result = await db.update_item(\"budgets\", budget_id, budget_dict)\n        return Budget(**result)\n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Error updating budget: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@router.delete(\"/budgets/{budget_id}\")\nasync def delete_budget(budget_id: str):\n    \"\"\"Delete budget.\"\"\"\n    try:\n        await db.delete_item(\"budgets\", budget_id)\n        return {\"message\": \"Budget deleted successfully\"}\n    except Exception as e:\n        logger.error(f\"Error deleting budget: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n# License CRUD endpoints (similar pattern)\n@router.get(\"/licenses\", response_model=List[License])\nasync def list_licenses():\n    \"\"\"List all software licenses.\"\"\"\n    try:\n        query = \"SELECT * FROM c WHERE c.type = 'license' ORDER BY c.name\"\n        licenses = await db.query_items(\"budgets\", query)\n        return [License(**lic) for lic in licenses]\n    except Exception as e:\n        logger.error(f\"Error listing licenses: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@router.post(\"/licenses\", response_model=License)\nasync def create_license(license: License):\n    \"\"\"Create new license entry.\"\"\"\n    # Similar to create_budget\n\n@router.put(\"/licenses/{license_id}\", response_model=License)\nasync def update_license(license_id: str, license: License):\n    \"\"\"Update license.\"\"\"\n    # Similar to update_budget\n\n@router.delete(\"/licenses/{license_id}\")\nasync def delete_license(license_id: str):\n    \"\"\"Delete license.\"\"\"\n    # Similar to delete_budget\n\n# Azure Cost Management integration endpoints\n@router.get(\"/costs/current\")\nasync def get_current_costs():\n    \"\"\"Get current month costs from Azure Cost Management API.\"\"\"\n    try:\n        costs = await cost_management_service.get_all_costs()\n        return {\"costs\": costs, \"total\": sum(c[\"cost\"] for c in costs)}\n    except Exception as e:\n        logger.error(f\"Error fetching current costs: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@router.get(\"/costs/by-service\")\nasync def get_costs_by_service():\n    \"\"\"Get cost breakdown by Azure service type.\"\"\"\n    try:\n        service_totals = await cost_management_service.get_service_type_totals()\n        return service_totals\n    except Exception as e:\n        logger.error(f\"Error fetching service costs: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@router.get(\"/costs/resource-group/{rg_name}\")\nasync def get_resource_group_costs(rg_name: str):\n    \"\"\"Get costs for specific resource group.\"\"\"\n    try:\n        end_date = datetime.now()\n        start_date = end_date.replace(day=1)\n        costs = await cost_management_service.get_resource_group_costs(rg_name, start_date, end_date)\n        return {\"resource_group\": rg_name, \"costs\": costs, \"total\": sum(c[\"cost\"] for c in costs)}\n    except Exception as e:\n        logger.error(f\"Error fetching RG costs: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@router.post(\"/sync-budgets\")\nasync def sync_budgets_with_actual_costs():\n    \"\"\"Sync budget spent amounts with actual Azure costs.\"\"\"\n    try:\n        budgets = await list_budgets()\n        service_totals = await cost_management_service.get_service_type_totals()\n        \n        updated_count = 0\n        for budget in budgets:\n            if budget.azure_service_type and budget.azure_service_type in service_totals:\n                actual_cost = service_totals[budget.azure_service_type]\n                budget.spent = actual_cost\n                \n                # Update status based on thresholds\n                percent_used = (actual_cost / budget.amount) * 100 if budget.amount > 0 else 0\n                if percent_used >= 100:\n                    budget.status = \"Exceeded\"\n                elif percent_used >= budget.threshold_critical:\n                    budget.status = \"Critical\"\n                elif percent_used >= budget.threshold_warning:\n                    budget.status = \"Warning\"\n                else:\n                    budget.status = \"On Track\"\n                \n                await update_budget(budget.id, budget)\n                updated_count += 1\n        \n        return {\"message\": f\"Synced {updated_count} budgets with actual costs\"}\n    except Exception as e:\n        logger.error(f\"Error syncing budgets: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n```\n\n### 4. Register Router (backend/src/main.py)\nAdd budget router import and registration following existing pattern (see azure_resources router registration):\n\n```python\nfrom src.routers import budget\napp.include_router(budget.router)\n```\n\n### 5. Database Setup\nCreate Cosmos DB container \"budgets\" with partition key \"/type\" to store Budget and License documents, following existing container patterns.\n\n## Frontend Implementation\n\n### 1. API Client (frontend/lib/api.ts)\nAdd budget API methods following existing patterns (lines 1-100):\n\n```typescript\nexport interface Budget {\n  id: string;\n  name: string;\n  category: 'Azure Service' | 'Software License' | 'Custom Allocation';\n  resource_group?: string;\n  azure_service_type?: string;\n  amount: number;\n  spent: number;\n  currency: string;\n  period: string;\n  status: 'On Track' | 'Warning' | 'Critical' | 'Exceeded';\n  threshold_warning: number;\n  threshold_critical: number;\n  start_date: string;\n  end_date?: string;\n  notes?: string;\n  created_at: string;\n  updated_at: string;\n}\n\nexport interface License {\n  id: string;\n  name: string;\n  license_type: string;\n  seats?: number;\n  cost_per_seat?: number;\n  total_monthly_cost: number;\n  renewal_date?: string;\n  vendor: string;\n  status: string;\n  notes?: string;\n  created_at: string;\n  updated_at: string;\n}\n\nexport const api = {\n  // ... existing methods\n  budget: {\n    list: async (): Promise<Budget[]> => {\n      const res = await fetch(`${API_URL}/api/budget/budgets`, {\n        headers: { 'X-API-Key': API_KEY }\n      });\n      if (!res.ok) throw new Error('Failed to fetch budgets');\n      return res.json();\n    },\n    create: async (budget: Partial<Budget>): Promise<Budget> => {\n      const res = await fetch(`${API_URL}/api/budget/budgets`, {\n        method: 'POST',\n        headers: { 'X-API-Key': API_KEY, 'Content-Type': 'application/json' },\n        body: JSON.stringify(budget)\n      });\n      if (!res.ok) throw new Error('Failed to create budget');\n      return res.json();\n    },\n    update: async (id: string, budget: Partial<Budget>): Promise<Budget> => {\n      const res = await fetch(`${API_URL}/api/budget/budgets/${id}`, {\n        method: 'PUT',\n        headers: { 'X-API-Key': API_KEY, 'Content-Type': 'application/json' },\n        body: JSON.stringify(budget)\n      });\n      if (!res.ok) throw new Error('Failed to update budget');\n      return res.json();\n    },\n    delete: async (id: string): Promise<void> => {\n      const res = await fetch(`${API_URL}/api/budget/budgets/${id}`, {\n        method: 'DELETE',\n        headers: { 'X-API-Key': API_KEY }\n      });\n      if (!res.ok) throw new Error('Failed to delete budget');\n    },\n    getCurrentCosts: async () => {\n      const res = await fetch(`${API_URL}/api/budget/costs/current`, {\n        headers: { 'X-API-Key': API_KEY }\n      });\n      if (!res.ok) throw new Error('Failed to fetch current costs');\n      return res.json();\n    },\n    syncBudgets: async () => {\n      const res = await fetch(`${API_URL}/api/budget/sync-budgets`, {\n        method: 'POST',\n        headers: { 'X-API-Key': API_KEY }\n      });\n      if (!res.ok) throw new Error('Failed to sync budgets');\n      return res.json();\n    }\n  },\n  licenses: {\n    list: async (): Promise<License[]> => {\n      const res = await fetch(`${API_URL}/api/budget/licenses`, {\n        headers: { 'X-API-Key': API_KEY }\n      });\n      if (!res.ok) throw new Error('Failed to fetch licenses');\n      return res.json();\n    },\n    create: async (license: Partial<License>): Promise<License> => {\n      const res = await fetch(`${API_URL}/api/budget/licenses`, {\n        method: 'POST',\n        headers: { 'X-API-Key': API_KEY, 'Content-Type': 'application/json' },\n        body: JSON.stringify(license)\n      });\n      if (!res.ok) throw new Error('Failed to create license');\n      return res.json();\n    },\n    update: async (id: string, license: Partial<License>): Promise<License> => {\n      const res = await fetch(`${API_URL}/api/budget/licenses/${id}`, {\n        method: 'PUT',\n        headers: { 'X-API-Key': API_KEY, 'Content-Type': 'application/json' },\n        body: JSON.stringify(license)\n      });\n      if (!res.ok) throw new Error('Failed to update license');\n      return res.json();\n    },\n    delete: async (id: string): Promise<void> => {\n      const res = await fetch(`${API_URL}/api/budget/licenses/${id}`, {\n        method: 'DELETE',\n        headers: { 'X-API-Key': API_KEY }\n      });\n      if (!res.ok) throw new Error('Failed to delete license');\n    }\n  }\n};\n```\n\n### 2. Replace Mock UI (frontend/app/(authenticated)/budget/page.tsx)\nReplace existing mock implementation (lines 1-355) with real data integration:\n\n- Remove mockBudgetData (lines 32-75) and licenses (lines 77-102) constants\n- Replace with useState hooks loading data from api.budget.list() and api.licenses.list()\n- Add useEffect to fetch data on component mount\n- Add loading states and error handling\n- Implement CRUD dialogs using existing patterns from tasks/meetings pages\n- Add \"Sync with Azure\" button calling api.budget.syncBudgets()\n- Keep existing UI structure (summary cards lines 153-239, service-level tracking 241-287, license management 289-350)\n- Add create/edit/delete dialogs for budgets and licenses using Radix UI Dialog component\n- Implement resource group drill-down pages at /budget/[resource-group] route\n\n### 3. Resource Group Detail Pages\nCreate frontend/app/(authenticated)/budget/[resource-group]/page.tsx for per-RG cost breakdown:\n\n- Display costs filtered by resource group\n- Show service-level breakdown within that RG\n- Historical cost chart using recharts library\n- Link back to main budget dashboard\n\n## Configuration Requirements\n\n### Azure Setup\n1. Ensure Azure service principal has \"Cost Management Reader\" role on subscription\n2. Set environment variable AZURE_SUBSCRIPTION_ID in backend/.env\n3. DefaultAzureCredential will use existing AZURE_CLIENT_ID, AZURE_TENANT_ID, AZURE_CLIENT_SECRET\n\n### Cosmos DB\nCreate \"budgets\" container with partition key \"/type\" in existing Cosmos DB account\n\n## Security & Permissions\n- Cost Management API requires \"Cost Management Reader\" RBAC role on subscription or resource groups\n- Verify service principal has required permissions before testing\n- Budget/license CRUD operations require X-API-Key authentication (existing pattern)\n\n## Dependencies\n- Backend: azure-mgmt-costmanagement==4.0.1 (already in requirements.txt line 16)\n- Frontend: No new dependencies needed (uses existing UI components)",
        "testStrategy": "## Testing Strategy\n\n### Backend Testing\n\n**1. Cost Management Service Tests**\n- Test connection to Azure Cost Management API with valid credentials\n- Test get_resource_group_costs() with each of 7 resource groups\n- Verify cost data parsing from Azure API response format\n- Test caching mechanism (6-hour cache duration)\n- Test force_refresh parameter bypasses cache\n- Mock Azure API responses for unit tests to avoid actual API calls\n- Test error handling for invalid resource groups, API failures, credential issues\n\n**2. Budget Router Endpoint Tests**\n- Test GET /api/budget/budgets returns all budgets from Cosmos DB\n- Test POST /api/budget/budgets creates budget with correct fields\n- Test PUT /api/budget/budgets/{id} updates existing budget\n- Test DELETE /api/budget/budgets/{id} removes budget\n- Test GET /api/budget/licenses returns all licenses\n- Test CRUD operations for licenses (create, update, delete)\n- Test GET /api/budget/costs/current returns aggregated costs from all RGs\n- Test GET /api/budget/costs/by-service groups costs by Azure service type\n- Test GET /api/budget/costs/resource-group/{name} filters by specific RG\n- Test POST /api/budget/sync-budgets updates budget.spent from actual costs and recalculates status (On Track, Warning, Critical, Exceeded) based on thresholds\n- Verify authentication with X-API-Key header required for all endpoints\n\n**3. Integration Tests**\n- Test end-to-end flow: create budget → sync with Azure → verify spent amount updated\n- Test budget status calculation logic (thresholds at 75%, 90%, 100%)\n- Test querying costs for non-existent resource group returns empty/error\n- Test Cosmos DB storage and retrieval of Budget and License models\n- Verify Budget.status updates correctly when spent crosses threshold_warning (75%) and threshold_critical (90%)\n\n### Frontend Testing\n\n**4. API Client Tests**\n- Test api.budget.list() fetches and parses budget data correctly\n- Test api.budget.create() sends correct request body\n- Test api.licenses.list() returns license data\n- Test api.budget.syncBudgets() calls sync endpoint and handles response\n- Test error handling for network failures, 404, 500 responses\n\n**5. UI Component Tests**\n- Test budget dashboard loads and displays summary cards with correct totals\n- Test service-level budget list renders with progress bars showing correct percentages\n- Test status badges display correct colors (green On Track, orange Warning, red Critical/Exceeded)\n- Test time period selector (week, month, quarter, year) affects displayed data\n- Test \"Sync with Azure\" button triggers sync and refreshes data\n- Test create budget dialog opens, validates input, submits data\n- Test edit budget dialog pre-fills existing data\n- Test delete budget confirmation dialog prevents accidental deletion\n- Test license management section displays licenses with seats, costs, renewal dates\n- Test resource group drill-down navigation to /budget/[resource-group]\n\n**6. E2E Testing Scenarios**\n- Load budget page → verify mock data replaced with real API data\n- Create new budget for \"Azure OpenAI\" in \"rg-agent-architecture\" with $5000 amount\n- Sync budgets → verify spent amount updates from Azure Cost Management API\n- Verify budget status changes from \"On Track\" to \"Warning\" when spent > 75%\n- Create license entry for \"GitHub Copilot\" with 50 seats at $19/seat\n- Edit license to update seat count and verify total cost recalculates\n- Delete budget → confirm dialog → verify removed from list\n- Navigate to /budget/rg-agent-architecture → verify filtered cost view\n\n**7. Azure RBAC Verification**\n- Test with service principal that has Cost Management Reader role → succeeds\n- Test with service principal without Cost Management Reader role → returns 403 error with helpful message\n- Verify AZURE_SUBSCRIPTION_ID environment variable is set correctly\n- Test DefaultAzureCredential authentication flow in Azure environment\n\n**8. Performance & Reliability**\n- Test cost API response time with 7 resource groups (should complete within 10 seconds)\n- Test cache effectiveness (second request within 6 hours should be instant)\n- Test handling of Azure API rate limits with retry logic\n- Test behavior when one resource group query fails (should not block others)\n- Load test with 50+ budgets and 20+ licenses to verify UI performance",
        "status": "pending",
        "dependencies": [
          "4",
          "12"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Add Budget and License Data Models to backend/src/models.py",
            "description": "Create Pydantic models for Budget, License, BudgetCategory, BudgetStatus, and CostDataPoint enums following existing model patterns",
            "dependencies": [],
            "details": "Add new enums: BudgetCategory (Azure Service, Software License, Custom Allocation), BudgetStatus (On Track, Warning, Critical, Exceeded). Add Budget model with fields: id, name, category, resource_group, azure_service_type, amount, spent, currency, period, status, threshold_warning (75.0), threshold_critical (90.0), start_date, end_date, notes, created_at, updated_at. Add License model with fields: id, name, license_type, seats, cost_per_seat, total_monthly_cost, renewal_date, vendor, status (active/expiring/expired), notes, created_at, updated_at. Add CostDataPoint model with fields: date, cost, resource_group, service_type. Follow existing patterns from Meeting/Task/Agent models (lines 93-727 in models.py).",
            "status": "pending",
            "testStrategy": "Validate model instantiation with all required fields. Test enum values. Verify default values (status=On Track, threshold_warning=75.0, threshold_critical=90.0). Test optional fields can be None.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create Azure Cost Management Service (backend/src/cost_management_service.py)",
            "description": "Build service integrating Azure Cost Management API to fetch real-time cost data from 7 resource groups with 6-hour caching",
            "dependencies": [
              1
            ],
            "details": "Create CostManagementService class using azure-mgmt-costmanagement SDK (already in requirements.txt line 16). Initialize with DefaultAzureCredential and subscription ID from env var AZURE_SUBSCRIPTION_ID. Define resource_groups list: ['rg-agent-architecture', 'mb-ai-foundry', 'mb-ai-foundry-hub', 'fourth-ai-prod', 'ai-dev-education', 'personal-agent-rg', 'boyan-ai-experiments']. Implement get_resource_group_costs(rg, start_date, end_date) using QueryDefinition with ActualCost type, Daily granularity, Cost aggregation, grouping by ServiceName and ResourceId. Implement get_all_costs() with 6-hour cache (_cache_duration=timedelta(hours=6)). Implement get_service_type_totals() aggregating by service name. Follow azure_resources_service.py pattern (lines 1-85). Export singleton instance cost_management_service.",
            "status": "pending",
            "testStrategy": "Test Azure credential authentication. Test get_resource_group_costs() returns data for each of 7 RGs. Verify cache mechanism works (first call hits API, second uses cache within 6 hours). Test get_service_type_totals() aggregates correctly. Validate error handling for API failures.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Create Budget Router with CRUD and Azure Cost Sync Endpoints (backend/src/routers/budget.py)",
            "description": "Implement FastAPI router with budget/license CRUD endpoints and Azure Cost Management integration endpoints",
            "dependencies": [
              2
            ],
            "details": "Create router with prefix '/api/budget'. Implement endpoints: GET /budgets (list all from Cosmos DB 'budgets' container where type='budget'), POST /budgets (create with type='budget'), PUT /budgets/{id} (update with updated_at timestamp), DELETE /budgets/{id}. Implement same pattern for licenses (type='license'). Add GET /costs/current (calls cost_management_service.get_all_costs()), GET /costs/by-service (calls get_service_type_totals()), GET /costs/resource-group/{rg_name} (calls get_resource_group_costs). Implement POST /sync-budgets: fetches all budgets, gets service_totals from Azure, updates budget.spent with actual costs, calculates status based on thresholds (>=100%=Exceeded, >=threshold_critical=Critical, >=threshold_warning=Warning, else On Track), saves updated budgets. Follow patterns from azure_resources.py router. Use db.query_items, db.create_item, db.update_item, db.delete_item from database.py.",
            "status": "pending",
            "testStrategy": "Test all CRUD endpoints with valid/invalid data. Test /costs/current returns data structure with costs array and total. Test /costs/by-service aggregates correctly. Test /costs/resource-group/{rg} for each RG. Test /sync-budgets updates budget.spent and status correctly based on thresholds. Verify error handling (404 for missing budget, 500 for DB/API errors).",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Register Budget Router and Create Cosmos DB Container",
            "description": "Add budget router to main.py and create 'budgets' Cosmos DB container with partition key '/type'",
            "dependencies": [
              3
            ],
            "details": "In backend/src/main.py line 10, add import: from src.routers import budget. After existing router registrations (after line with azure_resources), add: app.include_router(budget.router). In backend/src/database.py line 31-45, add to container_definitions list: ('budgets', '/type'). The container will store both Budget (type='budget') and License (type='license') documents using partition key '/type' for efficient querying. Follow existing patterns from meetings/tasks/agents containers.",
            "status": "pending",
            "testStrategy": "Verify backend starts without errors. Test container creation by checking Azure Cosmos DB portal shows 'budgets' container with partition key '/type'. Test creating budget document with type='budget' succeeds. Test query 'SELECT * FROM c WHERE c.type = \"budget\"' returns only budgets.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Add Budget API Client Methods to frontend/lib/api.ts",
            "description": "Extend api.ts with TypeScript interfaces and methods for budget, license, and cost endpoints",
            "dependencies": [
              4
            ],
            "details": "Add Budget interface matching backend model (id, name, category, resource_group, azure_service_type, amount, spent, currency, period, status, threshold_warning, threshold_critical, start_date, end_date, notes, created_at, updated_at). Add License interface matching backend model. Add api.budget object with methods: list() -> Promise<Budget[]>, create(budget: Partial<Budget>) -> Promise<Budget>, update(id, budget) -> Promise<Budget>, delete(id) -> Promise<void>, getCurrentCosts() -> Promise<{costs: any[], total: number}>, syncBudgets() -> Promise<{message: string}>. Add api.licenses with similar CRUD methods. All methods use fetch with ${API_URL}/api/budget/* endpoints, X-API-Key header from API_KEY constant (line 6), handle errors with throw new Error(). Follow existing patterns from api.meetings/api.tasks (lines 8-100).",
            "status": "pending",
            "testStrategy": "Test each API method with TypeScript type checking. Verify fetch calls use correct endpoints and headers. Test error handling with network failures. Validate Budget/License interfaces match backend Pydantic models exactly.",
            "parentId": "undefined"
          }
        ]
      },
      {
        "id": "31",
        "title": "Build Business Showcase Page V2 - Redesign /showcase for Non-Technical Business Users",
        "description": "Redesign the /showcase page to better communicate AI benefits to business users across departments (Customer Support, Marketing, Sales, Finance, BizOps, Executive), emphasizing orchestrator agents, data connectivity, and custom AI tooling for Fourth business services. Maintain existing ASCII starfield design aesthetic.",
        "details": "## Context Analysis\n\nCurrent implementation at `frontend/app/showcase/page.tsx:1-376` is technically-focused with Microsoft ecosystem details (Agent Framework, Foundry, Copilot Studio). The page uses:\n- AsciiStarfield component (`frontend/components/AsciiStarfield.tsx`) for visual aesthetic\n- Emerald/cyan color scheme with monospace fonts\n- GlitchText and TypewriterText animations\n- Technical terminology targeting developers\n\n## Required Changes\n\n### 1. Hero Section Update (`frontend/app/showcase/page.tsx:164-228`)\n**Replace technical messaging with business-focused value proposition:**\n- **Current:** \"THE FUTURE OF BUSINESS AI\" / \"Powered by Microsoft's AI Ecosystem\"\n- **New Headline:** \"BEYOND GENERIC COPILOT\" or \"AI THAT KNOWS YOUR BUSINESS\"\n- **New Subheading:** Emphasize three core messages:\n  1. \"Orchestrator agents deployed in user-friendly ways\"\n  2. \"Company data connected cohesively for agent use\"\n  3. \"Custom AI tooling for Fourth business services\"\n- Keep GlitchText animation and TypewriterText for engagement\n- Update CTA buttons: Replace \"EXPLORE ARCHITECTURE\" with \"SEE HOW IT WORKS\" and \"ACCESS PORTAL\" with \"REQUEST A DEMO\"\n\n### 2. Problem Statement Section (NEW - Insert after hero)\n**Add new section highlighting business pain points:**\n```typescript\nconst problemStatements = [\n  {\n    icon: AlertCircle, // from lucide-react\n    title: \"Generic AI Can't Help\",\n    description: \"Off-the-shelf Copilot doesn't understand Fourth's unique workflows, data structures, or business context\",\n    color: 'red'\n  },\n  {\n    icon: Database,\n    title: \"Data Silos Block Intelligence\",\n    description: \"Customer data, financials, and operations exist in separate systems—agents can't connect the dots\",\n    color: 'amber'\n  },\n  {\n    icon: Puzzle,\n    title: \"No Custom Tools for Fourth Services\",\n    description: \"Standard AI can't handle Fourth's specific needs: contract analysis, menu optimization, workforce planning\",\n    color: 'purple'\n  }\n]\n```\n- Use existing card styling pattern from `page.tsx:244-273`\n- 3-column grid on desktop, stack on mobile\n- Maintain emerald accent color scheme\n\n### 3. Department-Specific Value Proposition Cards (`frontend/app/showcase/page.tsx:230-273`)\n**Replace technical capabilities with department-focused benefits:**\n\n```typescript\nconst departmentCards = [\n  {\n    id: 'customer-support',\n    department: 'Customer Support',\n    icon: Headphones,\n    color: 'blue',\n    headline: 'Instant Context, Faster Resolution',\n    summary: 'AI agents that understand customer history, contract terms, and Fourth services—no more digging through tickets',\n    benefits: [\n      'Agent reads past interactions and contract details automatically',\n      'Suggests solutions based on similar customer issues',\n      'Drafts responses in your team\\'s tone and style'\n    ],\n    expandableExamples: [\n      {\n        title: 'Agent: Support Ticket Analyzer',\n        scenario: 'Customer reports invoice discrepancy',\n        actions: [\n          'Retrieves customer contract terms from SharePoint',\n          'Pulls invoice history from Finance system',\n          'Identifies billing rule mismatch',\n          'Drafts explanation email with correction steps'\n        ],\n        outcome: 'Resolution time reduced from 2 hours to 15 minutes'\n      }\n    ]\n  },\n  {\n    id: 'marketing',\n    department: 'Marketing',\n    icon: Megaphone,\n    color: 'purple',\n    headline: 'Campaigns That Know Your Audience',\n    summary: 'Agents analyze customer data to create targeted campaigns, measure impact, and optimize messaging automatically',\n    benefits: [\n      'Segment customers based on usage patterns and feedback',\n      'Generate campaign copy tailored to each segment',\n      'Track campaign performance and suggest optimizations'\n    ],\n    expandableExamples: [\n      {\n        title: 'Agent: Campaign Optimizer',\n        scenario: 'Launching new product feature to existing customers',\n        actions: [\n          'Analyzes customer usage data to identify best-fit segments',\n          'Generates personalized email variations by segment',\n          'Monitors open rates and adjusts messaging in real-time',\n          'Reports ROI and suggests next-best actions'\n        ],\n        outcome: 'Campaign engagement increased 40% vs. generic approach'\n      }\n    ]\n  },\n  // Similar cards for Sales, Finance, BizOps, Executive\n  {\n    id: 'sales',\n    department: 'Sales',\n    icon: TrendingUp,\n    color: 'emerald',\n    headline: 'Pipeline Intelligence, Deal Acceleration',\n    summary: 'AI that understands prospect needs, Fourth\\'s offerings, and competitive positioning to help close deals faster',\n    benefits: [\n      'Prepares prospect briefings with company research and pain points',\n      'Suggests Fourth solutions matching prospect\\'s industry/size',\n      'Drafts proposal sections and pricing recommendations'\n    ],\n    expandableExamples: [/* Similar structure */]\n  },\n  {\n    id: 'finance',\n    department: 'Finance',\n    icon: DollarSign,\n    color: 'green',\n    headline: 'Forecast Accuracy, Budget Control',\n    summary: 'Agents that analyze spending patterns, predict budget overruns, and recommend cost optimizations',\n    benefits: [\n      'Auto-categorizes expenses and flags anomalies',\n      'Forecasts quarterly spend based on historical trends',\n      'Suggests budget reallocations for better ROI'\n    ],\n    expandableExamples: [/* Similar structure */]\n  },\n  {\n    id: 'bizops',\n    department: 'BizOps',\n    icon: Cog,\n    color: 'orange',\n    headline: 'Process Optimization, Data-Driven Decisions',\n    summary: 'Identify inefficiencies, automate workflows, and surface insights from operational data',\n    benefits: [\n      'Maps current workflows and identifies bottlenecks',\n      'Automates repetitive data entry and reporting',\n      'Generates operational dashboards with key metrics'\n    ],\n    expandableExamples: [/* Similar structure */]\n  },\n  {\n    id: 'executive',\n    department: 'Executive',\n    icon: Crown,\n    color: 'amber',\n    headline: 'Strategic Insights, Risk Visibility',\n    summary: 'Board-ready summaries, competitive intelligence, and early warning signals from across the business',\n    benefits: [\n      'Synthesizes department reports into executive summaries',\n      'Tracks competitor moves and market trends',\n      'Flags potential risks based on data anomalies'\n    ],\n    expandableExamples: [/* Similar structure */]\n  }\n]\n```\n\n**Implementation Pattern:**\n- Default view shows collapsed cards (2-column grid on desktop)\n- Click/tap expands card to full-width modal overlay similar to `landingV2/page.tsx:882-942`\n- Expanded view shows department icon, headline, summary, bullet benefits, and 1-2 agent examples\n- Each example follows structure: Title → Scenario → Actions (step-by-step) → Outcome\n- Use existing colorClasses pattern from `landingV2/page.tsx:716-730`\n\n### 4. Simplified MSFT Ecosystem Visual (`frontend/app/showcase/page.tsx:275-325`)\n**Replace technical stack section with simplified \"How It Works\" diagram:**\n\n```typescript\nconst ecosystemSteps = [\n  {\n    step: 1,\n    icon: Database,\n    label: 'Connect Your Data',\n    description: 'Fourth\\'s data sources (SharePoint, Dynamics, custom databases) linked securely',\n    color: 'cyan'\n  },\n  {\n    step: 2,\n    icon: Brain,\n    label: 'Train Custom Agents',\n    description: 'AI agents learn Fourth\\'s business rules, terminology, and workflows',\n    color: 'purple'\n  },\n  {\n    step: 3,\n    icon: Layers,\n    label: 'Orchestrate Intelligence',\n    description: 'Agents work together: one retrieves data, another analyzes, a third drafts response',\n    color: 'emerald'\n  },\n  {\n    step: 4,\n    icon: Sparkles,\n    label: 'Deliver in Your Tools',\n    description: 'Access via Teams, Outlook, web portal—no new software to learn',\n    color: 'amber'\n  }\n]\n```\n\n- Visual flow: Step 1 → Step 2 → Step 3 → Step 4 with connecting lines/arrows\n- Each step card shows icon, step number, label, and brief description\n- Mobile: vertical stack, Desktop: horizontal flow\n- Remove technical JSON code block (lines 306-323)\n\n### 5. Journey/Roadmap Section (NEW - Replace \"START BUILDING\" CTA)\n**Add staged adoption path for business users:**\n\n```typescript\nconst journeyStages = [\n  {\n    stage: 'Pilot',\n    duration: '1-2 months',\n    icon: FlaskConical,\n    color: 'blue',\n    activities: [\n      'Select 1-2 departments for initial agents',\n      'Connect relevant data sources',\n      'Train agents with department workflows',\n      'Gather feedback from 10-20 users'\n    ],\n    outcome: 'Proof of value with measurable time savings'\n  },\n  {\n    stage: 'Scale',\n    duration: '3-6 months',\n    icon: TrendingUp,\n    color: 'emerald',\n    activities: [\n      'Expand to additional departments',\n      'Build department-specific agent library',\n      'Establish governance and security policies',\n      'Train champions in each department'\n    ],\n    outcome: 'Organization-wide adoption with documented ROI'\n  },\n  {\n    stage: 'Optimize',\n    duration: 'Ongoing',\n    icon: Zap,\n    color: 'purple',\n    activities: [\n      'Refine agents based on usage patterns',\n      'Add new data sources as business evolves',\n      'Develop advanced orchestration workflows',\n      'Monitor costs and optimize efficiency'\n    ],\n    outcome: 'AI becomes embedded in daily operations'\n  }\n]\n```\n\n- Timeline visualization with 3 stages\n- Each stage shows icon, duration, activities list, and expected outcome\n- Emphasizes gradual, low-risk adoption path\n\n### 6. UI/UX Enhancements\n\n**Maintain existing aesthetic:**\n- AsciiStarfield background (lines 127-134)\n- Emerald/cyan accent colors\n- Monospace fonts (font-mono class)\n- Border glow effects on hover\n- GlitchText for key headlines\n\n**Add new interactive elements:**\n- Expandable department cards with smooth transitions\n- \"See Example\" buttons that reveal agent workflow details\n- Hover states that highlight key information\n- Smooth scroll navigation between sections\n\n### 7. Component Structure\n\n**File modifications:**\n- `frontend/app/showcase/page.tsx`: Complete page redesign\n- Reuse `frontend/components/AsciiStarfield.tsx`: No changes needed\n- Reuse existing UI components from `frontend/components/ui/`:\n  - Card components for department cards\n  - Dialog/AlertDialog for expanded examples (if needed)\n  - Button for CTAs\n\n**New TypeScript interfaces:**\n```typescript\ninterface DepartmentCard {\n  id: string;\n  department: string;\n  icon: LucideIcon;\n  color: string;\n  headline: string;\n  summary: string;\n  benefits: string[];\n  expandableExamples: AgentExample[];\n}\n\ninterface AgentExample {\n  title: string;\n  scenario: string;\n  actions: string[];\n  outcome: string;\n}\n\ninterface JourneyStage {\n  stage: string;\n  duration: string;\n  icon: LucideIcon;\n  color: string;\n  activities: string[];\n  outcome: string;\n}\n```\n\n### 8. Content Writing Guidelines\n\n**Tone:**\n- Business-focused, not technical\n- Emphasize outcomes over features\n- Use concrete examples from Fourth's business\n- Avoid jargon: \"orchestrator agents\" explained as \"agents that work together\"\n\n**Key Messaging:**\n- AI understands Fourth's unique context\n- Data from multiple systems connected intelligently\n- Custom tools for Fourth-specific workflows\n- Gradual adoption, proven results\n\n### 9. Responsive Design\n\n- Mobile-first approach\n- Hero section: Stack headline and subheading\n- Department cards: 1 column mobile, 2 columns tablet, 2-3 columns desktop\n- Ecosystem steps: Vertical on mobile, horizontal on desktop\n- Journey stages: Vertical stack on mobile, horizontal timeline on desktop\n\n### 10. Accessibility\n\n- Semantic HTML structure (header, main, section, article)\n- ARIA labels for interactive elements\n- Keyboard navigation support for expanded cards\n- Color contrast meeting WCAG AA standards (emerald on black background)\n- Alt text for all icons (use lucide-react's built-in accessibility)",
        "testStrategy": "## Testing Strategy\n\n### 1. Visual Regression Testing\n- **Baseline Screenshots:** Capture screenshots of each section at 375px (mobile), 768px (tablet), 1440px (desktop)\n- **Compare:** Ensure ASCII starfield renders correctly, cards align properly, no layout breaks\n- **Tools:** Browser DevTools responsive mode or Playwright visual testing\n\n### 2. Content Validation\n- **Review with Stakeholders:** Present to representatives from each department (Support, Marketing, Sales, Finance, BizOps, Executive)\n- **Feedback Questions:** \n  - Does the value proposition resonate with your department's needs?\n  - Are the agent examples realistic and relevant?\n  - Is the language clear and jargon-free?\n- **Iterate:** Refine content based on feedback\n\n### 3. Interactive Functionality Testing\n- **Department Card Expansion:**\n  - Click each department card → verify modal/expanded view opens smoothly\n  - Check that expandable examples render correctly\n  - Verify close button (X) closes expanded view\n  - Test keyboard navigation (Tab, Enter, Escape)\n- **Smooth Scrolling:**\n  - Test navigation links to sections (if added)\n  - Verify smooth scroll behavior across browsers\n\n### 4. Animation Performance\n- **GlitchText Animation:** Verify headline glitch completes without stuttering\n- **TypewriterText Animation:** Ensure subheading types smoothly at correct speed\n- **AsciiStarfield Performance:** Monitor frame rate—should maintain 60fps on modern devices\n- **Card Hover Effects:** Test border glow and shadow effects on hover/focus\n\n### 5. Cross-Browser Compatibility\n- **Browsers:** Chrome, Firefox, Safari, Edge (latest versions)\n- **Test:**\n  - Layout consistency across browsers\n  - Font rendering (monospace fonts)\n  - CSS animations (GlitchText, TypewriterText)\n  - Backdrop blur effects (may need fallback for older browsers)\n\n### 6. Mobile Responsiveness\n- **Devices:** Test on iOS (iPhone 12/13/14) and Android (Pixel, Samsung Galaxy)\n- **Breakpoints:** 375px, 768px, 1024px, 1440px\n- **Checks:**\n  - Text remains readable at all sizes\n  - Cards stack properly on mobile\n  - Touch targets meet 44x44px minimum size\n  - No horizontal scrolling on small screens\n\n### 7. Accessibility Audit\n- **Automated Tools:** Run Lighthouse accessibility audit (target score: 95+)\n- **Manual Testing:**\n  - Navigate entire page using keyboard only (Tab, Shift+Tab, Enter, Escape)\n  - Test with screen reader (NVDA on Windows or VoiceOver on macOS)\n  - Verify color contrast ratios (emerald text on black background should pass WCAG AA)\n  - Check ARIA labels on interactive elements\n\n### 8. Performance Testing\n- **Page Load Time:** \n  - Measure initial load time (target: <3 seconds on 3G network)\n  - Monitor First Contentful Paint (FCP) and Largest Contentful Paint (LCP)\n- **JavaScript Bundle Size:** \n  - Check bundle size increase from new components\n  - Lazy load heavy components if needed\n- **Starfield Performance:** \n  - Monitor CPU usage during starfield animation\n  - Test on lower-end devices (consider reducing star count on mobile)\n\n### 9. User Acceptance Testing (UAT)\n- **Test Users:** Recruit 2-3 users from each target department (12-18 total)\n- **Tasks:**\n  1. Visit the /showcase page and describe what you understand about the AI platform\n  2. Find information relevant to your department\n  3. Expand a department card and review the agent example\n  4. Describe the adoption journey stages\n- **Success Metrics:**\n  - 80%+ users can explain the three core messages (orchestrator agents, data connectivity, custom tooling)\n  - 90%+ users can identify their department's value proposition\n  - 100% users can successfully expand/collapse department cards\n\n### 10. A/B Testing (Optional - Post-Launch)\n- **Variant A:** Current technical-focused /showcase page\n- **Variant B:** New business-focused /showcase page\n- **Metrics:**\n  - Time on page (target: 50% increase)\n  - Scroll depth (target: 80% reach bottom)\n  - CTA clicks (\"Request Demo\" target: 3x increase)\n  - Bounce rate (target: 20% reduction)\n- **Duration:** Run for 2 weeks with equal traffic split\n- **Decision:** Roll out winning variant to 100% traffic\n\n### 11. Content Quality Assurance\n- **Spelling/Grammar:** Run through Grammarly or similar tool\n- **Brand Consistency:** Verify Fourth branding guidelines followed (colors, terminology, tone)\n- **Accuracy:** Fact-check all department examples and outcomes\n- **Link Validation:** Test all CTA links and ensure they navigate correctly\n\n### 12. Regression Testing\n- **Verify existing functionality:** Ensure navigation bar, footer, and other shared components still work correctly\n- **Check other pages:** Confirm /landing and / (home) pages remain unaffected\n- **AsciiStarfield component:** Verify it still renders correctly on other pages using it (if any)",
        "status": "pending",
        "dependencies": [
          "4"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Update Hero Section with Business-Focused Messaging",
            "description": "Replace technical hero messaging with business value proposition emphasizing orchestrator agents, data connectivity, and custom AI tooling",
            "dependencies": [],
            "details": "Modify frontend/app/showcase/page.tsx:164-228. Replace 'THE FUTURE OF BUSINESS AI' headline with 'BEYOND GENERIC COPILOT' or 'AI THAT KNOWS YOUR BUSINESS'. Update GlitchText component usage. Change subheading from 'Powered by Microsoft's AI Ecosystem' to emphasize: (1) Orchestrator agents deployed in user-friendly ways, (2) Company data connected cohesively for agent use, (3) Custom AI tooling for Fourth business services. Keep TypewriterText animation. Update CTAs: Replace 'EXPLORE ARCHITECTURE' button with 'SEE HOW IT WORKS' and 'ACCESS PORTAL' with 'REQUEST A DEMO'. Maintain existing emerald/cyan color scheme, monospace fonts, and GlitchText animations.",
            "status": "pending",
            "testStrategy": "Visual inspection: verify hero text renders correctly at 375px, 768px, 1440px viewports. Test GlitchText animation completes without errors. Verify TypewriterText displays full message. Test CTA button links and hover states. Confirm emerald/cyan gradient renders properly on headline.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create Department-Specific Value Proposition Cards with Expandable Examples",
            "description": "Replace technical capabilities section with 6 department-focused benefit cards (Customer Support, Marketing, Sales, Finance, BizOps, Executive) featuring expandable agent workflow examples",
            "dependencies": [
              1
            ],
            "details": "Modify frontend/app/showcase/page.tsx:230-273. Create departmentCards array with 6 cards (Customer Support, Marketing, Sales, Finance, BizOps, Executive). Each card includes: department name, lucide-react icon (Headphones, Megaphone, TrendingUp, DollarSign, Cog, Crown), color (blue, purple, emerald, green, orange, amber), headline, summary, 3 benefits bullets, expandableExamples array. Implement expandable card pattern similar to landingV2/page.tsx:882-942 with modal overlay. Use colorClasses pattern from landingV2/page.tsx:716-730 for consistent styling. Default view: 2-column grid on desktop, 1-column mobile. Click expands card to full-width overlay showing agent example with: Title, Scenario, Actions (step-by-step), Outcome. Add close button (X icon) in top-right. Reuse existing Card UI components from frontend/components/ui/card.tsx.",
            "status": "pending",
            "testStrategy": "Test card grid layout responsiveness (1-col mobile, 2-col desktop). Click each card to verify expansion animation. Test modal overlay closes on X button click and backdrop click. Verify all 6 departments render with correct icons and colors. Test keyboard navigation for accessibility. Check color contrast meets WCAG AA. Test expandable examples display all sections (Title, Scenario, Actions, Outcome) correctly.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Add Problem Statement Section and Simplified 'How It Works' Diagram",
            "description": "Insert new problem statement section after hero and replace technical stack section with business-friendly 4-step workflow visualization",
            "dependencies": [
              1
            ],
            "details": "Insert problem statement section after hero (line 228). Create problemStatements array with 3 cards using AlertCircle, Database, Puzzle icons from lucide-react. Titles: 'Generic AI Can't Help', 'Data Silos Block Intelligence', 'No Custom Tools for Fourth Services'. Use red, amber, purple colors. 3-column grid desktop, stack mobile. Reuse card styling from page.tsx:244-273. Replace technical stack section (lines 275-325) with ecosystemSteps array: 4 steps with Database, Brain, Layers, Sparkles icons. Labels: 'Connect Your Data', 'Train Custom Agents', 'Orchestrate Intelligence', 'Deliver in Your Tools'. Colors: cyan, purple, emerald, amber. Visual flow: horizontal on desktop with connecting arrows, vertical stack mobile. Remove JSON code block (lines 306-323). Maintain emerald accent colors and border glow effects.",
            "status": "pending",
            "testStrategy": "Verify problem statement section renders immediately after hero. Test 3-column grid on desktop (≥1024px) collapses to 1-column on mobile (<768px). Check icons render correctly from lucide-react. Verify 'How It Works' section shows horizontal flow on desktop with step numbers 1-4. Test vertical stack on mobile. Confirm JSON code block removed successfully. Test color classes apply correctly (red, amber, purple, cyan, emerald). Verify border glow effects on hover.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Build Journey/Roadmap Section with Staged Adoption Path",
            "description": "Replace 'START BUILDING' CTA section with 3-stage adoption journey visualization (Pilot, Scale, Optimize) emphasizing gradual business rollout",
            "dependencies": [
              2,
              3
            ],
            "details": "Replace section at frontend/app/showcase/page.tsx:327-350. Create journeyStages array with 3 stages: Pilot (1-2 months, FlaskConical icon, blue), Scale (3-6 months, TrendingUp icon, emerald), Optimize (Ongoing, Zap icon, purple). Each stage includes: duration, icon, color, activities array (4 items), outcome string. Implement timeline visualization with 3 stages horizontally on desktop, vertically on mobile. Each stage card shows: icon in colored circle, stage name, duration badge, activities as bulleted list, outcome in emphasized text. Use existing card patterns and emerald border styling. Add connecting lines/arrows between stages on desktop view. Icons from lucide-react. Maintain monospace font and emerald/cyan accent colors throughout.",
            "status": "pending",
            "testStrategy": "Verify 'START BUILDING' section completely replaced. Test timeline renders horizontally on desktop (≥1024px) with connecting lines between stages. Confirm vertical stack on mobile (<768px). Check all 3 stage cards render with correct icons (FlaskConical, TrendingUp, Zap). Verify duration badges display correctly. Test activities list renders 4 items per stage. Confirm outcome text emphasized properly. Test color application (blue, emerald, purple) matches colorClasses pattern. Verify responsive layout breaks at correct breakpoints.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Add TypeScript Interfaces and Implement Responsive Design Polish",
            "description": "Define TypeScript interfaces for new data structures, ensure mobile-first responsive design, and implement accessibility features across all new sections",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Add TypeScript interfaces to frontend/app/showcase/page.tsx after imports: DepartmentCard (id, department, icon, color, headline, summary, benefits, expandableExamples), AgentExample (title, scenario, actions, outcome), JourneyStage (stage, duration, icon, color, activities, outcome), ProblemStatement (icon, title, description, color), EcosystemStep (step, icon, label, description, color). Implement mobile-first responsive design: hero stacks headline/subheading on mobile, department cards 1-col mobile/2-col tablet/2-3-col desktop, ecosystem steps vertical mobile/horizontal desktop, journey stages vertical mobile/horizontal desktop. Add accessibility: semantic HTML (header, main, section, article), ARIA labels for interactive elements, keyboard navigation for expanded cards, alt text for icons via lucide-react, ensure color contrast meets WCAG AA (emerald on black). Test all sections at 375px, 768px, 1440px.",
            "status": "pending",
            "testStrategy": "Run TypeScript compiler to verify all interfaces defined correctly with no type errors. Test responsive breakpoints at 375px (mobile), 768px (tablet), 1024px (desktop), 1440px (wide). Verify hero section stacks properly on mobile. Check all grid layouts collapse correctly. Use axe DevTools or Lighthouse to verify WCAG AA compliance. Test keyboard navigation: Tab through all interactive elements, Enter/Space to expand cards, Escape to close modals. Verify ARIA labels present on buttons and interactive elements. Test screen reader announces content correctly. Confirm all lucide-react icons have implicit accessibility support.",
            "parentId": "undefined"
          }
        ]
      },
      {
        "id": "32",
        "title": "HMLR Memory System Production Deployment & Monitoring",
        "description": "Deploy the completed HMLR Memory System to production Azure environments (Container Apps + Azure SQL), implement comprehensive health monitoring, logging, and user acceptance testing for conversation memory, personalized suggestions, and cross-session context retention.",
        "details": "## Context Analysis\n\nThe HMLR Memory System was fully implemented in commit `e15d003` (Dec 6, 2025) with 8,358 lines added across 36 files. Core components include:\n\n**Backend Architecture** (`backend/src/hmlr/`):\n- `service.py:1-456` - HMLRService orchestrator\n- `governor.py:1-424` - Topic routing & continuity detection\n- `bridge_block_mgr.py:1-445` - Conversation block management\n- `fact_scrubber.py:1-374` - Fact extraction from conversations\n- `scribe.py:1-334` - User profile updates\n- `hydrator.py:1-307` - Context assembly for LLM\n- `sql_client.py:1-370` - Azure SQL persistence layer\n- `memory_accessor.py:1-197` - Suggestion data retrieval\n- `suggestion_orchestrator.py:1-185` + `suggestion_providers.py:1-336` - Personalized suggestions\n\n**Database Schema** (`backend/scripts/hmlr_azure_sql_schema.sql:1-197`):\n- `fact_store` table - Per-user persistent facts\n- `user_profiles` table - Preferences & interaction patterns\n- `fact_history` table - Fact change tracking\n- Stored procedures: `upsert_fact`, `upsert_user_profile`, `get_facts_by_keywords`\n\n**Frontend Integration**:\n- `frontend/components/providers/GuideProvider.tsx:1-88` - Fetches personalized suggestions\n- `frontend/app/(authenticated)/guide/page.tsx:79-80` - HMLR feature highlight\n- `frontend/lib/api.ts` - API client endpoints\n\n**Test Coverage**: 11 test files with 3,939 lines covering integration, edge cases, error handling, cross-session memory, follow-up suggestions, and governor boundaries.\n\n## Production Deployment Tasks\n\n### 1. Azure SQL Database Deployment\n**Goal:** Create production HMLR schema in Azure SQL Database\n\n**Steps:**\n1. Verify Azure SQL connection string in production environment variables:\n   - `AZURE_SQL_SERVER`, `AZURE_SQL_DATABASE`, `AZURE_SQL_USERNAME`, `AZURE_SQL_PASSWORD`\n   - Ensure firewall rules allow Container Apps ingress\n\n2. Execute schema script:\n   ```bash\n   cd backend/scripts\n   python run_hmlr_schema.py --environment production\n   ```\n   - Script should connect to production Azure SQL\n   - Create tables: `fact_store`, `user_profiles`, `fact_history`\n   - Create stored procedures: `upsert_fact`, `upsert_user_profile`, `get_facts_by_keywords`\n   - Validate successful creation with SELECT queries\n\n3. Seed initial test data (optional for pilot users):\n   ```bash\n   python populate_hmlr_test_data.py --environment production --user-ids pilot-user-1,pilot-user-2\n   ```\n\n4. Verify database objects:\n   - Query `sys.tables` to confirm tables exist\n   - Execute `SELECT TOP 1 * FROM fact_store` to verify schema\n   - Test stored procedure: `EXEC upsert_fact @user_id='test', @key='test', @value='test', @category='Definition'`\n\n### 2. Backend Container Apps Deployment\n**Goal:** Deploy updated backend with HMLR enabled to Azure Container Apps\n\n**Steps:**\n1. Update backend environment variables in Container Apps (`agent-arch-api`):\n   ```bash\n   az containerapp update \\\n     --name agent-arch-api \\\n     --resource-group rg-agent-architecture \\\n     --set-env-vars \\\n       HMLR_ENABLED=true \\\n       AZURE_SQL_SERVER=\"<prod-server>.database.windows.net\" \\\n       AZURE_SQL_DATABASE=\"<prod-db>\" \\\n       AZURE_SQL_USERNAME=\"<username>\" \\\n       AZURE_SQL_PASSWORD=\"<secure-password>\"\n   ```\n\n2. Build and push Docker image:\n   ```bash\n   cd backend\n   docker build -t agentarchacr.azurecr.io/backend:hmlr-v1.0 .\n   az acr login --name agentarchacr\n   docker push agentarchacr.azurecr.io/backend:hmlr-v1.0\n   ```\n\n3. Update Container App to use new image:\n   ```bash\n   az containerapp update \\\n     --name agent-arch-api \\\n     --resource-group rg-agent-architecture \\\n     --image agentarchacr.azurecr.io/backend:hmlr-v1.0\n   ```\n\n4. Verify deployment health:\n   - Check logs: `az containerapp logs show --name agent-arch-api --resource-group rg-agent-architecture --tail 100`\n   - Test health endpoint: `curl https://agent-arch-api.icyplant-75ca2495.westeurope.azurecontainerapps.io/api/health`\n   - Verify HMLR service initialization in logs: Search for \"HMLR service initialized\"\n\n### 3. Frontend Static Web App Deployment\n**Goal:** Deploy updated frontend with HMLR UI features to Azure SWA\n\n**Steps:**\n1. Update `.env.production` with production API URL:\n   ```\n   NEXT_PUBLIC_API_URL=https://agent-arch-api.icyplant-75ca2495.westeurope.azurecontainerapps.io\n   ```\n\n2. Build frontend WITHOUT `.env.local`:\n   ```bash\n   cd frontend\n   mv .env.local .env.local.bak\n   npm run build\n   cp staticwebapp.config.json out/\n   mv .env.local.bak .env.local\n   ```\n\n3. Deploy to Azure SWA:\n   ```bash\n   npx @azure/static-web-apps-cli deploy out \\\n     --deployment-token $(az staticwebapp secrets list --name agent-arch-web-prod --query \"properties.apiKey\" -o tsv) \\\n     --env production\n   ```\n\n4. Verify deployment:\n   - Navigate to https://witty-coast-0e5f72203.3.azurestaticapps.net/guide\n   - Check Network tab confirms API calls to production backend\n   - Verify HMLR feature highlight displays at line 79-80 of guide page\n\n### 4. Health Monitoring & Observability\n**Goal:** Implement monitoring for HMLR system performance and errors\n\n**Backend Logging Enhancements** (`backend/src/hmlr/service.py`):\n- Add structured logging for all HMLR operations:\n  - Governor routing decisions: `logger.info(f\"Governor routed to scenario {scenario}: {block_id}\")`\n  - Fact extraction counts: `logger.info(f\"Extracted {len(facts)} facts from message\")`\n  - SQL operation durations: `logger.debug(f\"SQL query took {elapsed_ms}ms\")`\n  - Error conditions: `logger.error(f\"HMLR operation failed: {error}\", exc_info=True)`\n\n**Azure Application Insights Integration**:\n- Add Application Insights SDK to `backend/requirements.txt`: `applicationinsights>=0.11.10`\n- Initialize in `backend/src/main.py`:\n  ```python\n  from applicationinsights import TelemetryClient\n  tc = TelemetryClient(settings.appinsights_key)\n  ```\n- Track custom metrics:\n  - `tc.track_metric(\"hmlr_facts_extracted\", fact_count)`\n  - `tc.track_metric(\"hmlr_query_latency_ms\", latency)`\n  - `tc.track_event(\"hmlr_governor_routing\", {\"scenario\": scenario_name})`\n\n**Health Check Endpoint** (`backend/src/main.py`):\n- Create `/api/health/hmlr` endpoint:\n  ```python\n  @app.get(\"/api/health/hmlr\")\n  async def hmlr_health():\n      try:\n          # Test SQL connection\n          await hmlr_service.sql_client.test_connection()\n          # Test fact retrieval\n          facts = await hmlr_service.get_user_facts(\"health-check-user\", limit=1)\n          return {\"status\": \"healthy\", \"components\": {\"sql\": \"up\", \"fact_store\": \"up\"}}\n      except Exception as e:\n          return {\"status\": \"unhealthy\", \"error\": str(e)}, 500\n  ```\n\n**Azure Monitor Alerts**:\n- Create alert for HMLR errors: Query `traces | where message contains \"HMLR\" and severityLevel >= 3`\n- Create alert for slow HMLR queries: Query `customMetrics | where name == \"hmlr_query_latency_ms\" and value > 2000`\n\n### 5. User Acceptance Testing (UAT)\n**Goal:** Validate HMLR functionality with real users in production\n\n**Test Scenarios** (execute in production `/guide` page):\n\n**Scenario 1: Fact Extraction & Retention**\n1. User says: \"My name is David and I own the backend service\"\n2. Ask in new session: \"What's my name?\"\n3. Expected: Agent retrieves fact \"David\" from `fact_store` and responds correctly\n4. Validation: Query `SELECT * FROM fact_store WHERE user_id='<user_id>' AND category='Entity'`\n\n**Scenario 2: Open Loop Tracking**\n1. User says: \"Remind me to review the budget dashboard later\"\n2. Continue conversation on different topic\n3. Check suggestions bar for open loop reminder\n4. Expected: Suggestion like \"You wanted to review the budget dashboard\"\n5. Validation: Query `bridge_blocks` for open_loops array contains reminder\n\n**Scenario 3: Cross-Session Context**\n1. Session 1: Discuss task management for 5+ turns\n2. Close session, wait 1 hour\n3. Session 2: Ask \"What were we discussing earlier?\"\n4. Expected: Agent retrieves bridge blocks from Session 1, mentions task management context\n5. Validation: Check `backend/src/hmlr/memory_accessor.py:99-121` cross-session retrieval logic\n\n**Scenario 4: Personalized Suggestions**\n1. User performs 3+ queries about \"agents\"\n2. Return to `/guide` page\n3. Expected: Suggestions bar shows agent-related prompts (e.g., \"Tell me about agent tiers\")\n4. Validation: Check Network tab → `/api/guide/suggestions` returns `is_personalized: true`\n\n**Scenario 5: Topic Continuity**\n1. User discusses budget for 3 turns\n2. Switch to different topic (e.g., meetings)\n3. User says: \"Go back to budget discussion\"\n4. Expected: Governor routes to paused budget block, hydrator includes budget context\n5. Validation: Check logs for `Governor routed to scenario 2: TOPIC_RESUMPTION`\n\n**Performance Benchmarks**:\n- HMLR query latency: Target <500ms for suggestion retrieval\n- SQL fact lookup: Target <200ms for `get_user_facts()`\n- Memory overhead: Monitor Container Apps memory usage, ensure <80% utilization\n\n### 6. Documentation & Rollout\n**Goal:** Prepare user-facing documentation and gradual rollout plan\n\n**Update User Documentation**:\n- Add HMLR overview to platform help section\n- Reference `backend/docs/HMLR_Memory_System_Overview.md:1-80` for non-technical explanation\n- Create FAQ:\n  - \"How long does the system remember my conversations?\" → 7 days for open loops\n  - \"Can I delete my memory data?\" → Yes, via profile settings (future feature)\n  - \"Is my data secure?\" → Yes, Azure SQL with encryption at rest\n\n**Rollout Plan**:\n- **Phase 1 (Week 1)**: Deploy to 5 pilot users, daily check-ins for feedback\n- **Phase 2 (Week 2)**: Expand to 20 users, monitor Application Insights metrics\n- **Phase 3 (Week 3)**: Full rollout to all users\n- **Rollback Plan**: Set environment variable `HMLR_ENABLED=false` to disable without code changes\n\n**Monitoring Dashboard**:\n- Create Azure Dashboard with:\n  - HMLR query volume (queries/hour)\n  - Fact extraction rate (facts/conversation)\n  - Error rate (errors/hour)\n  - Average latency (ms)\n  - Top user queries triggering personalization\n\n### 7. Post-Deployment Validation\n**Goal:** Confirm HMLR system operates correctly in production\n\n**Validation Checklist**:\n- [ ] Azure SQL tables created successfully (fact_store, user_profiles, fact_history)\n- [ ] Backend Container Apps environment variables set (HMLR_ENABLED=true)\n- [ ] Backend logs show \"HMLR service initialized\" on startup\n- [ ] Frontend deployed with HMLR UI features visible\n- [ ] Health check endpoint `/api/health/hmlr` returns 200 OK\n- [ ] Test user can see personalized suggestions after 3+ interactions\n- [ ] Cross-session memory retrieves previous conversation context\n- [ ] Application Insights receives HMLR telemetry data\n- [ ] Alert rules configured for HMLR errors and latency\n- [ ] User documentation published and accessible\n\n**Performance Baselines** (establish in first week):\n- Record average HMLR query latency\n- Record average facts extracted per conversation\n- Record suggestion personalization rate (% of users seeing personalized suggestions)\n\n## Integration Points\n\n**Existing Systems**:\n- **Task 16 (AI Guide Intelligence)**: HMLR extends intent classification with memory-driven context\n- **Task 22 (AI Guide enhancements)**: Session memory and proactive insights now backed by HMLR persistent storage\n- **Azure SQL Database**: HMLR adds 3 new tables to existing database\n- **Container Apps**: Backend deployment uses existing `agent-arch-api` app\n- **Static Web Apps**: Frontend deployment uses existing `agent-arch-web-prod` app\n\n**API Endpoints** (added by HMLR):\n- `POST /api/guide/query` - Enhanced with HMLR context hydration\n- `GET /api/guide/suggestions` - Returns personalized suggestions from memory\n- `GET /api/health/hmlr` - HMLR health check (new)\n\n## Risk Mitigation\n\n**Risk 1: Azure SQL Connection Failures**\n- Mitigation: Implement connection pooling in `sql_client.py`, retry logic for transient errors\n- Fallback: If SQL unavailable, HMLR returns empty SuggestionData, system degrades gracefully\n\n**Risk 2: Memory Retrieval Latency**\n- Mitigation: Index optimization on `fact_store.user_id` and `fact_store.key` (already in schema)\n- Monitoring: Alert if latency exceeds 2 seconds\n\n**Risk 3: Data Privacy Concerns**\n- Mitigation: All facts tied to user_id, support future deletion endpoint\n- Compliance: Ensure Azure SQL encryption at rest enabled, audit logging configured\n\n**Risk 4: Fact Extraction Accuracy**\n- Mitigation: Confidence scores stored in `fact_store.confidence`, low-confidence facts flagged\n- Monitoring: Track fact verification rate over time",
        "testStrategy": "## Testing Strategy\n\n### Pre-Deployment Testing\n\n**1. Local Integration Testing**\n- Run full test suite: `cd backend && pytest tests/test_hmlr_*.py -v`\n- Expected: All 11 HMLR test files pass (3,939 lines of test coverage)\n- Verify test coverage: `pytest --cov=src.hmlr --cov-report=term-missing`\n- Target: >80% code coverage for all HMLR modules\n\n**2. Staging Environment Validation**\n- Deploy HMLR to staging environment first\n- Execute end-to-end tests with staging Azure SQL database\n- Test scenarios:\n  - Create test user, extract facts, retrieve in new session\n  - Simulate 50+ concurrent users querying suggestions endpoint\n  - Verify SQL connection pool handles load (monitor active connections)\n\n**3. Performance Load Testing**\n- Use Apache Bench or Locust to simulate 100 concurrent users\n- Target endpoints: `/api/guide/query`, `/api/guide/suggestions`\n- Success criteria: \n  - 95th percentile latency <1 second\n  - Zero 500 errors under normal load\n  - Container Apps CPU <70%, memory <80%\n\n### Production Deployment Testing\n\n**4. Smoke Tests (Immediately After Deployment)**\nExecute within 15 minutes of production deployment:\n\n```bash\n# Test 1: Health Check\ncurl https://agent-arch-api.icyplant-75ca2495.westeurope.azurecontainerapps.io/api/health/hmlr\n# Expected: {\"status\": \"healthy\", \"components\": {\"sql\": \"up\", \"fact_store\": \"up\"}}\n\n# Test 2: Suggestions Endpoint\ncurl -X GET \"https://agent-arch-api.icyplant-75ca2495.westeurope.azurecontainerapps.io/api/guide/suggestions?user_id=smoke-test-user&page_type=guide\"\n# Expected: 200 OK with {\"suggestions\": [...], \"is_personalized\": false}\n\n# Test 3: Query with Memory\ncurl -X POST \"https://agent-arch-api.icyplant-75ca2495.westeurope.azurecontainerapps.io/api/guide/query\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"My name is Test User\", \"user_id\": \"smoke-test-user\"}'\n# Expected: 200 OK with AI response\n\n# Test 4: Fact Retrieval\ncurl -X POST \"https://agent-arch-api.icyplant-75ca2495.westeurope.azurecontainerapps.io/api/guide/query\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"What is my name?\", \"user_id\": \"smoke-test-user\"}'\n# Expected: AI response contains \"Test User\"\n```\n\n**5. User Acceptance Test Scripts**\n\n**UAT Script 1: Fact Extraction & Persistence**\n```\n1. Navigate to https://witty-coast-0e5f72203.3.azurestaticapps.net/guide\n2. Login as pilot user (user_id recorded)\n3. Type: \"My department is Engineering and I work on backend services\"\n4. Wait for response\n5. Close browser completely\n6. Reopen browser, navigate to /guide\n7. Type: \"What department do I work in?\"\n8. VERIFY: Response mentions \"Engineering\" or \"backend services\"\n9. Database check:\n   - Query: SELECT * FROM fact_store WHERE user_id='<pilot_user_id>'\n   - VERIFY: At least 1 row with category='Definition' or 'Entity'\n```\n\n**UAT Script 2: Cross-Session Open Loops**\n```\n1. Login to /guide\n2. Type: \"Remind me to update the tech radar next week\"\n3. Continue conversation about unrelated topic (e.g., \"Show me my tasks\")\n4. Logout and wait 2 hours\n5. Login again to /guide\n6. VERIFY: Suggestions bar contains \"Update the tech radar\" or similar\n7. Database check:\n   - Query: SELECT open_loops FROM bridge_blocks WHERE user_id='<user_id>' AND session_id='<session1_id>'\n   - VERIFY: open_loops array contains \"update the tech radar\"\n```\n\n**UAT Script 3: Personalized Suggestions**\n```\n1. New user (never used /guide before)\n2. Type 5 queries about agents: \"What is an agent?\", \"Show me agent tiers\", \"List all agents\", \"What is agent status?\", \"Explain agent lifecycle\"\n3. Navigate away from /guide and return\n4. VERIFY: Suggestions bar shows agent-related prompts (e.g., \"Tell me about agent deployment\")\n5. Network tab check:\n   - GET /api/guide/suggestions\n   - Response: is_personalized=true\n6. Database check:\n   - Query: SELECT common_queries FROM user_profiles WHERE user_id='<user_id>'\n   - VERIFY: common_queries array contains agent-related keywords\n```\n\n**UAT Script 4: Topic Continuity**\n```\n1. Login to /guide\n2. Have 4-turn conversation about budget (e.g., \"Show budget\", \"What's our spend?\", \"Any overages?\", \"Cost breakdown?\")\n3. Switch topic: \"Show me upcoming meetings\"\n4. Type: \"Let's go back to discussing the budget\"\n5. VERIFY: AI response references previous budget discussion context (e.g., mentions cost breakdown from turn 4)\n6. Logs check:\n   - Search backend logs for \"Governor routed to scenario 2: TOPIC_RESUMPTION\"\n   - VERIFY: Log entry exists within last 5 minutes\n```\n\n### Post-Deployment Monitoring\n\n**6. Application Insights Validation**\nExecute 1 hour after deployment:\n\n```kusto\n// Query 1: HMLR Operation Volume\ntraces\n| where timestamp > ago(1h)\n| where message contains \"HMLR\"\n| summarize count() by bin(timestamp, 5m)\n// VERIFY: Non-zero operation count if users active\n\n// Query 2: HMLR Error Rate\ntraces\n| where timestamp > ago(1h)\n| where message contains \"HMLR\" and severityLevel >= 3\n| summarize ErrorCount=count()\n// VERIFY: ErrorCount = 0 (no errors in first hour)\n\n// Query 3: Average Latency\ncustomMetrics\n| where timestamp > ago(1h)\n| where name == \"hmlr_query_latency_ms\"\n| summarize avg(value), percentile(value, 95)\n// VERIFY: avg <500ms, p95 <1000ms\n```\n\n**7. Database Integrity Checks**\nExecute 24 hours after deployment:\n\n```sql\n-- Check 1: Fact Store Growth\nSELECT COUNT(*) AS TotalFacts, \n       COUNT(DISTINCT user_id) AS UniqueUsers,\n       AVG(confidence) AS AvgConfidence\nFROM fact_store\nWHERE created_at >= DATEADD(day, -1, GETUTCDATE());\n-- VERIFY: TotalFacts > 0, UniqueUsers > 0, AvgConfidence > 0.7\n\n-- Check 2: User Profile Updates\nSELECT COUNT(*) AS ProfileCount,\n       COUNT(DISTINCT user_id) AS ActiveUsers\nFROM user_profiles\nWHERE last_updated >= DATEADD(day, -1, GETUTCDATE());\n-- VERIFY: ProfileCount > 0, ActiveUsers matches pilot user count\n\n-- Check 3: Orphaned Data\nSELECT COUNT(*) AS OrphanedFacts\nFROM fact_store fs\nLEFT JOIN user_profiles up ON fs.user_id = up.user_id\nWHERE up.user_id IS NULL;\n-- VERIFY: OrphanedFacts = 0 (all facts have corresponding profiles)\n```\n\n**8. Performance Regression Testing**\nExecute weekly for 4 weeks post-deployment:\n\n- Compare baseline metrics (established Week 1) to current week:\n  - HMLR query latency (should not increase >20%)\n  - Fact extraction rate (should stabilize after Week 2)\n  - Suggestion personalization rate (should increase as profiles build)\n- Alert if regressions detected in any metric\n\n### Rollback Testing\n\n**9. Rollback Validation**\nTest rollback procedure in staging:\n\n```bash\n# Disable HMLR\naz containerapp update --name agent-arch-api-staging --set-env-vars HMLR_ENABLED=false\n\n# Test 1: Suggestions fallback to static\ncurl /api/guide/suggestions?user_id=test\n# VERIFY: is_personalized=false, fallback_reason=\"HMLR disabled\"\n\n# Test 2: Queries still work without memory\ncurl -X POST /api/guide/query -d '{\"query\": \"Show tasks\", \"user_id\": \"test\"}'\n# VERIFY: 200 OK response (graceful degradation)\n\n# Re-enable HMLR\naz containerapp update --name agent-arch-api-staging --set-env-vars HMLR_ENABLED=true\n```\n\n### Success Criteria\n\nDeployment is considered successful if:\n- [ ] All smoke tests pass within 15 minutes of deployment\n- [ ] All 4 UAT scripts complete successfully with pilot users (Week 1)\n- [ ] Application Insights shows zero HMLR errors in first 24 hours\n- [ ] Database integrity checks pass at 24-hour mark\n- [ ] Average HMLR query latency <500ms (p95 <1000ms)\n- [ ] At least 60% of pilot users see personalized suggestions by Week 2\n- [ ] Zero production incidents related to HMLR in first week\n- [ ] Rollback procedure tested and validated in staging",
        "status": "done",
        "dependencies": [
          "16",
          "22"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Deploy HMLR database schema to production Azure SQL",
            "description": "Execute Azure SQL schema creation script to provision fact_store, user_profiles, and fact_history tables with stored procedures in production database",
            "dependencies": [],
            "details": "Run backend/scripts/run_hmlr_schema.py with production Azure SQL connection string. Script creates 3 tables (fact_store with indexes on key/user_id/category/block_id, user_profiles with user_id index, fact_history with foreign key to fact_store) and 3 stored procedures (upsert_fact, get_facts_by_keywords, upsert_user_profile). Verify deployment by querying sys.tables and sys.procedures. Connection string format: Driver={ODBC Driver 18 for SQL Server};Server=tcp:<server>.database.windows.net,1433;Database=<db>;Uid=<user>;Pwd=<pass>;Encrypt=yes;TrustServerCertificate=yes;Connection Timeout=30. Confirm ODBC drivers installed in Dockerfile (lines 6-15 already include msodbcsql18). Test with SELECT TOP 1 * FROM fact_store and EXEC upsert_fact test call.",
            "status": "pending",
            "testStrategy": "Query INFORMATION_SCHEMA.TABLES to confirm 3 tables exist. Query sys.procedures to confirm 3 stored procedures exist. Execute test upsert_fact with @user_id='test', @fact_key='test', @value='test', @category='Definition' and verify row inserted with SELECT FROM fact_store WHERE user_id='test'.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Build and deploy backend Docker image with HMLR to Container Apps",
            "description": "Build backend Docker image with HMLR modules, push to Azure Container Registry (agentarchacr.azurecr.io), and update agent-arch-api Container App with new image and HMLR environment variables",
            "dependencies": [
              1
            ],
            "details": "From backend/ directory: (1) Build: docker build -t agentarchacr.azurecr.io/backend:hmlr-v1.0 . (uses existing Dockerfile with ODBC drivers at lines 6-15, healthcheck at line 28-29 checks /health endpoint at main.py:190). (2) Push: az acr login --name agentarchacr && docker push agentarchacr.azurecr.io/backend:hmlr-v1.0. (3) Update Container App: az containerapp update --name agent-arch-api --resource-group rg-agent-architecture --image agentarchacr.azurecr.io/backend:hmlr-v1.0 --set-env-vars HMLR_ENABLED=true HMLR_SQL_CONNECTION_STRING='<prod-connection-string>' (config.py:54-61 defines these vars). (4) Verify: Check logs with az containerapp logs show --tail 100, confirm 'HMLR service initialized' message (service instantiated at main.py:879), test https://agent-arch-api.icyplant-75ca2495.westeurope.azurecontainerapps.io/health returns {status:healthy}.",
            "status": "pending",
            "testStrategy": "After deployment: (1) curl https://agent-arch-api.icyplant-75ca2495.westeurope.azurecontainerapps.io/health should return 200 OK with {status:healthy,environment:production}. (2) Container logs should show 'HMLR service initialized' without errors. (3) Test HMLR endpoint: POST /api/guide/query with user message, verify response includes context from hmlr_service.route_query (main.py:2428).",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Deploy frontend with HMLR UI to Azure Static Web Apps",
            "description": "Build Next.js frontend with production API URL configured, copy staticwebapp.config.json, and deploy to Azure Static Web Apps witty-coast-0e5f72203.3.azurestaticapps.net",
            "dependencies": [
              2
            ],
            "details": "From frontend/ directory: (1) Temporarily move .env.local: mv .env.local .env.local.bak (prevents override of .env.production). (2) Build: npm run build (uses .env.production with NEXT_PUBLIC_API_URL=https://agent-arch-api.icyplant-75ca2495.westeurope.azurecontainerapps.io). (3) Copy config: cp staticwebapp.config.json out/ (existing config at frontend/staticwebapp.config.json:1-31 includes navigationFallback, 404 handling, CSP headers with backend API in connect-src line 19). (4) Deploy: npx @azure/static-web-apps-cli deploy out --deployment-token $(az staticwebapp secrets list --name agent-arch-web-prod --query properties.apiKey -o tsv) --env production. (5) Restore local env: mv .env.local.bak .env.local. (6) Verify: Navigate to https://witty-coast-0e5f72203.3.azurestaticapps.net/guide, check Network tab shows API calls to production backend.",
            "status": "pending",
            "testStrategy": "After deployment: (1) Open browser DevTools Network tab, navigate to https://witty-coast-0e5f72203.3.azurestaticapps.net/guide. (2) Verify API requests go to https://agent-arch-api.icyplant-75ca2495.westeurope.azurecontainerapps.io (not localhost). (3) Clear browser cache + hard reload (Ctrl+Shift+R). (4) Test personalized suggestions: perform 3+ queries about 'agents', verify GET /api/guide/suggestions returns suggestions with is_personalized:true. (5) Verify HMLR feature mentioned in guide page UI.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement HMLR health monitoring and observability",
            "description": "Add HMLR-specific health check endpoint, structured logging for HMLR operations, and Azure Application Insights telemetry tracking for fact extraction, query latency, and error monitoring",
            "dependencies": [
              2
            ],
            "details": "Create new endpoint in backend/src/main.py after existing /health endpoint (line 190): @app.get('/api/health/hmlr') async def hmlr_health_check() -> dict: try: await hmlr_service.sql_client.test_connection() (add test_connection method to sql_client.py), facts = await hmlr_service.get_user_facts('health-check-user', limit=1), return {status:'healthy',components:{sql:'up',fact_store:'up'}}; except Exception as e: return {status:'unhealthy',error:str(e)}, 500. Add structured logging to service.py route_query/store_turn methods: logger.info(f'Governor routed scenario {decision.scenario}: block {decision.target_block_id}'), logger.info(f'Extracted {len(facts)} facts from turn'), logger.debug(f'SQL query duration {elapsed_ms}ms'). Add Application Insights: pip install applicationinsights to requirements.txt, initialize in main.py lifespan with TelemetryClient(settings.appinsights_key), track metrics: tc.track_metric('hmlr_facts_extracted', count), tc.track_metric('hmlr_query_latency_ms', latency), tc.track_event('hmlr_governor_routing', {scenario: scenario_name}).",
            "status": "pending",
            "testStrategy": "Test /api/health/hmlr endpoint returns 200 {status:healthy} when SQL connected, 500 {status:unhealthy} when SQL unreachable. Verify Container Apps logs show structured HMLR logs with 'Governor routed', 'Extracted N facts', 'SQL query' messages. Create Azure Monitor alert query: traces | where message contains 'HMLR' and severityLevel >= 3 (errors). Create latency alert: customMetrics | where name == 'hmlr_query_latency_ms' and value > 2000. Confirm Application Insights receives telemetry by querying customMetrics and customEvents tables.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Execute user acceptance testing and performance validation",
            "description": "Conduct end-to-end UAT scenarios in production environment to validate fact extraction, cross-session memory, open loop tracking, personalized suggestions, and topic continuity with performance benchmarks",
            "dependencies": [
              3,
              4
            ],
            "details": "Execute 5 test scenarios: (1) Fact Retention: User says 'My name is David, I own backend service', new session ask 'What's my name?', verify response includes David, query SELECT * FROM fact_store WHERE user_id='<test-user>' AND category='Entity' confirms fact stored. (2) Open Loop Tracking: User says 'Remind me to review budget dashboard later', continue conversation on different topic, check suggestions bar shows open loop reminder, query bridge_blocks for open_loops array. (3) Cross-Session Context: Session 1 discuss task management 5+ turns, wait 1hr, Session 2 ask 'What were we discussing?', verify response mentions task management, confirm memory_accessor.py:99-121 retrieves prior session blocks. (4) Personalized Suggestions: Perform 3+ queries about 'agents', return to /guide page, verify suggestions bar shows agent-related prompts, check Network /api/guide/suggestions returns is_personalized:true. (5) Topic Continuity: Discuss budget 3 turns, switch to meetings, say 'Go back to budget', verify logs show 'Governor routed to scenario 2: TOPIC_RESUMPTION'. Performance benchmarks: HMLR query <500ms (P95), SQL fact lookup <200ms, Container Apps memory <80%.",
            "status": "pending",
            "testStrategy": "Document test results with screenshots and timestamps for each scenario. Query Azure SQL to verify fact_store, user_profiles, bridge_blocks data matches expected state after each scenario. Use browser DevTools Network tab to measure API response times for /api/guide/query and /api/guide/suggestions. Monitor Container Apps metrics in Azure Portal for memory/CPU utilization during testing. Capture Application Insights metrics for hmlr_query_latency_ms (target P95 <500ms) and hmlr_facts_extracted (track extraction rate). Create test report with pass/fail status for each scenario, performance metrics vs targets, and screenshots of UI showing personalized suggestions and memory recall.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Post-AI Response Chips Enhancement",
            "description": "COMPLETED Dec 7 2025: Enhanced 6 suggestion chips with source-based styling. Files: models.py (source field), suggestion_orchestrator.py (get_high_priority_open_loops), main.py (6-chip blending), api.ts (TS interface), GuideActionSuggestions.tsx (amber dashed styling for open loops)",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 32,
            "updatedAt": "2025-12-07T23:09:35.072Z",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-07T23:15:31.960Z"
      },
      {
        "id": "33",
        "title": "Code Review Fixes: Smart Auto-Scroll Implementation Issues",
        "description": "Address code review issues in smart auto-scroll implementation including undefined forceScroll function, scroll behavior inconsistencies, and accessibility improvements identified during review",
        "details": "## Identified Issues from Code Review\n\n### 1. **CRITICAL: Undefined forceScroll() function (frontend/app/(authenticated)/guide/page.tsx:32, 39)**\n- Line 32: `forceScroll` is destructured from useSmartAutoScroll return value\n- Line 39: `forceScroll()` is called in handleSend()\n- **Problem**: useSmartAutoScroll hook (frontend/hooks/useSmartAutoScroll.ts:102-107) does NOT export forceScroll\n- **Impact**: Runtime error \"forceScroll is not a function\" when sending message on guide page\n- **Fix**: Either add forceScroll to hook return value OR replace with scrollToBottom('instant') like GuideWidgetPanel.tsx:67\n\n### 2. **Inconsistent scroll behavior between components**\n- GuideWidgetPanel.tsx:67 calls `scrollToBottom('instant')` on send\n- guide/page.tsx:39 attempts to call `forceScroll()` (undefined)\n- **Fix**: Standardize both to use `scrollToBottom('instant')` for immediate scroll on message send\n\n### 3. **Missing auto-scroll trigger on new messages**\n- useSmartAutoScroll.ts:96-100: Auto-scroll only triggers when `isStreaming` changes\n- **Issue**: If user is at bottom and streaming completes, no new content triggers scroll until next streaming starts\n- **Fix**: Add dependency on messages.length or last message content to trigger scroll when new content arrives while at bottom\n\n### 4. **Accessibility improvements for ScrollToBottomButton**\n- ScrollToBottomButton.tsx:34: Has aria-live=\"polite\" but no dynamic text updates\n- **Enhancement**: Consider adding screen reader announcement when new content arrives while scrolled up\n- Button text changes between \"New content\" and \"Jump to bottom\" but may not be announced effectively\n\n### 5. **Debounce timing may be too aggressive**\n- useSmartAutoScroll.ts:5: SCROLL_DEBOUNCE_MS = 150ms\n- **Issue**: Fast scrolling may feel laggy, especially on mobile\n- **Suggestion**: Test with 100ms or make configurable via options\n\n### 6. **Memory leak prevention improvements**\n- useSmartAutoScroll.ts:28-36: isMounted ref prevents state updates after unmount (GOOD)\n- useSmartAutoScroll.ts:71-79: setTimeout cleanup in useEffect (GOOD)\n- **Enhancement**: Consider adding cleanup for IntersectionObserver in isMounted check (lines 45-47)\n\n## Implementation Plan\n\n### Phase 1: Fix Critical Runtime Error (PRIORITY)\n1. Remove forceScroll from guide/page.tsx:32 destructuring\n2. Replace forceScroll() call on line 39 with scrollToBottom('instant')\n3. Test: Send message on /guide page, verify no console errors\n\n### Phase 2: Standardize Behavior\n1. Verify GuideWidgetPanel.tsx already uses correct pattern (scrollToBottom('instant'))\n2. Document pattern in code comments for consistency\n3. Test: Both widget and page scroll immediately on send\n\n### Phase 3: Enhance Auto-Scroll Trigger\n1. Add optional dependency to useEffect (lines 96-100) to track content changes\n2. Options:\n   - Add messagesLength parameter to options\n   - Add lastMessageId parameter to options  \n   - Make auto-scroll trigger configurable\n3. Test: Verify scroll continues working during streaming and on new messages\n\n### Phase 4: Accessibility & Polish (Optional)\n1. Add aria-atomic=\"true\" to ScrollToBottomButton for better screen reader support\n2. Consider adding visually-hidden text that updates for screen readers\n3. Test debounce timing with user feedback\n4. Add unit tests for scroll behavior edge cases\n\n## Files to Modify\n\n**Critical:**\n- `frontend/app/(authenticated)/guide/page.tsx` (lines 32, 39)\n\n**Recommended:**\n- `frontend/hooks/useSmartAutoScroll.ts` (lines 96-100: auto-scroll trigger)\n- `frontend/components/guide/ScrollToBottomButton.tsx` (accessibility enhancements)\n\n**Documentation:**\n- Add JSDoc comments to useSmartAutoScroll explaining usage patterns\n- Document the instant vs smooth scroll behavior",
        "testStrategy": "## Testing Checklist\n\n### Critical Fix Verification (Must Pass)\n1. **Runtime Error Fix:**\n   - [ ] Navigate to /guide page\n   - [ ] Open browser console (F12)\n   - [ ] Type message and press Enter\n   - [ ] Verify NO errors about \"forceScroll is not a function\"\n   - [ ] Verify page scrolls to bottom immediately on send\n\n2. **Widget Behavior (Regression Test):**\n   - [ ] Open AI Guide widget from any page\n   - [ ] Send message, verify immediate scroll to bottom\n   - [ ] Verify widget auto-scrolls during streaming\n   - [ ] Scroll up manually during streaming, verify auto-scroll pauses\n   - [ ] Verify ScrollToBottomButton appears when scrolled up\n\n### Scroll Behavior Tests\n3. **Auto-scroll during streaming:**\n   - [ ] Send query that triggers long streaming response\n   - [ ] Verify auto-scrolls smoothly during streaming\n   - [ ] Scroll up 200px during streaming\n   - [ ] Verify auto-scroll pauses\n   - [ ] Verify ScrollToBottomButton appears with \"New content\" text\n   - [ ] Wait for streaming to complete while scrolled up\n   - [ ] Verify button remains visible with \"Jump to bottom\" text\n\n4. **Manual scroll detection:**\n   - [ ] Send 10+ messages to create scrollable content\n   - [ ] Scroll to middle of conversation\n   - [ ] Verify ScrollToBottomButton appears within 150ms\n   - [ ] Click button, verify smooth scroll to bottom\n   - [ ] Verify button disappears when at bottom\n\n5. **Edge cases:**\n   - [ ] Test with 0 messages (button should not appear)\n   - [ ] Test rapid scroll up/down (debounce should prevent jank)\n   - [ ] Test keyboard navigation (Space/Arrow keys)\n   - [ ] Test on mobile viewport (touch scrolling)\n\n### Accessibility Tests\n6. **Screen reader compatibility:**\n   - [ ] Enable screen reader (NVDA/JAWS/VoiceOver)\n   - [ ] Navigate to ScrollToBottomButton when visible\n   - [ ] Verify announces \"New content\" or \"Jump to bottom\" based on state\n   - [ ] Activate button, verify scroll occurs\n   - [ ] Verify button becomes unavailable (tabIndex=-1) when hidden\n\n7. **Keyboard navigation:**\n   - [ ] Tab to ScrollToBottomButton when visible\n   - [ ] Press Enter or Space to activate\n   - [ ] Verify focus management after scroll\n\n### Performance Tests\n8. **Memory leak verification:**\n   - [ ] Open guide page, send 10 messages\n   - [ ] Navigate away from page\n   - [ ] Check browser DevTools Memory tab for unmounted component leaks\n   - [ ] Verify IntersectionObserver and setTimeout cleaned up\n\n9. **Debounce timing:**\n   - [ ] Rapidly scroll up and down\n   - [ ] Verify UI remains responsive\n   - [ ] Verify button state updates correctly without flickering",
        "status": "done",
        "dependencies": [
          "22"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2025-12-07T23:10:33.006Z"
      },
      {
        "id": "34",
        "title": "Implement User Memories Page with HMLR Integration",
        "description": "Create comprehensive memories management system allowing authenticated users to view and manage AI-learned memories (facts, profile data, conversation topics/bridge blocks with open loops) from the HMLR system through a dedicated /memories page with tabbed interface.",
        "details": "## Backend Implementation\n\n### 1. Create Memories Router (backend/src/routers/memories.py)\nCreate new FastAPI router with 11 endpoints for CRUD operations:\n\n```python\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom typing import List, Optional\nfrom src.hmlr.models import Fact, UserProfile, BridgeBlock, FactCategory\nfrom src.hmlr.sql_client import HMLRSQLClient\nfrom src.hmlr.bridge_block_mgr import BridgeBlockManager\nfrom src.auth import get_current_user_id\n\nrouter = APIRouter(prefix=\"/api/memories\", tags=[\"memories\"])\n\n# Facts endpoints\n@router.get(\"/facts\") -> List[Fact]\n  # Query params: search (optional), limit (default 50)\n  # Call sql_client.get_facts_by_user() or search_facts()\n  \n@router.get(\"/facts/{fact_id}\") -> Fact\n  # Call sql_client.get_fact_by_key()\n  \n@router.post(\"/facts/verify/{fact_id}\") -> dict\n  # Update fact.verified = True via sql_client\n  \n@router.delete(\"/facts/{fact_id}\") -> dict\n  # Call sql_client.delete_fact()\n\n# Profile endpoints  \n@router.get(\"/profile\") -> UserProfile\n  # Call sql_client.get_user_profile()\n  # Return empty profile if None\n  \n@router.put(\"/profile\") -> UserProfile\n  # Body: preferences, common_queries, known_entities updates\n  # Call sql_client.update_profile_field() or save_user_profile()\n\n# Bridge Blocks / Topics endpoints\n@router.get(\"/topics\") -> List[BridgeBlock]\n  # Query param: sessions (optional, limit to specific sessions)\n  # Call bridge_block_mgr.get_user_blocks_with_open_loops(limit=20)\n  \n@router.get(\"/topics/{block_id}\") -> BridgeBlock\n  # Requires session_id query param\n  # Call bridge_block_mgr.get_block()\n  \n@router.delete(\"/topics/{block_id}\") -> dict\n  # Requires session_id query param\n  # Call bridge_block_mgr.delete_block()\n  \n@router.get(\"/stats\") -> dict\n  # Return aggregate stats: total_facts, verified_facts, \n  # total_topics, open_loops_count, profile_updated\n```\n\nUse dependency injection for user authentication (get_current_user_id) to ensure users only access their own memories.\n\n### 2. Update main.py\nAdd memories router:\n```python\nfrom src.routers import memories\napp.include_router(memories.router)\n```\n\n### 3. Extend sql_client.py\nAdd methods if missing:\n- `verify_fact(fact_id: int)` - set verified=True\n- `get_fact_stats(user_id: str)` - return counts for stats\n\n### 4. Extend bridge_block_mgr.py  \nEnsure `get_user_blocks_with_open_loops()` exists (already present in codebase at line 183-219).\n\n## Frontend Implementation\n\n### 1. Create Main Page (frontend/app/(authenticated)/memories/page.tsx)\nFollowing patterns from tasks/page.tsx:\n\n```tsx\n'use client';\nimport { useState, useEffect } from 'react';\nimport { Tabs, TabsList, TabsTrigger, TabsContent } from '@/components/ui/tabs';\nimport { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';\nimport { Badge } from '@/components/ui/badge';\nimport { Button } from '@/components/ui/button';\nimport { Brain, User, MessageCircle } from 'lucide-react';\nimport { FactsTab } from '@/components/memories/FactsTab';\nimport { ProfileTab } from '@/components/memories/ProfileTab';\nimport { TopicsTab } from '@/components/memories/TopicsTab';\nimport { memoriesApi } from '@/lib/api';\n\nexport default function MemoriesPage() {\n  const [stats, setStats] = useState(null);\n  const [activeTab, setActiveTab] = useState('facts');\n  \n  useEffect(() => {\n    loadStats();\n  }, []);\n  \n  // Stat cards: Total Facts, Verified, Topics, Open Loops\n  // 3 tabs: Facts, Profile, Topics\n}\n```\n\n### 2. Create Components (frontend/components/memories/)\n\n**FactsTab.tsx** - Facts management with search/filter\n- Display facts in card grid (key, value, category, confidence, verified badge)\n- Search bar for keyword filtering\n- Category filter dropdown (All, Definition, Acronym, Entity, Secret)\n- Verify button (checkmark) for unverified facts\n- Delete button with confirmation dialog\n- Evidence snippet tooltip on hover\n- API calls: memoriesApi.getFacts(), verifyFact(), deleteFact()\n\n**ProfileTab.tsx** - User profile view/edit\n- 3 sections: Preferences (key-value pairs), Common Queries (list), Known Entities (name-role pairs)\n- Editable forms using Input, Textarea components\n- Add/remove items buttons\n- Save button calls memoriesApi.updateProfile()\n- Display last_updated timestamp\n\n**TopicsTab.tsx** - Bridge blocks with open loops\n- Card list showing: topic_label, summary, open_loops (as badges), last_activity\n- Expandable accordion to view turns within block\n- Delete button for entire block\n- Filter by sessions (dropdown if user has multiple sessions)\n- Badge count for open_loops per block\n- API calls: memoriesApi.getTopics(), deleteTopic()\n\n**StatsCard.tsx** - Reusable stat display component\n```tsx\ninterface StatsCardProps {\n  title: string;\n  value: number;\n  icon: React.ComponentType;\n  description?: string;\n}\n```\n\n**FactCard.tsx** - Individual fact display with actions\n**TopicCard.tsx** - Individual bridge block display with details\n\n### 3. Update API Client (frontend/lib/api.ts)\nAdd memoriesApi section:\n\n```typescript\nexport interface Memory {\n  fact_id?: number;\n  user_id: string;\n  key: string;\n  value: string;\n  category: 'Definition' | 'Acronym' | 'Secret' | 'Entity';\n  confidence: number;\n  verified: boolean;\n  evidence_snippet?: string;\n  created_at: string;\n}\n\nexport interface UserProfileData {\n  user_id: string;\n  preferences: Record<string, any>;\n  common_queries: string[];\n  known_entities: Array<{name: string, role: string}>;\n  interaction_patterns: Record<string, any>;\n  last_updated: string;\n}\n\nexport interface Topic {\n  id: string;\n  session_id: string;\n  topic_label: string;\n  summary: string;\n  open_loops: string[];\n  turns: Array<{index: number, query: string, response_summary: string}>;\n  last_activity: string;\n}\n\nexport const memoriesApi = {\n  async getStats() {\n    return fetchApi('/api/memories/stats');\n  },\n  async getFacts(search?: string) {\n    const params = search ? `?search=${encodeURIComponent(search)}` : '';\n    return fetchApi<Memory[]>(`/api/memories/facts${params}`);\n  },\n  async verifyFact(factId: number) {\n    return fetchApi(`/api/memories/facts/verify/${factId}`, { method: 'POST' });\n  },\n  async deleteFact(factId: number) {\n    return fetchApi(`/api/memories/facts/${factId}`, { method: 'DELETE' });\n  },\n  async getProfile() {\n    return fetchApi<UserProfileData>('/api/memories/profile');\n  },\n  async updateProfile(updates: Partial<UserProfileData>) {\n    return fetchApi('/api/memories/profile', { \n      method: 'PUT', \n      body: JSON.stringify(updates) \n    });\n  },\n  async getTopics() {\n    return fetchApi<Topic[]>('/api/memories/topics');\n  },\n  async deleteTopic(blockId: string, sessionId: string) {\n    return fetchApi(`/api/memories/topics/${blockId}?session_id=${sessionId}`, { \n      method: 'DELETE' \n    });\n  },\n};\n```\n\n### 4. Update Sidebar (frontend/components/Sidebar.tsx)\nAdd Memories navigation item around line 49:\n```tsx\n{ name: 'My Memories', href: '/memories', icon: Brain },\n```\n\nImport Brain from lucide-react at top.\n\n## Design Patterns to Follow\n- Use existing component patterns from tasks/page.tsx (dialogs, filters, search)\n- Match UI styling from meetings/page.tsx (card layouts, stat cards)\n- Use Radix UI components (Tabs, Dialog, AlertDialog) like other pages\n- Follow error handling patterns with try-catch and console.error\n- Use async/await for all API calls\n- Implement optimistic UI updates where appropriate\n- Use lucide-react icons consistently\n\n## Security Considerations\n- All endpoints must validate user_id from JWT token\n- Users can only access their own memories (enforce in router with Depends(get_current_user_id))\n- No direct SQL injection risk (using parameterized queries in sql_client)\n- Delete operations require confirmation dialogs on frontend\n\n## Data Flow\n1. User navigates to /memories → page.tsx loads\n2. Stats endpoint called → displays 4 stat cards at top\n3. User switches tabs → respective Tab component loads data\n4. Facts tab: search/filter → filters client-side or re-fetches with search param\n5. Verify/Delete actions → optimistic update → reload data\n6. Profile tab: edit mode → save button → PUT /api/memories/profile\n7. Topics tab: display blocks → delete button → confirmation → DELETE with session_id",
        "testStrategy": "## Backend Testing\n\n1. **Endpoint Testing (Postman/curl)**\n   - Test GET /api/memories/facts returns facts array for authenticated user\n   - Test GET /api/memories/facts with search param filters correctly\n   - Test POST /api/memories/facts/verify/{id} sets verified=True\n   - Test DELETE /api/memories/facts/{id} removes fact and returns success\n   - Test GET /api/memories/profile returns profile or empty object\n   - Test PUT /api/memories/profile updates preferences/queries/entities\n   - Test GET /api/memories/topics returns blocks with open_loops\n   - Test DELETE /api/memories/topics/{id} with session_id removes block\n   - Test GET /api/memories/stats returns correct counts\n   - Test all endpoints reject requests without valid JWT token\n   - Test endpoints return 403 when trying to access other user's data\n\n2. **Integration Testing**\n   - Verify sql_client methods work with Azure SQL (facts CRUD)\n   - Verify bridge_block_mgr methods work with Cosmos DB (blocks CRUD)\n   - Test cross-session open loops retrieval (user_blocks_with_open_loops)\n   - Test fact verification persists to database\n   - Test profile updates merge correctly (don't overwrite entire object)\n\n3. **Error Handling**\n   - Test 404 when fact_id or block_id not found\n   - Test 400 for invalid input (missing required fields)\n   - Test 500 handling when database connection fails\n   - Verify error messages are user-friendly and don't leak sensitive info\n\n## Frontend Testing\n\n1. **Page Load Testing**\n   - Navigate to /memories → verify 3 tabs render (Facts, Profile, Topics)\n   - Verify stats cards display at top with correct numbers\n   - Verify default tab (Facts) loads data automatically\n   - Test tab switching loads appropriate data\n\n2. **Facts Tab Testing**\n   - Verify facts display in grid/list with all fields (key, value, category, confidence)\n   - Test search functionality filters facts by keyword\n   - Test category filter dropdown (All, Definition, Acronym, Entity, Secret)\n   - Click verify button on unverified fact → badge changes to \"Verified\"\n   - Click delete button → confirmation dialog appears\n   - Confirm delete → fact removed from list immediately\n   - Test with 0 facts → displays \"No facts yet\" message\n   - Test evidence snippet shows on hover/tooltip\n\n3. **Profile Tab Testing**\n   - Verify 3 sections render (Preferences, Common Queries, Known Entities)\n   - Test add/remove items in each section (buttons work)\n   - Edit fields and click Save → verify success message\n   - Verify last_updated timestamp displays and updates after save\n   - Test with empty profile → displays default/empty state correctly\n\n4. **Topics Tab Testing**\n   - Verify bridge blocks display with topic_label, summary, open_loops\n   - Test open_loops render as badges with count\n   - Click to expand block → turns list displays\n   - Click delete → confirmation dialog → confirm → block removed\n   - Test with 0 topics → \"No conversation topics yet\" message\n   - Verify last_activity displays in readable format\n\n5. **UI/UX Testing**\n   - Test responsive design on mobile/tablet/desktop\n   - Verify all buttons have appropriate icons (lucide-react)\n   - Test loading states show spinners during API calls\n   - Verify error states display user-friendly messages\n   - Test keyboard navigation (tab through forms, Escape closes dialogs)\n   - Verify confirmation dialogs prevent accidental deletes\n\n6. **API Integration Testing**\n   - Open DevTools Network tab → verify correct endpoints called\n   - Test error handling when API returns 500 (show error toast/message)\n   - Test with slow network → loading indicators display\n   - Verify JWT token sent in Authorization header\n   - Test session expiry handling (redirect to login)\n\n7. **Cross-Browser Testing**\n   - Test in Chrome, Firefox, Safari, Edge\n   - Verify Radix UI components render correctly\n   - Test dialogs, tabs, tooltips work across browsers\n\n## End-to-End Testing\n\n1. **Complete User Flow**\n   - Login → navigate to Memories page\n   - View all 3 tabs and verify data loads\n   - Search for a fact → verify it → see verified badge\n   - Delete a fact → confirm → verify it's gone\n   - Edit profile preferences → save → reload page → verify changes persisted\n   - Delete a topic → confirm → verify it's removed\n   - Check stats update after deletions\n\n2. **Multi-User Testing**\n   - Login as User A → add facts → logout\n   - Login as User B → verify cannot see User A's facts\n   - Verify User B sees only their own memories\n\n3. **Performance Testing**\n   - Test with 100+ facts → verify pagination or lazy loading\n   - Test with 20+ bridge blocks → verify performance acceptable\n   - Monitor memory usage during tab switching\n\n## Regression Testing\n\n1. **Existing Features**\n   - Verify HMLR suggestion system still works (/guide page)\n   - Verify AI Guide can still access facts during conversations\n   - Verify bridge blocks still created during AI Guide sessions\n   - Test fact extraction during normal guide conversations still works\n\n2. **Sidebar Navigation**\n   - Verify new \"My Memories\" link appears in sidebar\n   - Verify clicking navigates to /memories correctly\n   - Verify active state highlights correctly on /memories page",
        "status": "in-progress",
        "dependencies": [
          "18",
          "22"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Backend Memories Router with HMLR Endpoints",
            "description": "Implement FastAPI router backend/src/routers/memories.py with 11 CRUD endpoints for facts, profile, topics/bridge blocks with authentication",
            "dependencies": [],
            "details": "Create router following pattern from transcripts.py and audit.py. Implement GET /api/memories/facts with optional search param, GET /api/memories/facts/{fact_id}, POST /api/memories/facts/verify/{fact_id} to set verified=True, DELETE /api/memories/facts/{fact_id}. Add GET /api/memories/profile and PUT /api/memories/profile for user profile CRUD. Implement GET /api/memories/topics calling bridge_block_mgr.get_user_blocks_with_open_loops (exists at line 183-219), GET /api/memories/topics/{block_id} with session_id param, DELETE /api/memories/topics/{block_id}. Add GET /api/memories/stats returning aggregate counts. Use HMLRSQLClient methods: get_facts_by_user (line 91-139), search_facts (line 141-183), get_fact_by_key (line 185-232), delete_fact (line 234-252), get_user_profile (line 258-294), save_user_profile (line 296-324), update_profile_field (line 326+). Add authentication dependency injection pattern (check audit.py for reference). Import models from src.hmlr.models: Fact, UserProfile, BridgeBlock, FactCategory. Update backend/src/routers/__init__.py and backend/src/main.py to include new router.",
            "status": "pending",
            "testStrategy": "Test all 11 endpoints with curl/Postman. Verify GET /facts returns array, search param filters correctly. Test POST /verify updates verified flag. Test DELETE removes fact. Verify GET /profile returns profile or empty object. Test PUT /profile updates fields. Verify GET /topics returns blocks with open_loops. Test DELETE /topics with session_id. Verify GET /stats returns correct counts. Test authentication blocking unauthorized access.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Extend SQL Client with Missing Methods",
            "description": "Add verify_fact and get_fact_stats methods to HMLRSQLClient if not present, ensure compatibility with router endpoints",
            "dependencies": [
              1
            ],
            "details": "In backend/src/hmlr/sql_client.py, add verify_fact(fact_id: int) method that executes UPDATE fact_store SET verified = 1 WHERE fact_id = ? with commit. Add get_fact_stats(user_id: str) method returning dict with total_facts (COUNT(*)), verified_facts (COUNT WHERE verified=1), grouped by category if needed. Follow existing async method patterns (lines 50-324). Handle exceptions with logger.error and return appropriate defaults. Ensure methods use parameterized queries to prevent SQL injection. Test methods return correct data types matching Pydantic models.",
            "status": "pending",
            "testStrategy": "Unit test verify_fact updates verified column. Test get_fact_stats returns accurate counts for user. Verify SQL syntax with Azure SQL. Test error handling for invalid fact_ids. Confirm parameterized queries prevent injection.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Create Frontend Memories Page with Tabbed Interface",
            "description": "Build frontend/app/(authenticated)/memories/page.tsx with stats cards and 3-tab system (Facts, Profile, Topics) following tasks/page.tsx patterns",
            "dependencies": [
              1
            ],
            "details": "Create 'use client' page following pattern from tasks/page.tsx (lines 1-80). Import Tabs, TabsList, TabsTrigger, TabsContent from @/components/ui/tabs. Import Card, Badge, Button, Input components. Import Brain, User, MessageCircle icons from lucide-react. Create state for stats, activeTab. Add useEffect to call memoriesApi.getStats() on mount. Display 4 stat cards at top: Total Facts, Verified Facts, Topics, Open Loops using Card components. Implement 3 tabs: Facts (FactsTab component), Profile (ProfileTab component), Topics (TopicsTab component). Pass reload callbacks to child components. Match styling patterns from meetings/page.tsx for card layouts. Handle loading states and errors with try-catch. Use existing API client patterns from lib/api.ts (lines 1-80).",
            "status": "pending",
            "testStrategy": "Test page loads without errors. Verify stats cards display correct numbers from API. Test tab switching between Facts/Profile/Topics. Verify loading states show during API calls. Test error handling displays user-friendly messages. Confirm responsive layout on mobile/desktop. Verify navigation from sidebar works.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Build Memory Management Components (FactsTab, ProfileTab, TopicsTab)",
            "description": "Create 3 tab components in frontend/components/memories/ for managing facts, profile, and topics with full CRUD operations",
            "dependencies": [
              3
            ],
            "details": "Create frontend/components/memories/ directory. Build FactsTab.tsx: display facts in card grid (key, value, category badge, confidence, verified badge), implement search bar with debounce, category filter dropdown (All, Definition, Acronym, Entity, Secret), verify button (checkmark icon) calling memoriesApi.verifyFact(), delete button with AlertDialog confirmation, evidence tooltip on hover using Tooltip component. Build ProfileTab.tsx: 3 sections (Preferences key-value, Common Queries list, Known Entities name-role pairs), editable forms using Input/Textarea, add/remove buttons, save button calling memoriesApi.updateProfile(), display last_updated timestamp. Build TopicsTab.tsx: card list with topic_label, summary, open_loops badges, last_activity, accordion for turn details, delete button with confirmation, filter by sessions dropdown, call memoriesApi.getTopics() and deleteTopic(). Create reusable StatsCard.tsx (title, value, icon, description props), FactCard.tsx (fact display with actions), TopicCard.tsx (bridge block display). Follow component patterns from tasks/page.tsx dialogs (lines 10-26). Use existing UI components from frontend/components/ui/. Implement optimistic updates for verify/delete operations.",
            "status": "pending",
            "testStrategy": "Test FactsTab search filters facts correctly. Verify category dropdown filters by category. Test verify button updates fact and shows badge. Confirm delete confirmation dialog works. Test ProfileTab saves edits correctly. Verify add/remove buttons for lists. Test TopicsTab displays blocks with open loops. Confirm accordion expands turn details. Test delete topic confirmation. Verify all optimistic updates work correctly.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Update API Client and Sidebar Navigation",
            "description": "Add memoriesApi section to frontend/lib/api.ts with type definitions and add Memories navigation link to Sidebar.tsx",
            "dependencies": [
              1
            ],
            "details": "In frontend/lib/api.ts, add interface Memory extending Fact fields (fact_id, user_id, key, value, category, confidence, verified, evidence_snippet, created_at). Add interface UserProfileData matching UserProfile model (user_id, preferences, common_queries, known_entities, interaction_patterns, last_updated). Add interface Topic for BridgeBlock (id, session_id, topic_label, summary, open_loops, turns, last_activity). Create memoriesApi object with methods: getStats() → GET /api/memories/stats, getFacts(search?) → GET /api/memories/facts with optional search param, verifyFact(factId) → POST /api/memories/facts/verify/{factId}, deleteFact(factId) → DELETE /api/memories/facts/{factId}, getProfile() → GET /api/memories/profile, updateProfile(updates) → PUT /api/memories/profile with JSON body, getTopics() → GET /api/memories/topics, deleteTopic(blockId, sessionId) → DELETE /api/memories/topics/{blockId}?session_id={sessionId}. Use fetchApi helper following existing patterns (lines 1-80). In frontend/components/Sidebar.tsx, add to navigation array around line 49: { name: 'My Memories', href: '/memories', icon: Brain }. Import Brain from lucide-react.",
            "status": "pending",
            "testStrategy": "Test all memoriesApi methods call correct endpoints. Verify type definitions match backend models. Test error handling in API calls. Confirm fetchApi wrapper handles auth headers. Test Sidebar link navigates to /memories. Verify Brain icon displays correctly. Test link highlights when on /memories route.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2025-12-09T13:42:08.119Z"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2025-12-09T13:42:08.126Z",
      "taskCount": 34,
      "completedCount": 22,
      "tags": [
        "master"
      ]
    }
  }
}